{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Beau\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Beau\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Beau\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Beau\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import spacy\n",
    "from spacy.attrs import ORTH\n",
    "import textacy\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_excel(\"long_answer/training_set_rel3.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_lg',disable=['ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply spacy model to each essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['doc'] = raw_data.essay.apply(lambda essay: nlp(essay.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_height(root):\n",
    "\n",
    "    if not list(root.children):\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 + max(tree_height(x) for x in root.children)\n",
    "    \n",
    "def get_average_heights(paragraph):\n",
    "\n",
    "    if type(paragraph) == str:\n",
    "        doc = nlp(paragraph)\n",
    "    else:\n",
    "        doc = paragraph\n",
    "    roots = [sent.root for sent in doc.sents]\n",
    "    return np.mean([tree_height(root) for root in roots])\n",
    "\n",
    "def get_variance_heights(paragraph):\n",
    "\n",
    "    if type(paragraph) == str:\n",
    "        doc = nlp(paragraph)\n",
    "    else:\n",
    "        doc = paragraph\n",
    "    roots = [sent.root for sent in doc.sents]\n",
    "    return np.std([tree_height(root) for root in roots])\n",
    "\n",
    "def get_tree_heights(paragraph):\n",
    "    if type(paragraph) == str:\n",
    "        doc = nlp(paragraph)\n",
    "    else:\n",
    "        doc = paragraph\n",
    "    roots = [sent.root for sent in doc.sents]\n",
    "    return [tree_height(root) for root in roots]\n",
    "\n",
    "def get_sentences(doc):\n",
    "    sents = list(doc.sents)\n",
    "    return sents\n",
    "\n",
    "def get_sentence_count(sentences):\n",
    "    return float(len(sentences))\n",
    "\n",
    "def get_word_counts(doc):\n",
    "    return doc.count_by(ORTH)\n",
    "\n",
    "def get_connectives(doc):\n",
    "    text = doc.text.lower()\n",
    "    connectives = [\n",
    "    'after',\n",
    "    'earlier',\n",
    "    'before',\n",
    "    'during',\n",
    "    'while',\n",
    "    'later',\n",
    "    'because',\n",
    "    'consequently',\n",
    "    'thus',\n",
    "    'both',\n",
    "    'additionally',\n",
    "    'furthermore',\n",
    "    'moreover',\n",
    "    'actually',\n",
    "    'as a result',\n",
    "    'due to',\n",
    "    'but',\n",
    "    'yet',\n",
    "    'however',\n",
    "    'although',\n",
    "    'nevertheless'\n",
    "    ]\n",
    "    total = 0\n",
    "    for connector in connectives:\n",
    "        total += text.count(connector)\n",
    "    return float((total/len(doc)))\n",
    "\n",
    "def get_pos(doc):\n",
    "    return [token.pos_ for token in doc]\n",
    "\n",
    "\n",
    "def get_posngrams(poslist,n):\n",
    "    posngrams = []\n",
    "    for item in range(len(poslist) - n + 1):\n",
    "        posngrams.append(tuple([poslist[item+i] for i in range(n)]))\n",
    "    return posngrams\n",
    "\n",
    "def get_posgrams_counts(list_grams):\n",
    "    posgrams_counts = defaultdict(int)\n",
    "    for gram in list_grams:\n",
    "        posgrams_counts[gram] += 1\n",
    "    return posgrams_counts\n",
    "\n",
    "def get_TF(list_dicts):\n",
    "    TF_dict = defaultdict(int)\n",
    "    for dictionary in list_dicts:\n",
    "        for gram in dictionary:\n",
    "            TF_dict[gram] += dictionary[gram]\n",
    "    return TF_dict\n",
    "\n",
    "def get_mean_tfTF(posgram_counts,TF):\n",
    "    tfTF_ratios = list()\n",
    "    for key, value in posgram_counts.items():\n",
    "        tfTF_ratios.append(value/TF[key])\n",
    "    return np.mean(tfTF_ratios)\n",
    "\n",
    "def get_posngram_ratio(posngrams):\n",
    "    if len(posngrams) > 0:\n",
    "        return float(len(set(posngrams))/len(posngrams))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_reading_scores(doc):\n",
    "    scores = textacy.TextStats(doc).readability_stats\n",
    "    del scores['smog_index']\n",
    "    return scores\n",
    "\n",
    "def get_word_lengths(doc):\n",
    "    lengths = list()\n",
    "    for word in doc:\n",
    "        if word.is_alpha:\n",
    "            lengths.append(float(len(word)))\n",
    "    return lengths\n",
    "\n",
    "def get_words_of_length(lengths, n, p):\n",
    "    count = 0\n",
    "    for length in lengths:\n",
    "        if length > n and length < p:\n",
    "            count += 1\n",
    "    return float(count)\n",
    "\n",
    "def get_similarity_scores(doc):\n",
    "    sents = [sent for sent in doc.sents]\n",
    "    similarity_scores = list()\n",
    "    for i in range(1,len(sents)):\n",
    "        sent1 = sents[i-1]\n",
    "        sent2 = sents[i]\n",
    "        similarity_scores.append(sent1.similarity(sent2))\n",
    "    return np.mean(similarity_scores)\n",
    "\n",
    "def nth_root(x,n):\n",
    "    return x ** (1/float(n))\n",
    "\n",
    "def get_yules_k(word_counts):\n",
    "    m1 =  sum(word_counts.values())\n",
    "    m2 = sum([freq ** 2 for freq in word_counts.values()])\n",
    "    if m1 == m2:\n",
    "        k = 0 \n",
    "    else:\n",
    "        i = (m1*m1) / (m2-m1)\n",
    "        k = 1/i * 10000\n",
    "    if np.isnan(k):\n",
    "        k=10000\n",
    "    try:\n",
    "        return float(k)\n",
    "    except ZeroDivisionError:\n",
    "        return 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature preengineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preengineering\n",
    "raw_data['sentences'] = raw_data.doc.apply(get_sentences)\n",
    "raw_data['word_counts'] = raw_data.doc.apply(get_word_counts)\n",
    "raw_data['word_lengths'] = raw_data.doc.apply(get_word_lengths)\n",
    "raw_data['pos'] = raw_data.doc.apply(get_pos)\n",
    "\n",
    "raw_data['pos_trigrams'] = raw_data.pos.apply(lambda pos: get_posngrams(pos, n=3))\n",
    "raw_data['pos_fourgrams'] = raw_data.pos.apply(lambda pos: get_posngrams(pos, n=4))\n",
    "raw_data['pos_trigram_counts'] = raw_data.pos_trigrams.apply(get_posgrams_counts)\n",
    "raw_data['pos_fourgram_counts'] = raw_data.pos_fourgrams.apply(get_posgrams_counts)\n",
    "tri_pos_TF = get_TF(raw_data.pos_trigram_counts)\n",
    "four_pos_TF = get_TF(raw_data.pos_fourgram_counts)\n",
    "\n",
    "raw_data['tree_heights'] = raw_data.doc.apply(lambda doc: get_tree_heights(doc))\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "raw_data['reading_scores'] = raw_data.doc.apply(get_reading_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical Features\n",
    "raw_data['words_length_4'] = raw_data.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 4,6))\n",
    "raw_data['words_length_6'] = raw_data.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 6,8))\n",
    "raw_data['words_length_8'] = raw_data.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 8,10))\n",
    "raw_data['words_length_10'] = raw_data.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 10,12))\n",
    "raw_data['words_length_12'] = raw_data.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 12,100))\n",
    "raw_data['mean_word_length'] = raw_data.word_lengths.apply(np.mean)\n",
    "raw_data['variance_word_length'] = raw_data.word_lengths.apply(np.std)\n",
    "\n",
    "\n",
    "# Length Features\n",
    "raw_data['essay_length'] = raw_data.doc.apply(len)\n",
    "raw_data['num_words'] = raw_data.doc.apply(lambda doc: float(len([word for word in doc if word.is_alpha])))\n",
    "raw_data['num_sentences'] = raw_data.sentences.apply(get_sentence_count)\n",
    "raw_data['mean_sentence_length'] = raw_data.num_words/raw_data.num_sentences\n",
    "raw_data['num_characters'] = raw_data.essay.apply(len)\n",
    "raw_data['fourth_root_num_characters'] = raw_data.num_characters.apply(nth_root, n=4)\n",
    "\n",
    "# # Occurrence Features\n",
    "raw_data['num_commas'] = raw_data.essay.apply(lambda essay: float(essay.count(',')))\n",
    "raw_data['num_periods'] = raw_data.essay.apply(lambda essay: float(essay.count('.')))\n",
    "raw_data['num_exclaim'] = raw_data.essay.apply(lambda essay: float(essay.count('!')))\n",
    "raw_data['num_question'] = raw_data.essay.apply(lambda essay: float(essay.count('?')))\n",
    "raw_data['num_semicolon'] = raw_data.essay.apply(lambda essay: float(essay.count(';')))\n",
    "raw_data['num_colon'] = raw_data.essay.apply(lambda essay: float(essay.count(':')))\n",
    "\n",
    "# # Style Features\n",
    "# FIX raw_data['vocabulary'] = raw_data.word_tokens.apply(lambda word_tokens: set(word.lower() for word in word_tokens if word.isalpha()))\n",
    "raw_data['vocab_size'] = raw_data.word_counts.apply(len)\n",
    "raw_data['type_token_ratio'] = raw_data.word_counts.apply(len) / raw_data.essay_length\n",
    "raw_data['yules_k'] = raw_data.word_counts.apply(get_yules_k)\n",
    "\n",
    "# # Syntactical Features\n",
    "# # the number for these lengths comes from Chen and He 2013\n",
    "raw_data['sentence_lengths'] = raw_data.sentences.apply(lambda sentences: [len(sent) for sent in sentences])\n",
    "raw_data['very_short_sentences'] = raw_data.sentence_lengths.apply(lambda sentence_lengths: float(sum([length <= 10 for length in sentence_lengths])))\n",
    "raw_data['short_sentences'] = raw_data.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 10 and length <18 for length in sentence_lengths])))\n",
    "raw_data['medium_sentences'] = raw_data.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 18 and length <25 for length in sentence_lengths])))\n",
    "raw_data['long_sentences'] = raw_data.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 25 for length in sentence_lengths])))\n",
    "raw_data['variance_sentence_length'] = raw_data.sentence_lengths.apply(lambda sentence_lengths: np.std(sentence_lengths))\n",
    "\n",
    "raw_data['max_height'] = raw_data.tree_heights.apply(lambda heights: float(max(heights)))\n",
    "raw_data['sum_heights'] = raw_data.tree_heights.apply(sum)\n",
    "raw_data['mean_heights'] = raw_data.tree_heights.apply(np.mean)\n",
    "\n",
    "# raw_data['mean_sentence_similarity'] = raw_data.doc.apply(get_similarity_scores)\n",
    "\n",
    "# # POS Ngrams\n",
    "raw_data['pos_trigram_ratio'] = raw_data.pos_trigrams.apply(get_posngram_ratio)\n",
    "raw_data['pos_fourgram_ratio'] = raw_data.pos_fourgrams.apply(get_posngram_ratio)\n",
    "raw_data['mean_trigram_tfTF'] = raw_data.pos_trigram_counts.apply(lambda pos_trigram_counts: get_mean_tfTF(pos_trigram_counts, TF=tri_pos_TF))\n",
    "raw_data['mean_fourgram_tfTF'] = raw_data.pos_fourgram_counts.apply(lambda pos_fourgram_counts: get_mean_tfTF(pos_fourgram_counts, TF=four_pos_TF))\n",
    "\n",
    "# # Cohesion Features\n",
    "raw_data['connectives'] = raw_data.doc.apply(get_connectives)\n",
    "\n",
    "# Readability Features\n",
    "raw_data['flesch_kincaid_grade_level'] = raw_data.reading_scores.apply(lambda score_dict:score_dict['flesch_kincaid_grade_level'])\n",
    "raw_data['flesch_reading_ease'] = raw_data.reading_scores.apply(lambda score_dict:score_dict['flesch_reading_ease'])\n",
    "raw_data['gunning_fog_index'] = raw_data.reading_scores.apply(lambda score_dict:score_dict['gunning_fog_index'])\n",
    "raw_data['coleman_liau_index'] = raw_data.reading_scores.apply(lambda score_dict:score_dict['coleman_liau_index'])\n",
    "raw_data['automated_readability_index'] = raw_data.reading_scores.apply(lambda score_dict:score_dict['automated_readability_index'])\n",
    "raw_data['lix'] = raw_data.reading_scores.apply(lambda score_dict:score_dict['lix'])\n",
    "raw_data['gulpease_index'] = raw_data.reading_scores.apply(lambda score_dict:score_dict['gulpease_index'])\n",
    "raw_data['wiener_sachtextformel'] = raw_data.reading_scores.apply(lambda score_dict:score_dict['wiener_sachtextformel'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign correct score scale to each essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DivSeries = pd.DataFrame({'div': [12,5,3,3,4,4,25,50],'essay_set':[1,2,3,4,5,6,7,8]})\n",
    "eng_data = raw_data.merge(DivSeries, on='essay_set')\n",
    "eng_data['score'] = eng_data.domain1_score/eng_data['div']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and pickle training data dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata = eng_data.iloc[:,37:]\n",
    "trainingdata = trainingdata.drop(['sentence_lengths','div','reading_scores','tree_heights'], axis=1)\n",
    "trainingdata['essay_set'] = raw_data.essay_set\n",
    "trainingdata['essay_id'] = raw_data.essay_id\n",
    "trainingdata.to_pickle(\"./engineered_data.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
