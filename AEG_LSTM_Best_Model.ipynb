{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "import json, os, re, shutil, sys, time\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "from collections import defaultdict\n",
    "import xmltodict\n",
    "import untangle\n",
    "import xml.etree.ElementTree as ET\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NumPy, Pandas and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "\n",
    "from numpy.random import seed\n",
    "from pandas import read_csv, DataFrame\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Activation, Dropout, Conv1D, MaxPooling1D, Bidirectional, Flatten, TimeDistributed\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AEG Long Essay Sentence Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "aeg_long = pd.read_csv(\"../data-DNC/AEG/training_set_rel3.tsv\",sep='\\t',encoding = \"latin1\")\n",
    "#aeg_long.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test. We need to do this first to ensure that when we split to \n",
    "# sentence level, we have sentences of a given essay in either training or test but not on both.\n",
    "\n",
    "train_comb,test_comb = train_test_split(aeg_long,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10380, 5)\n",
      "(2596, 5)\n",
      "[ 3567  4233  6518 15174 18855]\n",
      "[ 9908  9872   305 12771  6839]\n"
     ]
    }
   ],
   "source": [
    "# Create train and test data frames with relevant fields\n",
    "\n",
    "x_train_df = train_comb.filter(['essay_id','essay_set','essay','domain1_score'], axis=1)\n",
    "x_test_df = test_comb.filter(['essay_id','essay_set','essay','domain1_score'], axis=1)\n",
    "\n",
    "x_train_df = x_train_df.reset_index()\n",
    "x_test_df = x_test_df.reset_index()\n",
    "print(x_train_df.shape)\n",
    "print(x_test_df.shape)\n",
    "print(x_train_df.essay_id[:5].values)\n",
    "print(x_test_df.essay_id[:5].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each essay set has a different scoring range. We need to normalize the scores to a standard scale for training.\n",
    "def normalize_score(essay):\n",
    "    \"\"\" Normalizes the domain score based on percentage\"\"\"\n",
    "    score = 0\n",
    "    score = float(essay.domain1_score)\n",
    "    essay_set = essay.essay_set\n",
    "    if essay_set == 1:\n",
    "        div = 12\n",
    "    elif essay_set == 2:\n",
    "        div = 5\n",
    "    elif essay_set == 3:\n",
    "        div = 3\n",
    "    elif essay_set == 4:\n",
    "        div = 3\n",
    "    elif essay_set == 5:\n",
    "        div = 4\n",
    "    elif essay_set == 6:\n",
    "        div = 4\n",
    "    elif essay_set == 7:\n",
    "        div = 25\n",
    "    elif essay_set == 8:\n",
    "        div = 50\n",
    "    return score/div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_df['Norm_Score'] = x_train_df.apply(normalize_score,axis=1)\n",
    "x_test_df['Norm_Score'] = x_test_df.apply(normalize_score,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10380,)\n",
      "(10380,)\n",
      "(2596,)\n",
      "(2596,)\n"
     ]
    }
   ],
   "source": [
    "x_essay_train = x_train_df['essay'].values\n",
    "y_essay_train = x_train_df['Norm_Score'].values\n",
    "x_essay_test = x_test_df['essay'].values\n",
    "y_essay_test = x_test_df['Norm_Score'].values\n",
    "print(x_essay_train.shape)\n",
    "print(y_essay_train.shape)\n",
    "print(x_essay_test.shape)\n",
    "print(y_essay_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer_essay = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer_essay.fit_on_texts(x_essay_train)\n",
    "essay_train_seq = tokenizer_essay.texts_to_sequences(x_essay_train)\n",
    "train_essay_data = pad_sequences(essay_train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len_class = train_essay_data.shape[1]\n",
    "max_words_class = vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2596, 1065)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_test_seq = tokenizer_essay.texts_to_sequences(x_essay_test)\n",
    "test_essay_data = pad_sequences(essay_test_seq, maxlen=max_len_class)\n",
    "test_essay_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('/Users/kurapati/W266/data/glove.42B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "for word, index in tokenizer_essay.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common Functions:\n",
    "\n",
    "def find_mult_factor(x):\n",
    "    \"\"\" Function to find the multiplication factor for denormalizing\"\"\"\n",
    "    if x[1] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.around(x[0]/x[1])\n",
    "    \n",
    "def denormalize(x):\n",
    "    \"\"\" Function to Denormalize the score\"\"\"\n",
    "    return np.around(x[2] * x[3])\n",
    "\n",
    "\n",
    "def essay_set_metrics_new(df):\n",
    "    \"\"\" Calculate per essay set metrics\"\"\"\n",
    "    set_df = pd.DataFrame(columns=['essay_set','RMSE','Kappa','Kappa_Q','Accuracy','Norm_RMSE'])\n",
    "    e_sets = np.unique(df.essay_set)\n",
    "    for e_s in e_sets:\n",
    "        df_s = df[df.essay_set == e_s]\n",
    "        original_score = df_s.orig_score.values.astype(int)\n",
    "        norm_original_score = df_s.norm_orig_score.values.astype(float)\n",
    "        predicted_score = df_s.pred_score.values.astype(int)\n",
    "        norm_predicted_score = df_s.norm_pred_score.values.astype(float)\n",
    "        rmse = RMSE(original_score,predicted_score)\n",
    "        kappa = cohen_kappa_score(original_score,predicted_score)\n",
    "        kappa_q = cohen_kappa_score(original_score,predicted_score,weights='quadratic')\n",
    "        accuracy = accuracy_score(original_score,predicted_score)\n",
    "        norm_rmse = RMSE(norm_original_score,norm_predicted_score)\n",
    "        set_df = set_df.append({'essay_set':e_s,'RMSE':rmse,'Kappa':kappa,'Kappa_Q':kappa_q,\n",
    "                                'Accuracy':accuracy,'Norm_RMSE':norm_rmse},ignore_index=True)\n",
    "    return set_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSE(actual, predict):\n",
    "    diff = actual - predict\n",
    "    diff = sum(diff**2) / len(actual)\n",
    "    return np.sqrt(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model definitions\n",
    "\n",
    "def FF_1_NN():\n",
    "    \"\"\" Simple feed forward NN\"\"\"\n",
    "    model_ff = Sequential()\n",
    "    model_ff.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_ff.add(tf.keras.layers.Flatten())\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_ff.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "    model_ff.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_ff\n",
    "\n",
    "def FF_2_NN():\n",
    "    \"\"\" Simple feed forward NN\"\"\"\n",
    "    model_ff = Sequential()\n",
    "    model_ff.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_ff.add(tf.keras.layers.Flatten())\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_ff.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "    model_ff.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_ff\n",
    "\n",
    "def FF_3_NN():\n",
    "    \"\"\" Simple feed forward NN\"\"\"\n",
    "    model_ff = Sequential()\n",
    "    model_ff.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_ff.add(tf.keras.layers.Flatten())\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_ff.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "    model_ff.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_ff\n",
    "\n",
    "def GRU_32():\n",
    "    \"\"\" Gated Recurrent Unit\"\"\"\n",
    "    model_gru = Sequential()\n",
    "    model_gru.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_gru.add(tf.keras.layers.GRU(32,activation='tanh'))\n",
    "    model_gru.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_gru.add(tf.keras.layers.Dense(1,name='out_layer'))\n",
    "    model_gru.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_gru\n",
    "\n",
    "def GRU_50():\n",
    "    \"\"\" Gated Recurrent Unit\"\"\"\n",
    "    model_gru = Sequential()\n",
    "    model_gru.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_gru.add(tf.keras.layers.GRU(32,activation='tanh'))\n",
    "    model_gru.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_gru.add(tf.keras.layers.Dense(1,name='out_layer'))\n",
    "    model_gru.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_gru\n",
    "\n",
    "def CNN_FF():\n",
    "    \"\"\" CNN with Feed Forward NN \"\"\"\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_conv.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(tf.keras.layers.Flatten())\n",
    "    model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "def CNN_lstm():\n",
    "    \"\"\" CNN with single layer LSTM & Feed Forward NN\"\"\"\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    #model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(tf.keras.layers.LSTM(100))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "def stack_lstm():\n",
    "    \"\"\" Three layered stacked LSTM.\"\"\"\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32,return_sequences=True))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32, return_sequences=True))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.2))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "def stateful_stacked_lstm():\n",
    "    # In stateful, total samples needs to be divisible by batch size\n",
    "    # we have 147026 samples, so selecting 6683 (6683*22=147026)\n",
    "    # The test sample need to be a multiple of 6683 as well\n",
    "    batch_size=2\n",
    "    model_conv = Sequential()\n",
    "    # In stateful, we have to pass batch_input_shape to the first layer\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], \n",
    "                                             trainable=False,batch_input_shape=(batch_size,max_len_class)))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32,stateful=True,return_sequences=True))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    #sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    #model_conv.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_conv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "estimator_lstm = KerasRegressor(build_fn=stack_lstm, epochs=20, batch_size=500)\n",
    "estimator_lstm.fit(train_essay_data, y_essay_train)\n",
    "\n",
    "# Predict for test data\n",
    "prediction_lstm=estimator_lstm.predict(test_essay_data)\n",
    "\n",
    "# Construct pandas data frame of scores of each sentences\n",
    "lstm_df = pd.DataFrame([x_test_df['domain1_score'].astype(np.double), y_essay_test.astype(np.double),\n",
    "                      prediction_lstm.astype(np.double)]).transpose()\n",
    "lstm_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "# Find Multiplication factor using the function we defined before\n",
    "lstm_df['Mult_Factor'] = lstm_df.apply(find_mult_factor,axis=1)\n",
    "# Find the denormalized predicted score\n",
    "lstm_df['Denorm_Pred_Score'] = lstm_df.apply(denormalize,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame([x_test_df.essay_id.values,x_test_df.essay_set.values,\n",
    "                         x_test_df.essay.values,lstm_df.Orig_Score.values,\n",
    "                          lstm_df.Norm_Score,lstm_df.Denorm_Pred_Score.values,\n",
    "                          lstm_df.Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','norm_orig_score',\n",
    "                     'pred_score','norm_pred_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>sentence</th>\n",
       "      <th>orig_score</th>\n",
       "      <th>norm_orig_score</th>\n",
       "      <th>pred_score</th>\n",
       "      <th>norm_pred_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9908</td>\n",
       "      <td>4</td>\n",
       "      <td>The author concludes the story w/this paragrap...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.478571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9872</td>\n",
       "      <td>4</td>\n",
       "      <td>I believe that the author concludes the story ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>0.675031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>305</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers, a very much talked about subject. D...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>9</td>\n",
       "      <td>0.736944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12771</td>\n",
       "      <td>5</td>\n",
       "      <td>I think in my opion is that the author was ver...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>0.474299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6839</td>\n",
       "      <td>3</td>\n",
       "      <td>The setting that affect the cyclist is the con...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>0.513603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id essay_set                                           sentence  \\\n",
       "0     9908         4  The author concludes the story w/this paragrap...   \n",
       "1     9872         4  I believe that the author concludes the story ...   \n",
       "2      305         1  Computers, a very much talked about subject. D...   \n",
       "3    12771         5  I think in my opion is that the author was ver...   \n",
       "4     6839         3  The setting that affect the cyclist is the con...   \n",
       "\n",
       "  orig_score norm_orig_score pred_score norm_pred_score  \n",
       "0          1        0.333333          1        0.478571  \n",
       "1          2        0.666667          2        0.675031  \n",
       "2         10        0.833333          9        0.736944  \n",
       "3          1            0.25          2        0.474299  \n",
       "4          1        0.333333          2        0.513603  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_set      RMSE     Kappa   Kappa_Q  Accuracy  Norm_RMSE\n",
      "0        1.0  1.072508  0.175147  0.691199  0.346995   0.086998\n",
      "1        2.0  0.680123  0.362906  0.598438  0.606952   0.120653\n",
      "2        3.0  0.616636  0.523499  0.702359  0.682635   0.191670\n",
      "3        4.0  0.465475  0.700051  0.881794  0.783333   0.179551\n",
      "4        5.0  0.634743  0.446078  0.772169  0.605797   0.142789\n",
      "5        6.0  0.586443  0.538680  0.825395  0.679894   0.138145\n",
      "6        7.0  3.109481  0.072556  0.721600  0.132450   0.123605\n",
      "7        8.0  5.240215  0.024575  0.476691  0.094891   0.104494\n",
      "Mean Quadratic Kappa:  0.708705928439022\n"
     ]
    }
   ],
   "source": [
    "essay_set_results = essay_set_metrics_new(result_df)\n",
    "print(essay_set_results)\n",
    "print(\"Mean Quadratic Kappa: \",np.mean(essay_set_results.Kappa_Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_lstm = stack_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_30 (Embedding)     (None, 1065, 300)         15000000  \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 1065, 32)          42624     \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 1065, 32)          8320      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 1065, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 15,059,297\n",
      "Trainable params: 59,297\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "s_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
