{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Activation, Dropout, Conv1D, MaxPooling1D, Bidirectional, Flatten, TimeDistributed\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(r'training_set_rel3.tsv',sep='\\t', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentenceEnders=r'!.?'\n",
    "SentenceContinuation=r',:;-'\n",
    "\n",
    "def EssayLength(Essay):\n",
    "    return len(Essay.split())\n",
    "\n",
    "def CountSentences(Essay):\n",
    "    count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "    return count(Essay, set(SentenceEnders))\n",
    "\n",
    "def CountContinuation(Essay):\n",
    "    count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "    return count(Essay, set(SentenceContinuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data = all_data\n",
    "DivSeries = pd.DataFrame({'div': [12,5,3,3,4,4,25,50],'dataset':[1,2,3,4,5,6,7,8]})\n",
    "\n",
    "# score normalization\n",
    "for i in all_data.essay_set.unique():\n",
    "    if(i==1):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 12\n",
    "    elif (i==2):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 5\n",
    "    elif (i in (3,4)):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 3\n",
    "    elif (i in (5,6)):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 4\n",
    "    elif (i == 7):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 25\n",
    "    else:\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 50\n",
    "\n",
    "\n",
    "train_sa_x_class,test_sa_x_class,train_sa_y_class,test_sa_y_class = train_test_split(np.asarray(classification_data.essay), classification_data[['adjusted_domain1_score','essay_set']],test_size=0.2, random_state=42)\n",
    "\n",
    "max_len_class = all_data.essay.apply(EssayLength).sort_values(ascending=True).iloc[int(np.floor(len(all_data)*.95))]\n",
    "\n",
    "tok_class = Tokenizer()\n",
    "tok_class.fit_on_texts(pd.Series(train_sa_x_class))\n",
    "sequences_class = tok_class.texts_to_sequences(train_sa_x_class)\n",
    "sequences_matrix_class = sequence.pad_sequences(sequences_class,maxlen=max_len_class)\n",
    "\n",
    "sequences_test_class = tok_class.texts_to_sequences(test_sa_x_class)\n",
    "sequences_test_matrix_class = sequence.pad_sequences(sequences_test_class,maxlen=max_len_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c8c51211564b4895f362a84dc19e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36434), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#looking to pull glove embeddings so no embedding training required.\n",
    "import csv\n",
    "gloves = pd.read_table(r\"glove.42B.300d.txt\", sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((len(tok_class.word_index)+1, 300))\n",
    "for word, i in tqdm_notebook(tok_class.word_index.items()):\n",
    "    if word in gloves.index:\n",
    "        embedding_matrix[i] = np.asarray(gloves.loc[word])\n",
    "    else:\n",
    "        embedding_matrix[i] = np.zeros(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 9s 972us/step - loss: 0.0904 - acc: 0.1286 - val_loss: 0.0526 - val_acc: 0.1445\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 7s 778us/step - loss: 0.0442 - acc: 0.1336 - val_loss: 0.0364 - val_acc: 0.1452\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 6s 734us/step - loss: 0.0291 - acc: 0.1379 - val_loss: 0.0313 - val_acc: 0.1452\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 7s 739us/step - loss: 0.0222 - acc: 0.1394 - val_loss: 0.0296 - val_acc: 0.1445\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 7s 798us/step - loss: 0.0168 - acc: 0.1400 - val_loss: 0.0282 - val_acc: 0.1458\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 7s 821us/step - loss: 0.0141 - acc: 0.1401 - val_loss: 0.0280 - val_acc: 0.1477\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 7s 833us/step - loss: 0.0125 - acc: 0.1402 - val_loss: 0.0280 - val_acc: 0.1471\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 7s 786us/step - loss: 0.0107 - acc: 0.1402 - val_loss: 0.0281 - val_acc: 0.1471\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 7s 790us/step - loss: 0.0093 - acc: 0.1402 - val_loss: 0.0298 - val_acc: 0.1458\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 6s 713us/step - loss: 0.0083 - acc: 0.1402 - val_loss: 0.0302 - val_acc: 0.1464\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 7s 739us/step - loss: 0.0073 - acc: 0.1403 - val_loss: 0.0292 - val_acc: 0.1471\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 7s 750us/step - loss: 0.0068 - acc: 0.1404 - val_loss: 0.0305 - val_acc: 0.1484\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 7s 771us/step - loss: 0.0062 - acc: 0.1404 - val_loss: 0.0305 - val_acc: 0.1471\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 7s 781us/step - loss: 0.0057 - acc: 0.1405 - val_loss: 0.0302 - val_acc: 0.1477\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 7s 794us/step - loss: 0.0058 - acc: 0.1407 - val_loss: 0.0326 - val_acc: 0.1484\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 7s 743us/step - loss: 0.0055 - acc: 0.1407 - val_loss: 0.0314 - val_acc: 0.1477\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 7s 746us/step - loss: 0.0049 - acc: 0.1407 - val_loss: 0.0318 - val_acc: 0.1477\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 7s 782us/step - loss: 0.0046 - acc: 0.1407 - val_loss: 0.0342 - val_acc: 0.1471\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 7s 807us/step - loss: 0.0044 - acc: 0.1407 - val_loss: 0.0334 - val_acc: 0.1471\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 7s 771us/step - loss: 0.0041 - acc: 0.1407 - val_loss: 0.0333 - val_acc: 0.1477\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 7s 813us/step - loss: 0.0040 - acc: 0.1408 - val_loss: 0.0339 - val_acc: 0.1471\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 7s 775us/step - loss: 0.0040 - acc: 0.1408 - val_loss: 0.0348 - val_acc: 0.1458\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 7s 760us/step - loss: 0.0040 - acc: 0.1408 - val_loss: 0.0345 - val_acc: 0.1471\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 7s 795us/step - loss: 0.0040 - acc: 0.1408 - val_loss: 0.0337 - val_acc: 0.1471\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 7s 830us/step - loss: 0.0042 - acc: 0.1408 - val_loss: 0.0351 - val_acc: 0.1458\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 7s 814us/step - loss: 0.0041 - acc: 0.1408 - val_loss: 0.0346 - val_acc: 0.1452\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 7s 763us/step - loss: 0.0046 - acc: 0.1408 - val_loss: 0.0373 - val_acc: 0.1458\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 7s 772us/step - loss: 0.0045 - acc: 0.1408 - val_loss: 0.0356 - val_acc: 0.1458\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 7s 748us/step - loss: 0.0041 - acc: 0.1409 - val_loss: 0.0352 - val_acc: 0.1439\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 7s 760us/step - loss: 0.0038 - acc: 0.1409 - val_loss: 0.0358 - val_acc: 0.1445\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 7s 783us/step - loss: 0.0038 - acc: 0.1410 - val_loss: 0.0382 - val_acc: 0.1452\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 7s 788us/step - loss: 0.0036 - acc: 0.1410 - val_loss: 0.0370 - val_acc: 0.1439\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 7s 802us/step - loss: 0.0033 - acc: 0.1410 - val_loss: 0.0359 - val_acc: 0.1426\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 7s 739us/step - loss: 0.0032 - acc: 0.1410 - val_loss: 0.0378 - val_acc: 0.1432\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 7s 761us/step - loss: 0.0031 - acc: 0.1410 - val_loss: 0.0386 - val_acc: 0.1432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f86169c2550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_1 = Sequential()\n",
    "ff_1.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "ff_1.add(Dense(100,name='deep1'))\n",
    "ff_1.add(Flatten())\n",
    "#Regression\n",
    "ff_1.add(Dense(1,activation='sigmoid',name='out_layer'))\n",
    "ff_1.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "ff_1.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#Classification\n",
    "#ff_1.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),activation='sigmoid',name='out_layer'))\n",
    "#ff_1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#ff_1.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0699 - acc: 0.1225 - val_loss: 0.0500 - val_acc: 0.1374\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 9s 970us/step - loss: 0.0363 - acc: 0.1360 - val_loss: 0.0347 - val_acc: 0.1445\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0260 - acc: 0.1379 - val_loss: 0.0280 - val_acc: 0.1477\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0215 - acc: 0.1384 - val_loss: 0.0276 - val_acc: 0.1477\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 9s 995us/step - loss: 0.0183 - acc: 0.1391 - val_loss: 0.0267 - val_acc: 0.1464\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 9s 983us/step - loss: 0.0162 - acc: 0.1393 - val_loss: 0.0285 - val_acc: 0.1464\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0146 - acc: 0.1396 - val_loss: 0.0278 - val_acc: 0.1471\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0131 - acc: 0.1399 - val_loss: 0.0291 - val_acc: 0.1471\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 8s 944us/step - loss: 0.0122 - acc: 0.1399 - val_loss: 0.0283 - val_acc: 0.1471\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0115 - acc: 0.1401 - val_loss: 0.0310 - val_acc: 0.1471\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0105 - acc: 0.1402 - val_loss: 0.0308 - val_acc: 0.1484\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0096 - acc: 0.1402 - val_loss: 0.0306 - val_acc: 0.1477\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0091 - acc: 0.1402 - val_loss: 0.0312 - val_acc: 0.1471\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0085 - acc: 0.1402 - val_loss: 0.0317 - val_acc: 0.1464\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 9s 989us/step - loss: 0.0084 - acc: 0.1402 - val_loss: 0.0320 - val_acc: 0.1464\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0088 - acc: 0.1403 - val_loss: 0.0346 - val_acc: 0.1458\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 8s 940us/step - loss: 0.0077 - acc: 0.1404 - val_loss: 0.0343 - val_acc: 0.1458\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0069 - acc: 0.1405 - val_loss: 0.0365 - val_acc: 0.1464\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0067 - acc: 0.1404 - val_loss: 0.0366 - val_acc: 0.1458\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0062 - acc: 0.1405 - val_loss: 0.0361 - val_acc: 0.1464\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 9s 994us/step - loss: 0.0059 - acc: 0.1407 - val_loss: 0.0381 - val_acc: 0.1464\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 9s 967us/step - loss: 0.0056 - acc: 0.1408 - val_loss: 0.0386 - val_acc: 0.1458\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0054 - acc: 0.1408 - val_loss: 0.0389 - val_acc: 0.1471\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0052 - acc: 0.1408 - val_loss: 0.0401 - val_acc: 0.1439\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0050 - acc: 0.1408 - val_loss: 0.0416 - val_acc: 0.1445\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0049 - acc: 0.1408 - val_loss: 0.0402 - val_acc: 0.1445\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 8s 955us/step - loss: 0.0047 - acc: 0.1408 - val_loss: 0.0381 - val_acc: 0.1445\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0047 - acc: 0.1408 - val_loss: 0.0400 - val_acc: 0.1445\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 9s 975us/step - loss: 0.0044 - acc: 0.1408 - val_loss: 0.0414 - val_acc: 0.1439\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 9s 989us/step - loss: 0.0041 - acc: 0.1408 - val_loss: 0.0408 - val_acc: 0.1439\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 9s 980us/step - loss: 0.0039 - acc: 0.1408 - val_loss: 0.0409 - val_acc: 0.1439\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0037 - acc: 0.1408 - val_loss: 0.0423 - val_acc: 0.1439\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 9s 975us/step - loss: 0.0035 - acc: 0.1408 - val_loss: 0.0422 - val_acc: 0.1439\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0034 - acc: 0.1408 - val_loss: 0.0439 - val_acc: 0.1426\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 9s 965us/step - loss: 0.0034 - acc: 0.1409 - val_loss: 0.0431 - val_acc: 0.1439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8617f1fb38>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_2 = Sequential()\n",
    "ff_2.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "ff_2.add(Dense(100,name='deep1'))\n",
    "ff_2.add(Dense(50,name='deep2'))\n",
    "ff_2.add(Flatten())\n",
    "#Regression\n",
    "ff_2.add(Dense(1,activation='sigmoid',name='out_layer'))\n",
    "ff_2.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "ff_2.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#Classification\n",
    "#ff_2.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),activation='sigmoid',name='out_layer'))\n",
    "#ff_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#ff_2.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 12s 1ms/step - loss: 0.0478 - acc: 0.1204 - val_loss: 0.0322 - val_acc: 0.1458\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0282 - acc: 0.1366 - val_loss: 0.0295 - val_acc: 0.1452\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0232 - acc: 0.1376 - val_loss: 0.0272 - val_acc: 0.1458\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0206 - acc: 0.1383 - val_loss: 0.0271 - val_acc: 0.1458\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0183 - acc: 0.1388 - val_loss: 0.0264 - val_acc: 0.1464\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0170 - acc: 0.1390 - val_loss: 0.0264 - val_acc: 0.1458\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0160 - acc: 0.1390 - val_loss: 0.0270 - val_acc: 0.1458\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0149 - acc: 0.1395 - val_loss: 0.0274 - val_acc: 0.1464\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0138 - acc: 0.1397 - val_loss: 0.0274 - val_acc: 0.1471\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0129 - acc: 0.1400 - val_loss: 0.0285 - val_acc: 0.1471\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0123 - acc: 0.1400 - val_loss: 0.0286 - val_acc: 0.1458\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0118 - acc: 0.1400 - val_loss: 0.0296 - val_acc: 0.1464\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0109 - acc: 0.1401 - val_loss: 0.0302 - val_acc: 0.1458\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0106 - acc: 0.1402 - val_loss: 0.0311 - val_acc: 0.1458\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0098 - acc: 0.1402 - val_loss: 0.0310 - val_acc: 0.1452\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0095 - acc: 0.1402 - val_loss: 0.0322 - val_acc: 0.1458\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0090 - acc: 0.1402 - val_loss: 0.0330 - val_acc: 0.1452\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0085 - acc: 0.1402 - val_loss: 0.0365 - val_acc: 0.1471\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0083 - acc: 0.1403 - val_loss: 0.0348 - val_acc: 0.1464\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0078 - acc: 0.1403 - val_loss: 0.0359 - val_acc: 0.1452\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0074 - acc: 0.1403 - val_loss: 0.0356 - val_acc: 0.1452\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0071 - acc: 0.1402 - val_loss: 0.0388 - val_acc: 0.1471\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0070 - acc: 0.1403 - val_loss: 0.0366 - val_acc: 0.1452\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0069 - acc: 0.1403 - val_loss: 0.0382 - val_acc: 0.1452\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0064 - acc: 0.1404 - val_loss: 0.0382 - val_acc: 0.1452\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0061 - acc: 0.1405 - val_loss: 0.0406 - val_acc: 0.1445\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0059 - acc: 0.1405 - val_loss: 0.0399 - val_acc: 0.1445\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0059 - acc: 0.1405 - val_loss: 0.0414 - val_acc: 0.1452\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0060 - acc: 0.1405 - val_loss: 0.0423 - val_acc: 0.1445\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0057 - acc: 0.1405 - val_loss: 0.0418 - val_acc: 0.1439\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0053 - acc: 0.1408 - val_loss: 0.0419 - val_acc: 0.1452\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0051 - acc: 0.1408 - val_loss: 0.0443 - val_acc: 0.1432\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0048 - acc: 0.1408 - val_loss: 0.0448 - val_acc: 0.1426\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0047 - acc: 0.1408 - val_loss: 0.0460 - val_acc: 0.1432\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0046 - acc: 0.1408 - val_loss: 0.0453 - val_acc: 0.1432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f86161d3f60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_3 = Sequential()\n",
    "ff_3.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "ff_3.add(Dense(100,name='deep1'))\n",
    "ff_3.add(Dense(50,name='deep2'))\n",
    "ff_3.add(Dense(25,name='deep3'))\n",
    "ff_3.add(Flatten())\n",
    "#Regression\n",
    "ff_3.add(Dense(1,activation='sigmoid',name='out_layer'))\n",
    "ff_3.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "ff_3.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#Classification\n",
    "#ff_3.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),activation='sigmoid',name='out_layer'))\n",
    "#ff_3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#ff_3.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification only\n",
    "#required to convert from cat_CE output back to single prediction\n",
    "NormalizeSeries = pd.Series(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns)\n",
    "def RetrieveNormalize(IndexVal):\n",
    "    return NormalizeSeries.loc[IndexVal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d074a0248e24ef68693e3c6e34255c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2596), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Weighted Quadrating Kappa scores: \n",
      "ff1    0.525088\n",
      "ff2    0.503471\n",
      "ff3    0.483585\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Classification\n",
    "#scoringFrame_FF = pd.DataFrame(columns=['actual','dataset','feedforward','ff1','ff2', 'ff3'],index=range(0,len(test_sa_y_class)))\n",
    "#for i in tqdm_notebook(scoringFrame_FF.index):\n",
    "    #scoringFrame_FF.loc[i, 'actual'] = test_sa_y_class.adjusted_domain1_score.iloc[i]\n",
    "    #scoringFrame_FF.loc[i, 'dataset'] = test_sa_y_class.essay_set.iloc[i]\n",
    "#scoringFrame_FF.dataset = scoringFrame_FF.dataset.astype(int)\n",
    "\n",
    "#ff_pred1 = ff_1.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class)).flatten()\n",
    "#ff_pred2 = ff_2.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class)).flatten()\n",
    "#ff_pred3 = ff_3.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class)).flatten()\n",
    "\n",
    "#scoringFrame_FF['ff1'] = np.argmax(ff_pred1,axis=1)\n",
    "#scoringFrame_FF['ff2'] = np.argmax(ff_pred2,axis=1)\n",
    "#scoringFrame_FF['ff3'] = np.argmax(ff_pred3,axis=1)\n",
    "\n",
    "#NOTE - this way of scoring is much slower than the batch scoring done for CNN/RNN. Including it for completion's sake\n",
    "#regression\n",
    "scoringFrame_FF = pd.DataFrame(columns=['actual','dataset','feedforward','ff1','ff2', 'ff3'],index=range(0,len(test_sa_y_class)))\n",
    "for i in tqdm_notebook(scoringFrame_FF.index):\n",
    "    scoringFrame_FF.loc[i, 'actual'] = test_sa_y_class.adjusted_domain1_score.iloc[i]\n",
    "    scoringFrame_FF.loc[i, 'dataset'] = test_sa_y_class.essay_set.iloc[i]\n",
    "    scoringFrame_FF.loc[i, 'ff1'] = ff_1.predict(sequences_test_matrix_class[i].reshape(1,max_len_class))[0][0]\n",
    "    scoringFrame_FF.loc[i, 'ff2'] = ff_2.predict(sequences_test_matrix_class[i].reshape(1,max_len_class))[0][0]\n",
    "    scoringFrame_FF.loc[i, 'ff3'] = ff_3.predict(sequences_test_matrix_class[i].reshape(1,max_len_class))[0][0]\n",
    "scoringFrame_FF.dataset = scoringFrame_FF.dataset.astype(int)\n",
    "\n",
    "scoringFrame_FF = scoringFrame_FF.merge(DivSeries, on='dataset')\n",
    "\n",
    "for colName in ['actual','ff1','ff2','ff3']:\n",
    "    scoringFrame_FF[colName] = scoringFrame_FF[colName] * scoringFrame_FF['div']\n",
    "    scoringFrame_FF[colName] = scoringFrame_FF[colName].apply(round)\n",
    "\n",
    "scoringFrame_FF.actual = scoringFrame_FF.actual.astype(int)\n",
    "\n",
    "QuadKappaCalculation = pd.DataFrame(columns = ['ff1', 'ff2', 'ff3'],index = np.unique(scoringFrame_FF.dataset))\n",
    "for essaySetValue in np.unique(scoringFrame_FF.dataset):\n",
    "    temp_ff_ES = scoringFrame_FF[scoringFrame_FF.dataset == essaySetValue]\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'ff1'] = cohen_kappa_score(temp_ff_ES.actual, temp_ff_ES.ff1.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'ff2'] = cohen_kappa_score(temp_ff_ES.actual, temp_ff_ES.ff2.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'ff3'] = cohen_kappa_score(temp_ff_ES.actual, temp_ff_ES.ff3.apply(round),weights='quadratic')\n",
    "print(\"Mean Weighted Quadrating Kappa scores: \")\n",
    "print(QuadKappaCalculation.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff1 RMSE, Cohen, Quad Cohen, accuracy: 3.0328199614114753, 0.30789610591583094, 0.94380742738271, 0.3898305084745763\n",
      "ff2 RMSE, Cohen, Quad Cohen, accuracy: 3.2238039910287957, 0.3134868672604054, 0.9358966126520158, 0.39522342064714944\n",
      "ff3 RMSE, Cohen, Quad Cohen, accuracy: 3.1528229823587255, 0.2899852661633757, 0.9397881395544337, 0.37403697996918334\n"
     ]
    }
   ],
   "source": [
    "print(\"ff1 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(scoringFrame_FF.actual, scoringFrame_FF.ff1.apply(round))),\n",
    "                                       cohen_kappa_score(scoringFrame_FF.actual, scoringFrame_FF.ff1.apply(round)),\n",
    "                                        cohen_kappa_score(scoringFrame_FF.actual, scoringFrame_FF.ff1.apply(round),weights='quadratic'),\n",
    "                                                 accuracy_score(scoringFrame_FF.actual, scoringFrame_FF.ff1.apply(round))))\n",
    "\n",
    "print(\"ff2 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(scoringFrame_FF.actual, scoringFrame_FF.ff2.apply(round))),\n",
    "                                       cohen_kappa_score(scoringFrame_FF.actual, scoringFrame_FF.ff2.apply(round)),\n",
    "                                        cohen_kappa_score(scoringFrame_FF.actual, scoringFrame_FF.ff2.apply(round),weights='quadratic'),             \n",
    "                                      accuracy_score(scoringFrame_FF.actual, scoringFrame_FF.ff2.apply(round))))\n",
    "\n",
    "print(\"ff3 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(scoringFrame_FF.actual, scoringFrame_FF.ff3.apply(round))),\n",
    "                                       cohen_kappa_score(scoringFrame_FF.actual, scoringFrame_FF.ff3.apply(round)),\n",
    "                                       cohen_kappa_score(scoringFrame_FF.actual, scoringFrame_FF.ff3.apply(round),weights='quadratic'),               \n",
    "                                      accuracy_score(scoringFrame_FF.actual, scoringFrame_FF.ff3.apply(round))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.6386 - acc: 0.0619 - val_loss: 0.0499 - val_acc: 0.1310\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0573 - acc: 0.1322 - val_loss: 0.0514 - val_acc: 0.1342\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0368 - acc: 0.1334 - val_loss: 0.0329 - val_acc: 0.1452\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0298 - acc: 0.1367 - val_loss: 0.0308 - val_acc: 0.1452\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0276 - acc: 0.1368 - val_loss: 0.0302 - val_acc: 0.1445\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0260 - acc: 0.1369 - val_loss: 0.0298 - val_acc: 0.1452\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0246 - acc: 0.1371 - val_loss: 0.0294 - val_acc: 0.1452\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0234 - acc: 0.1373 - val_loss: 0.0291 - val_acc: 0.1452\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0223 - acc: 0.1377 - val_loss: 0.0289 - val_acc: 0.1458\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0212 - acc: 0.1378 - val_loss: 0.0285 - val_acc: 0.1458\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0203 - acc: 0.1378 - val_loss: 0.0283 - val_acc: 0.1458\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0193 - acc: 0.1379 - val_loss: 0.0282 - val_acc: 0.1458\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0185 - acc: 0.1382 - val_loss: 0.0282 - val_acc: 0.1458\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0177 - acc: 0.1384 - val_loss: 0.0280 - val_acc: 0.1458\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0170 - acc: 0.1387 - val_loss: 0.0280 - val_acc: 0.1458\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0162 - acc: 0.1390 - val_loss: 0.0279 - val_acc: 0.1458\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0156 - acc: 0.1391 - val_loss: 0.0279 - val_acc: 0.1464\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0149 - acc: 0.1393 - val_loss: 0.0279 - val_acc: 0.1458\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0143 - acc: 0.1393 - val_loss: 0.0278 - val_acc: 0.1464\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0137 - acc: 0.1395 - val_loss: 0.0278 - val_acc: 0.1464\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0131 - acc: 0.1395 - val_loss: 0.0278 - val_acc: 0.1464\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0126 - acc: 0.1395 - val_loss: 0.0279 - val_acc: 0.1464\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0120 - acc: 0.1395 - val_loss: 0.0279 - val_acc: 0.1464\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0115 - acc: 0.1397 - val_loss: 0.0280 - val_acc: 0.1458\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0110 - acc: 0.1399 - val_loss: 0.0281 - val_acc: 0.1458\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0105 - acc: 0.1401 - val_loss: 0.0282 - val_acc: 0.1458\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0100 - acc: 0.1402 - val_loss: 0.0283 - val_acc: 0.1452\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0096 - acc: 0.1402 - val_loss: 0.0284 - val_acc: 0.1452\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0091 - acc: 0.1402 - val_loss: 0.0286 - val_acc: 0.1452\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0087 - acc: 0.1402 - val_loss: 0.0286 - val_acc: 0.1452\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0083 - acc: 0.1402 - val_loss: 0.0287 - val_acc: 0.1452\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0079 - acc: 0.1402 - val_loss: 0.0289 - val_acc: 0.1458\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0075 - acc: 0.1402 - val_loss: 0.0291 - val_acc: 0.1458\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0072 - acc: 0.1402 - val_loss: 0.0292 - val_acc: 0.1458\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0068 - acc: 0.1402 - val_loss: 0.0294 - val_acc: 0.1458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0cc62bc50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_1 = Sequential()\n",
    "cnn_1.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "cnn_1.add(Conv1D(64, 5, activation='relu'))\n",
    "cnn_1.add(MaxPooling1D(pool_size=4))\n",
    "cnn_1.add(Flatten())\n",
    "#regression\n",
    "cnn_1.add(Dense(1,name='out_layer'))\n",
    "cnn_1.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "cnn_1.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#classification\n",
    "#cnn_1.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),name='out_layer'))\n",
    "#cnn_1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#cnn_1.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 25s 3ms/step - loss: 0.1345 - acc: 0.1147 - val_loss: 0.0414 - val_acc: 0.1407\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0364 - acc: 0.1346 - val_loss: 0.0354 - val_acc: 0.1407\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0297 - acc: 0.1363 - val_loss: 0.0316 - val_acc: 0.1426\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0255 - acc: 0.1368 - val_loss: 0.0299 - val_acc: 0.1439\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 25s 3ms/step - loss: 0.0231 - acc: 0.1376 - val_loss: 0.0293 - val_acc: 0.1445\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0212 - acc: 0.1377 - val_loss: 0.0284 - val_acc: 0.1439\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0195 - acc: 0.1379 - val_loss: 0.0280 - val_acc: 0.1445\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0181 - acc: 0.1380 - val_loss: 0.0278 - val_acc: 0.1445\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0172 - acc: 0.1386 - val_loss: 0.0274 - val_acc: 0.1452\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0160 - acc: 0.1386 - val_loss: 0.0272 - val_acc: 0.1452\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0152 - acc: 0.1387 - val_loss: 0.0272 - val_acc: 0.1458\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0142 - acc: 0.1388 - val_loss: 0.0271 - val_acc: 0.1458\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0134 - acc: 0.1393 - val_loss: 0.0269 - val_acc: 0.1464\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0127 - acc: 0.1394 - val_loss: 0.0269 - val_acc: 0.1464\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0120 - acc: 0.1397 - val_loss: 0.0267 - val_acc: 0.1464\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0113 - acc: 0.1397 - val_loss: 0.0268 - val_acc: 0.1464\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0106 - acc: 0.1399 - val_loss: 0.0270 - val_acc: 0.1464\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 25s 3ms/step - loss: 0.0101 - acc: 0.1400 - val_loss: 0.0270 - val_acc: 0.1471\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0095 - acc: 0.1401 - val_loss: 0.0268 - val_acc: 0.1471\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0089 - acc: 0.1401 - val_loss: 0.0269 - val_acc: 0.1471\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0082 - acc: 0.1402 - val_loss: 0.0271 - val_acc: 0.1471\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0077 - acc: 0.1402 - val_loss: 0.0275 - val_acc: 0.1477\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0072 - acc: 0.1402 - val_loss: 0.0273 - val_acc: 0.1471\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0067 - acc: 0.1403 - val_loss: 0.0274 - val_acc: 0.1464\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0062 - acc: 0.1403 - val_loss: 0.0277 - val_acc: 0.1464\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0057 - acc: 0.1403 - val_loss: 0.0279 - val_acc: 0.1464\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0053 - acc: 0.1403 - val_loss: 0.0280 - val_acc: 0.1464\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0048 - acc: 0.1403 - val_loss: 0.0282 - val_acc: 0.1464\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0045 - acc: 0.1409 - val_loss: 0.0287 - val_acc: 0.1471\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0041 - acc: 0.1408 - val_loss: 0.0287 - val_acc: 0.1464\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0040 - acc: 0.1409 - val_loss: 0.0295 - val_acc: 0.1458\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0037 - acc: 0.1410 - val_loss: 0.0295 - val_acc: 0.1464\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0033 - acc: 0.1409 - val_loss: 0.0301 - val_acc: 0.1471\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0030 - acc: 0.1410 - val_loss: 0.0297 - val_acc: 0.1458\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0026 - acc: 0.1410 - val_loss: 0.0300 - val_acc: 0.1464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0eeea3518>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_2 = Sequential()\n",
    "cnn_2.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "cnn_2.add(Conv1D(64, 5, activation='relu'))\n",
    "cnn_2.add(MaxPooling1D(pool_size=4))\n",
    "cnn_2.add(Conv1D(20, 5, activation='relu'))\n",
    "cnn_2.add(MaxPooling1D(pool_size=2))\n",
    "cnn_2.add(Flatten())\n",
    "#regression\n",
    "cnn_2.add(Dense(1,name='out_layer'))\n",
    "cnn_2.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "cnn_2.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#classification\n",
    "#cnn_2.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),name='out_layer'))\n",
    "#cnn_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#cnn_2.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 25s 3ms/step - loss: 0.0716 - acc: 0.1244 - val_loss: 0.0343 - val_acc: 0.1458\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0286 - acc: 0.1367 - val_loss: 0.0289 - val_acc: 0.1458\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0245 - acc: 0.1363 - val_loss: 0.0274 - val_acc: 0.1452\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0218 - acc: 0.1376 - val_loss: 0.0258 - val_acc: 0.1464\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0197 - acc: 0.1378 - val_loss: 0.0252 - val_acc: 0.1464\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0180 - acc: 0.1380 - val_loss: 0.0250 - val_acc: 0.1464\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0166 - acc: 0.1384 - val_loss: 0.0247 - val_acc: 0.1471\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0156 - acc: 0.1384 - val_loss: 0.0247 - val_acc: 0.1471\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 25s 3ms/step - loss: 0.0142 - acc: 0.1388 - val_loss: 0.0245 - val_acc: 0.1471\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0132 - acc: 0.1392 - val_loss: 0.0245 - val_acc: 0.1471\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0122 - acc: 0.1395 - val_loss: 0.0247 - val_acc: 0.1464\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0114 - acc: 0.1399 - val_loss: 0.0247 - val_acc: 0.1471\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0106 - acc: 0.1399 - val_loss: 0.0251 - val_acc: 0.1464\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0096 - acc: 0.1401 - val_loss: 0.0251 - val_acc: 0.1471\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0090 - acc: 0.1402 - val_loss: 0.0253 - val_acc: 0.1464\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0081 - acc: 0.1402 - val_loss: 0.0256 - val_acc: 0.1477\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0075 - acc: 0.1402 - val_loss: 0.0259 - val_acc: 0.1471\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0067 - acc: 0.1402 - val_loss: 0.0261 - val_acc: 0.1471\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0060 - acc: 0.1402 - val_loss: 0.0266 - val_acc: 0.1464\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0056 - acc: 0.1402 - val_loss: 0.0271 - val_acc: 0.1458\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0050 - acc: 0.1403 - val_loss: 0.0274 - val_acc: 0.1458\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 25s 3ms/step - loss: 0.0044 - acc: 0.1404 - val_loss: 0.0274 - val_acc: 0.1458\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0039 - acc: 0.1404 - val_loss: 0.0277 - val_acc: 0.1471\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0036 - acc: 0.1404 - val_loss: 0.0283 - val_acc: 0.1464\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0032 - acc: 0.1404 - val_loss: 0.0284 - val_acc: 0.1471\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0029 - acc: 0.1404 - val_loss: 0.0286 - val_acc: 0.1464\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0026 - acc: 0.1404 - val_loss: 0.0290 - val_acc: 0.1471\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0023 - acc: 0.1404 - val_loss: 0.0291 - val_acc: 0.1477\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0022 - acc: 0.1404 - val_loss: 0.0298 - val_acc: 0.1464\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0021 - acc: 0.1407 - val_loss: 0.0297 - val_acc: 0.1471\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0019 - acc: 0.1407 - val_loss: 0.0298 - val_acc: 0.1477\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0016 - acc: 0.1407 - val_loss: 0.0300 - val_acc: 0.1471\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0015 - acc: 0.1409 - val_loss: 0.0303 - val_acc: 0.1477\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 25s 3ms/step - loss: 0.0014 - acc: 0.1409 - val_loss: 0.0306 - val_acc: 0.1471\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 25s 3ms/step - loss: 0.0012 - acc: 0.1409 - val_loss: 0.0309 - val_acc: 0.1471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0ac415d30>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_3 = Sequential()\n",
    "cnn_3.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "cnn_3.add(Conv1D(64, 5, activation='relu'))\n",
    "cnn_3.add(MaxPooling1D(pool_size=4))\n",
    "cnn_3.add(Conv1D(20, 5, activation='relu'))\n",
    "cnn_3.add(MaxPooling1D(pool_size=2))\n",
    "cnn_3.add(Conv1D(20, 5, activation='relu'))\n",
    "cnn_3.add(MaxPooling1D(pool_size=2))\n",
    "cnn_3.add(Flatten())\n",
    "#regression\n",
    "cnn_3.add(Dense(1,name='out_layer'))\n",
    "cnn_3.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "cnn_3.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#classification\n",
    "#cnn_3.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),name='out_layer'))\n",
    "#cnn_3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#cnn_3.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN1    0.571500\n",
       "CNN2    0.578590\n",
       "CNN3    0.569282\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_pred1 = cnn_1.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class)).flatten()\n",
    "cnn_pred2 = cnn_2.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class)).flatten()\n",
    "cnn_pred3 = cnn_3.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class)).flatten()\n",
    "\n",
    "#Uncomment in classification case\n",
    "#cnn_pred1 = np.argmax(cnn_pred1,axis=1)\n",
    "#cnn_pred2 = np.argmax(cnn_pred2,axis=1)\n",
    "#cnn_pred3 = np.argmax(cnn_pred3,axis=1)\n",
    "\n",
    "CNN_pred_frame = pd.DataFrame({'actual':np.asarray(test_sa_y_class.adjusted_domain1_score),\n",
    "                           'dataset': np.asarray(test_sa_y_class.essay_set),\n",
    "                           'CNN1':cnn_pred1,\n",
    "                          'CNN2':cnn_pred2,\n",
    "                          'CNN3':cnn_pred3})\n",
    "\n",
    "cnn_scoring = CNN_pred_frame.merge(DivSeries, on='dataset')\n",
    "\n",
    "for colName in ['actual', 'CNN1', 'CNN2', 'CNN3']:\n",
    "    #Uncomment in classification case\n",
    "    #if(colName != 'actual'):\n",
    "    #    cnn_scoring[colName] = cnn_scoring[colName].apply(RetrieveNormalize)\n",
    "    cnn_scoring[colName] = cnn_scoring[colName] * cnn_scoring['div']\n",
    "    cnn_scoring[colName] = cnn_scoring[colName].apply(round)\n",
    "\n",
    "cnn_scoring.actual = cnn_scoring.actual.astype(int)\n",
    "\n",
    "QuadKappaCalculation = pd.DataFrame(columns = ['CNN1', 'CNN2', 'CNN3'],index = np.unique(cnn_scoring.dataset))\n",
    "for essaySetValue in np.unique(cnn_scoring.dataset):\n",
    "    temp_CNN_ES = cnn_scoring[cnn_scoring.dataset == essaySetValue]\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'CNN1'] = cohen_kappa_score(temp_CNN_ES.actual, temp_CNN_ES.CNN1.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'CNN2'] = cohen_kappa_score(temp_CNN_ES.actual, temp_CNN_ES.CNN2.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'CNN3'] = cohen_kappa_score(temp_CNN_ES.actual, temp_CNN_ES.CNN3.apply(round),weights='quadratic')\n",
    "QuadKappaCalculation.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN1 RMSE, Cohen, Quad Cohen, accuracy: 2.7530624494441263, 0.3313692333735996, 0.9551708905519521, 0.4133281972265023\n",
      "CNN2 RMSE, Cohen, Quad Cohen, accuracy: 2.3984779457160506, 0.3652710641678031, 0.963926811955723, 0.44414483821263484\n",
      "CNN3 RMSE, Cohen, Quad Cohen, accuracy: 2.563522711093615, 0.3429602729180997, 0.9607755874545442, 0.4241140215716487\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN1 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(cnn_scoring.actual, cnn_scoring.CNN1.apply(round))),\n",
    "                                       cohen_kappa_score(cnn_scoring.actual, cnn_scoring.CNN1.apply(round)),\n",
    "                                        cohen_kappa_score(cnn_scoring.actual, cnn_scoring.CNN1.apply(round),weights='quadratic'),\n",
    "                                                 accuracy_score(cnn_scoring.actual, cnn_scoring.CNN1.apply(round))))\n",
    "\n",
    "print(\"CNN2 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(cnn_scoring.actual, cnn_scoring.CNN2.apply(round))),\n",
    "                                       cohen_kappa_score(cnn_scoring.actual, cnn_scoring.CNN2.apply(round)),\n",
    "                                        cohen_kappa_score(cnn_scoring.actual, cnn_scoring.CNN2.apply(round),weights='quadratic'),\n",
    "                                                 accuracy_score(cnn_scoring.actual, cnn_scoring.CNN2.apply(round))))\n",
    "\n",
    "print(\"CNN3 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(cnn_scoring.actual, cnn_scoring.CNN3.apply(round))),\n",
    "                                       cohen_kappa_score(cnn_scoring.actual, cnn_scoring.CNN3.apply(round)),\n",
    "                                        cohen_kappa_score(cnn_scoring.actual, cnn_scoring.CNN3.apply(round),weights='quadratic'),             \n",
    "                                      accuracy_score(cnn_scoring.actual, cnn_scoring.CNN3.apply(round))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 32s 4ms/step - loss: 0.0876 - acc: 0.0936 - val_loss: 0.0694 - val_acc: 0.1079\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0584 - acc: 0.1091 - val_loss: 0.0601 - val_acc: 0.1162\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0537 - acc: 0.1124 - val_loss: 0.0577 - val_acc: 0.1175\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 30s 3ms/step - loss: 0.0515 - acc: 0.1144 - val_loss: 0.0569 - val_acc: 0.1169\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0501 - acc: 0.1158 - val_loss: 0.0562 - val_acc: 0.1182\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0491 - acc: 0.1166 - val_loss: 0.0559 - val_acc: 0.1169\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0482 - acc: 0.1186 - val_loss: 0.0553 - val_acc: 0.1188\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 30s 3ms/step - loss: 0.0471 - acc: 0.1200 - val_loss: 0.0547 - val_acc: 0.1175\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0459 - acc: 0.1208 - val_loss: 0.0536 - val_acc: 0.1188\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0447 - acc: 0.1227 - val_loss: 0.0523 - val_acc: 0.1227\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0432 - acc: 0.1244 - val_loss: 0.0508 - val_acc: 0.1240\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0416 - acc: 0.1274 - val_loss: 0.0480 - val_acc: 0.1323\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0374 - acc: 0.1308 - val_loss: 0.0435 - val_acc: 0.1355\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0345 - acc: 0.1324 - val_loss: 0.0408 - val_acc: 0.1394\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0306 - acc: 0.1359 - val_loss: 0.0361 - val_acc: 0.1432\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0274 - acc: 0.1375 - val_loss: 0.0330 - val_acc: 0.1458\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0250 - acc: 0.1385 - val_loss: 0.0315 - val_acc: 0.1477\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0236 - acc: 0.1385 - val_loss: 0.0287 - val_acc: 0.1477\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 30s 3ms/step - loss: 0.0220 - acc: 0.1390 - val_loss: 0.0278 - val_acc: 0.1484\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 30s 3ms/step - loss: 0.0209 - acc: 0.1391 - val_loss: 0.0266 - val_acc: 0.1484\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0199 - acc: 0.1393 - val_loss: 0.0261 - val_acc: 0.1484\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0194 - acc: 0.1392 - val_loss: 0.0254 - val_acc: 0.1484\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0186 - acc: 0.1394 - val_loss: 0.0249 - val_acc: 0.1484\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0182 - acc: 0.1394 - val_loss: 0.0247 - val_acc: 0.1490\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0177 - acc: 0.1395 - val_loss: 0.0245 - val_acc: 0.1490\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0178 - acc: 0.1396 - val_loss: 0.0248 - val_acc: 0.1484\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0171 - acc: 0.1396 - val_loss: 0.0240 - val_acc: 0.1484\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0168 - acc: 0.1396 - val_loss: 0.0240 - val_acc: 0.1484\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 30s 3ms/step - loss: 0.0164 - acc: 0.1396 - val_loss: 0.0238 - val_acc: 0.1484\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0159 - acc: 0.1400 - val_loss: 0.0236 - val_acc: 0.1484\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0156 - acc: 0.1397 - val_loss: 0.0236 - val_acc: 0.1484\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0153 - acc: 0.1401 - val_loss: 0.0236 - val_acc: 0.1490\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0152 - acc: 0.1400 - val_loss: 0.0242 - val_acc: 0.1490\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0150 - acc: 0.1399 - val_loss: 0.0233 - val_acc: 0.1484\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 30s 3ms/step - loss: 0.0145 - acc: 0.1401 - val_loss: 0.0236 - val_acc: 0.1490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8617c62588>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_1 = Sequential()\n",
    "rnn_1.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "rnn_1.add(LSTM(20))\n",
    "#Regression\n",
    "rnn_1.add(Dense(1,name='out_layer'))\n",
    "rnn_1.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "rnn_1.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#Classification\n",
    "#rnn_1.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),name='out_layer'))\n",
    "#rnn_1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#rnn_1.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 52s 6ms/step - loss: 0.0682 - acc: 0.1006 - val_loss: 0.0616 - val_acc: 0.1105\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 48s 5ms/step - loss: 0.0542 - acc: 0.1116 - val_loss: 0.0568 - val_acc: 0.1130\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0517 - acc: 0.1120 - val_loss: 0.0553 - val_acc: 0.1137\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 48s 5ms/step - loss: 0.0500 - acc: 0.1131 - val_loss: 0.0541 - val_acc: 0.1162\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 48s 5ms/step - loss: 0.0484 - acc: 0.1152 - val_loss: 0.0527 - val_acc: 0.1169\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 52s 6ms/step - loss: 0.0469 - acc: 0.1191 - val_loss: 0.0505 - val_acc: 0.1214\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0442 - acc: 0.1235 - val_loss: 0.0463 - val_acc: 0.1310\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0402 - acc: 0.1289 - val_loss: 0.0432 - val_acc: 0.1374\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0368 - acc: 0.1320 - val_loss: 0.0417 - val_acc: 0.1374\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0342 - acc: 0.1343 - val_loss: 0.0353 - val_acc: 0.1419\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0296 - acc: 0.1359 - val_loss: 0.0292 - val_acc: 0.1458\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0264 - acc: 0.1373 - val_loss: 0.0285 - val_acc: 0.1464\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0236 - acc: 0.1380 - val_loss: 0.0252 - val_acc: 0.1484\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0241 - acc: 0.1380 - val_loss: 0.0247 - val_acc: 0.1484\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0222 - acc: 0.1387 - val_loss: 0.0242 - val_acc: 0.1484\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0213 - acc: 0.1387 - val_loss: 0.0238 - val_acc: 0.1484\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0201 - acc: 0.1390 - val_loss: 0.0225 - val_acc: 0.1490\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0197 - acc: 0.1391 - val_loss: 0.0220 - val_acc: 0.1496\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0191 - acc: 0.1392 - val_loss: 0.0221 - val_acc: 0.1496\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 52s 6ms/step - loss: 0.0186 - acc: 0.1392 - val_loss: 0.0214 - val_acc: 0.1496\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0182 - acc: 0.1393 - val_loss: 0.0218 - val_acc: 0.1496\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0179 - acc: 0.1394 - val_loss: 0.0210 - val_acc: 0.1496\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 48s 5ms/step - loss: 0.0178 - acc: 0.1393 - val_loss: 0.0209 - val_acc: 0.1496\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 48s 5ms/step - loss: 0.0173 - acc: 0.1394 - val_loss: 0.0210 - val_acc: 0.1496\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 48s 5ms/step - loss: 0.0171 - acc: 0.1394 - val_loss: 0.0211 - val_acc: 0.1496\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0169 - acc: 0.1392 - val_loss: 0.0209 - val_acc: 0.1496\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0172 - acc: 0.1396 - val_loss: 0.0212 - val_acc: 0.1490\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0165 - acc: 0.1394 - val_loss: 0.0205 - val_acc: 0.1496\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0163 - acc: 0.1396 - val_loss: 0.0203 - val_acc: 0.1496\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0163 - acc: 0.1394 - val_loss: 0.0201 - val_acc: 0.1496\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0160 - acc: 0.1395 - val_loss: 0.0216 - val_acc: 0.1496\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0158 - acc: 0.1395 - val_loss: 0.0202 - val_acc: 0.1496\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0156 - acc: 0.1394 - val_loss: 0.0200 - val_acc: 0.1496\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0159 - acc: 0.1394 - val_loss: 0.0200 - val_acc: 0.1496\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 49s 6ms/step - loss: 0.0154 - acc: 0.1395 - val_loss: 0.0204 - val_acc: 0.1496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f858d3cec18>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_2 = Sequential()\n",
    "rnn_2.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "rnn_2.add(LSTM(20, return_sequences=True))\n",
    "rnn_2.add(LSTM(20))\n",
    "#regression\n",
    "rnn_2.add(Dense(1,name='out_layer'))\n",
    "rnn_2.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "rnn_2.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#Classification\n",
    "#rnn_2.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),name='out_layer'))\n",
    "#rnn_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#rnn_2.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 75s 8ms/step - loss: 0.1349 - acc: 0.0781 - val_loss: 0.0662 - val_acc: 0.1130\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0567 - acc: 0.1121 - val_loss: 0.0577 - val_acc: 0.1130\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0544 - acc: 0.1120 - val_loss: 0.0574 - val_acc: 0.1130\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 71s 8ms/step - loss: 0.0535 - acc: 0.1122 - val_loss: 0.0565 - val_acc: 0.1130\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0520 - acc: 0.1121 - val_loss: 0.0543 - val_acc: 0.1137\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0491 - acc: 0.1152 - val_loss: 0.0517 - val_acc: 0.1233\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0465 - acc: 0.1203 - val_loss: 0.0498 - val_acc: 0.1259\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0437 - acc: 0.1242 - val_loss: 0.0469 - val_acc: 0.1310\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0389 - acc: 0.1297 - val_loss: 0.0403 - val_acc: 0.1394\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0336 - acc: 0.1351 - val_loss: 0.0320 - val_acc: 0.1419\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0280 - acc: 0.1369 - val_loss: 0.0291 - val_acc: 0.1471\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 70s 8ms/step - loss: 0.0266 - acc: 0.1373 - val_loss: 0.0275 - val_acc: 0.1464\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 70s 8ms/step - loss: 0.0238 - acc: 0.1384 - val_loss: 0.0255 - val_acc: 0.1452\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 72s 8ms/step - loss: 0.0225 - acc: 0.1387 - val_loss: 0.0249 - val_acc: 0.1477\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 70s 8ms/step - loss: 0.0218 - acc: 0.1385 - val_loss: 0.0244 - val_acc: 0.1477\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0220 - acc: 0.1384 - val_loss: 0.0245 - val_acc: 0.1484\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0209 - acc: 0.1390 - val_loss: 0.0233 - val_acc: 0.1484\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0199 - acc: 0.1394 - val_loss: 0.0240 - val_acc: 0.1471\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0202 - acc: 0.1392 - val_loss: 0.0250 - val_acc: 0.1471\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0200 - acc: 0.1390 - val_loss: 0.0224 - val_acc: 0.1484\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0196 - acc: 0.1392 - val_loss: 0.0227 - val_acc: 0.1490\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0188 - acc: 0.1394 - val_loss: 0.0213 - val_acc: 0.1490\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0186 - acc: 0.1393 - val_loss: 0.0221 - val_acc: 0.1490\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0181 - acc: 0.1395 - val_loss: 0.0211 - val_acc: 0.1490\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 71s 8ms/step - loss: 0.0178 - acc: 0.1395 - val_loss: 0.0211 - val_acc: 0.1490\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0179 - acc: 0.1394 - val_loss: 0.0209 - val_acc: 0.1490\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 70s 8ms/step - loss: 0.0176 - acc: 0.1394 - val_loss: 0.0223 - val_acc: 0.1490\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0173 - acc: 0.1395 - val_loss: 0.0205 - val_acc: 0.1490\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0173 - acc: 0.1394 - val_loss: 0.0206 - val_acc: 0.1490\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0174 - acc: 0.1394 - val_loss: 0.0204 - val_acc: 0.1490\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 70s 8ms/step - loss: 0.0170 - acc: 0.1394 - val_loss: 0.0219 - val_acc: 0.1490\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 71s 8ms/step - loss: 0.0188 - acc: 0.1392 - val_loss: 0.0228 - val_acc: 0.1484\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0198 - acc: 0.1392 - val_loss: 0.0216 - val_acc: 0.1490\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0174 - acc: 0.1395 - val_loss: 0.0205 - val_acc: 0.1490\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0169 - acc: 0.1395 - val_loss: 0.0204 - val_acc: 0.1490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8581b78780>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_3 = Sequential()\n",
    "rnn_3.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "rnn_3.add(LSTM(20, return_sequences=True))\n",
    "rnn_3.add(LSTM(20, return_sequences=True))\n",
    "rnn_3.add(LSTM(20))\n",
    "#Regression\n",
    "rnn_3.add(Dense(1,name='out_layer'))\n",
    "rnn_3.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "rnn_3.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#Classification\n",
    "#rnn_3.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),name='out_layer'))\n",
    "#rnn_3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#rnn_3.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN1    0.652702\n",
       "RNN2    0.656994\n",
       "RNN3    0.680003\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_pred1 = rnn_1.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))\n",
    "rnn_pred2 = rnn_2.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))\n",
    "rnn_pred3 = rnn_3.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))\n",
    "\n",
    "#Uncomment for classification case with cat_ce loss\n",
    "#rnn_pred1 = np.argmax(rnn_pred1,axis=1)\n",
    "#rnn_pred2 = np.argmax(rnn_pred2,axis=1)\n",
    "#rnn_pred3 = np.argmax(rnn_pred3,axis=1)\n",
    "\n",
    "RNN_pred_frame = pd.DataFrame({'actual':np.asarray(test_sa_y_class.adjusted_domain1_score),\n",
    "                           'dataset': np.asarray(test_sa_y_class.essay_set),\n",
    "                           'RNN1':rnn_pred1.flatten(),\n",
    "                          'RNN2':rnn_pred2.flatten(),\n",
    "                          'RNN3':rnn_pred3.flatten()})\n",
    "\n",
    "rnn_scoring = RNN_pred_frame.merge(DivSeries, on='dataset')\n",
    "\n",
    "for colName in ['actual', 'RNN1', 'RNN2', 'RNN3']:\n",
    "    #Uncomment for classification case with cat_ce loss\n",
    "    #if(colName != 'actual'):\n",
    "     #   rnn_scoring[colName] = rnn_scoring[colName].apply(RetrieveNormalize)\n",
    "    rnn_scoring[colName] = rnn_scoring[colName] * rnn_scoring['div']\n",
    "    rnn_scoring[colName] = rnn_scoring[colName].apply(round)\n",
    "\n",
    "rnn_scoring.actual = rnn_scoring.actual.astype(int)\n",
    "\n",
    "QuadKappaCalculation = pd.DataFrame(columns = ['RNN1', 'RNN2', 'RNN3'],index = np.unique(rnn_scoring.dataset))\n",
    "for essaySetValue in np.unique(rnn_scoring.dataset):\n",
    "    temp_RNN_ES = rnn_scoring[rnn_scoring.dataset == essaySetValue]\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'RNN1'] = cohen_kappa_score(temp_RNN_ES.actual, temp_RNN_ES.RNN1.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'RNN2'] = cohen_kappa_score(temp_RNN_ES.actual, temp_RNN_ES.RNN2.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'RNN3'] = cohen_kappa_score(temp_RNN_ES.actual, temp_RNN_ES.RNN3.apply(round),weights='quadratic')\n",
    "QuadKappaCalculation.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN RMSE, Cohen, Quad Cohen, accuracy: 1.9465112404620202, 0.39328857697456265, 0.9761170303301328, 0.46802773497688754\n",
      "RNN2 RMSE, Cohen, Quad Cohen, accuracy: 1.7253659088032314, 0.4429170385173772, 0.9807697388568016, 0.512326656394453\n",
      "RNN3 RMSE, Cohen, Quad Cohen, accuracy: 1.6757584580903435, 0.4519293159349017, 0.9823337336873228, 0.522342064714946\n"
     ]
    }
   ],
   "source": [
    "print(\"RNN RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(rnn_scoring.actual, rnn_scoring.RNN1.apply(round))),\n",
    "                                       cohen_kappa_score(rnn_scoring.actual, rnn_scoring.RNN1.apply(round)),\n",
    "                                        cohen_kappa_score(rnn_scoring.actual, rnn_scoring.RNN1.apply(round),weights='quadratic'),\n",
    "                                                 accuracy_score(rnn_scoring.actual, rnn_scoring.RNN1.apply(round))))\n",
    "\n",
    "print(\"RNN2 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(rnn_scoring.actual, rnn_scoring.RNN2.apply(round))),\n",
    "                                       cohen_kappa_score(rnn_scoring.actual, rnn_scoring.RNN2.apply(round)),\n",
    "                                        cohen_kappa_score(rnn_scoring.actual, rnn_scoring.RNN2.apply(round),weights='quadratic'),\n",
    "                                                 accuracy_score(rnn_scoring.actual, rnn_scoring.RNN2.apply(round))))\n",
    "\n",
    "print(\"RNN3 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(rnn_scoring.actual, rnn_scoring.RNN3.apply(round))),\n",
    "                                       cohen_kappa_score(rnn_scoring.actual, rnn_scoring.RNN3.apply(round)),\n",
    "                                        cohen_kappa_score(rnn_scoring.actual, rnn_scoring.RNN3.apply(round),weights='quadratic'),             \n",
    "                                      accuracy_score(rnn_scoring.actual, rnn_scoring.RNN3.apply(round))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation split by dataset - top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  UnweightedKappa QuadKappa  Accuracy  UnweightedRMSE\n",
      "1         0.45316   0.45316  0.945355        0.079591\n",
      "2        0.491156  0.491156  0.919786        0.116014\n",
      "3        0.611015  0.611015  0.814371        0.192485\n",
      "4         0.72878   0.72878  0.866667        0.181767\n",
      "5        0.573857  0.573857  0.782609        0.141832\n",
      "6        0.638259  0.638259  0.849206        0.136209\n",
      "7        0.631245  0.631245  0.877483        0.122134\n",
      "8               0         0  0.963504        0.098858\n"
     ]
    }
   ],
   "source": [
    "preds_to_analyze = rnn_3.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))\n",
    "\n",
    "byset_analysis = pd.DataFrame({'actual':np.asarray(test_sa_y_class.adjusted_domain1_score),\n",
    "                           'dataset': np.asarray(test_sa_y_class.essay_set),\n",
    "                          'RNN3':preds_to_analyze.flatten()})\n",
    "\n",
    "preds_to_analyze = byset_analysis.merge(DivSeries, on='dataset')\n",
    "\n",
    "for colName in ['actual', 'RNN3']:\n",
    "    newColName = colName + '_adjusted'\n",
    "    preds_to_analyze[newColName] = preds_to_analyze[colName] * preds_to_analyze['div']\n",
    "    preds_to_analyze[newColName] = preds_to_analyze[colName].apply(round)\n",
    "\n",
    "preds_to_analyze['right'] = (preds_to_analyze.RNN3_adjusted == preds_to_analyze.actual_adjusted)\n",
    "preds_to_analyze['delta'] = (preds_to_analyze.RNN3_adjusted - preds_to_analyze.actual_adjusted)**2\n",
    "\n",
    "aggregates = pd.concat([preds_to_analyze.groupby('dataset')['right'].sum(),\n",
    "           preds_to_analyze.groupby('dataset')['actual'].count(),\n",
    "                        preds_to_analyze.groupby('dataset')['delta'].mean()],axis=1)\n",
    "aggregates.columns = ['correct', 'test_samples','RMSE']\n",
    "\n",
    "aggregates = pd.DataFrame(index=range(1,9),columns = ['UnweightedKappa', 'QuadKappa', 'Accuracy'])\n",
    "\n",
    "for i in aggregates.index:\n",
    "    temp = preds_to_analyze[preds_to_analyze.dataset == i]\n",
    "    aggregates.loc[i,'UnweightedKappa'] = cohen_kappa_score(temp.actual_adjusted, temp.RNN3_adjusted.apply(round))\n",
    "    aggregates.loc[i,'QuadKappa'] = cohen_kappa_score(temp.actual_adjusted, temp.RNN3_adjusted.apply(round),weights='quadratic')\n",
    "    aggregates.loc[i,'Accuracy'] = accuracy_score(temp.actual_adjusted, temp.RNN3_adjusted.apply(round))\n",
    "    aggregates.loc[i,'UnweightedRMSE'] = mean_squared_error(temp.actual, temp.RNN3)\n",
    "aggregates.UnweightedRMSE = aggregates.UnweightedRMSE.apply(sqrt)\n",
    "\n",
    "print(aggregates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Manual Features with Network Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nishray/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "import spacy\n",
    "from spacy.attrs import ORTH\n",
    "#import textacy\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_lg',disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_obj = pd.Series(train_sa_x_class).apply(lambda essay: nlp(essay.lower()))\n",
    "test_x_obj = pd.Series(test_sa_x_class).apply(lambda essay: nlp(essay.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_height(root):\n",
    "    \"\"\"\n",
    "    Find the maximum depth (height) of the dependency parse of a spacy sentence by starting with its root\n",
    "    Code adapted from https://stackoverflow.com/questions/35920826/how-to-find-height-for-non-binary-tree\n",
    "    :param root: spacy.tokens.token.Token\n",
    "    :return: int, maximum height of sentence's dependency parse tree\n",
    "    \"\"\"\n",
    "    if not list(root.children):\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 + max(tree_height(x) for x in root.children)\n",
    "    \n",
    "def get_average_heights(paragraph):\n",
    "    \"\"\"\n",
    "    Computes average height of parse trees for each sentence in paragraph.\n",
    "    :param paragraph: spacy doc object or str\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    if type(paragraph) == str:\n",
    "        doc = nlp(paragraph)\n",
    "    else:\n",
    "        doc = paragraph\n",
    "    roots = [sent.root for sent in doc.sents]\n",
    "    return np.mean([tree_height(root) for root in roots])\n",
    "\n",
    "def get_variance_heights(paragraph):\n",
    "    \"\"\"\n",
    "    Computes average height of parse trees for each sentence in paragraph.\n",
    "    :param paragraph: spacy doc object or str\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    if type(paragraph) == str:\n",
    "        doc = nlp(paragraph)\n",
    "    else:\n",
    "        doc = paragraph\n",
    "    roots = [sent.root for sent in doc.sents]\n",
    "    return np.std([tree_height(root) for root in roots])\n",
    "\n",
    "def get_tree_heights(paragraph):\n",
    "    \"\"\"\n",
    "    Computes average height of parse trees for each sentence in paragraph.\n",
    "    :param paragraph: spacy doc object or str\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    if type(paragraph) == str:\n",
    "        doc = nlp(paragraph)\n",
    "    else:\n",
    "        doc = paragraph\n",
    "    roots = [sent.root for sent in doc.sents]\n",
    "    return [tree_height(root) for root in roots]\n",
    "\n",
    "def get_sentences(doc):\n",
    "    sents = list(doc.sents)\n",
    "    return sents\n",
    "\n",
    "def get_sentence_lengths(sentences):\n",
    "    return float(len(sentences))\n",
    "\n",
    "def get_word_counts(doc):\n",
    "    return doc.count_by(ORTH)\n",
    "\n",
    "def get_connectives(doc):\n",
    "    text = doc.text.lower()\n",
    "    connectives = [\n",
    "    'after',\n",
    "    'earlier',\n",
    "    'before',\n",
    "    'during',\n",
    "    'while',\n",
    "    'later',\n",
    "    'because',\n",
    "    'consequently',\n",
    "    'thus',\n",
    "    'both',\n",
    "    'additionally',\n",
    "    'furthermore',\n",
    "    'moreover',\n",
    "    'actually',\n",
    "    'as a result',\n",
    "    'due to',\n",
    "    'but',\n",
    "    'yet',\n",
    "    'however',\n",
    "    'although',\n",
    "    'nevertheless'\n",
    "    ]\n",
    "    total = 0\n",
    "    for connector in connectives:\n",
    "        total += text.count(connector)\n",
    "    return float((total/len(doc)))\n",
    "\n",
    "def get_pos(doc):\n",
    "    return [token.pos_ for token in doc]\n",
    "\n",
    "\n",
    "def get_posngrams(poslist,n):\n",
    "    posngrams = []\n",
    "    for item in range(len(poslist) - n + 1):\n",
    "        posngrams.append(tuple([poslist[item+i] for i in range(n)]))\n",
    "    return posngrams\n",
    "\n",
    "def get_posgrams_counts(list_grams):\n",
    "    posgrams_counts = defaultdict(int)\n",
    "    for gram in list_grams:\n",
    "        posgrams_counts[gram] += 1\n",
    "    return posgrams_counts\n",
    "\n",
    "def get_TF(list_dicts):\n",
    "    TF_dict = defaultdict(int)\n",
    "    for dictionary in list_dicts:\n",
    "        for gram in dictionary:\n",
    "            TF_dict[gram] += dictionary[gram]\n",
    "    return TF_dict\n",
    "\n",
    "def get_mean_tfTF(posgram_counts,TF):\n",
    "    tfTF_ratios = list()\n",
    "    for key, value in posgram_counts.items():\n",
    "        tfTF_ratios.append(value/TF[key])\n",
    "    return np.mean(tfTF_ratios)\n",
    "\n",
    "def get_posngram_ratio(posngrams):\n",
    "    if len(posngrams) > 0:\n",
    "        return float(len(set(posngrams))/len(posngrams))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_reading_scores(doc):\n",
    "    scores = textacy.TextStats(doc).readability_stats\n",
    "    del scores['smog_index']\n",
    "    return scores\n",
    "\n",
    "def get_word_lengths(doc):\n",
    "    lengths = list()\n",
    "    for word in doc:\n",
    "        if word.is_alpha:\n",
    "            lengths.append(float(len(word)))\n",
    "    return lengths\n",
    "\n",
    "def get_words_of_length(lengths, n, p):\n",
    "    count = 0\n",
    "    for length in lengths:\n",
    "        if length > n and length < p:\n",
    "            count += 1\n",
    "    return float(count)\n",
    "\n",
    "def get_type_token_ratio(doc):\n",
    "    unique_words = set(word for word in doc if word.is_alpha)\n",
    "    total_words = [word for word in doc if word.is_alpha]\n",
    "    return float(len(unique_words)/len(total_words))\n",
    "\n",
    "def get_similarity_scores(doc):\n",
    "    sents = [sent for sent in doc.sents]\n",
    "    similarity_scores = list()\n",
    "    for i in range(1,len(sents)):\n",
    "        sent1 = sents[i-1]\n",
    "        sent2 = sents[i]\n",
    "        similarity_scores.append(sent1.similarity(sent2))\n",
    "    return np.mean(similarity_scores)\n",
    "\n",
    "def nth_root(x,n):\n",
    "    return x ** (1/float(n))\n",
    "\n",
    "def get_yules_k(word_counts):\n",
    "    m1 =  sum(word_counts.values())\n",
    "    m2 = sum([freq ** 2 for freq in word_counts.values()])\n",
    "    if m1 == m2:\n",
    "        k = 0 \n",
    "    else:\n",
    "        i = (m1*m1) / (m2-m1)\n",
    "        k = 1/i * 10000\n",
    "        return float(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_obj = pd.DataFrame(test_x_obj)\n",
    "test_x_obj['essay'] = test_sa_x_class\n",
    "test_x_obj.columns = ['doc', 'essay']\n",
    "\n",
    "train_x_obj = pd.DataFrame(train_x_obj)\n",
    "train_x_obj['essay'] = train_sa_x_class\n",
    "train_x_obj.columns = ['doc', 'essay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nishray/anaconda3/envs/w266Env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Preengineering\n",
    "train_x_obj['sentences'] = train_x_obj.doc.apply(get_sentences)\n",
    "train_x_obj['word_counts'] = train_x_obj.doc.apply(get_word_counts)\n",
    "train_x_obj['word_lengths'] = train_x_obj.doc.apply(get_word_lengths)\n",
    "train_x_obj['pos'] = train_x_obj.doc.apply(get_pos)\n",
    "train_x_obj['pos_trigrams'] = train_x_obj.pos.apply(lambda pos: get_posngrams(pos, n=3))\n",
    "train_x_obj['pos_4grams'] = train_x_obj.pos.apply(lambda pos: get_posngrams(pos, n=4))\n",
    "train_x_obj['pos_trigram_counts'] = train_x_obj.pos_trigrams.apply(get_posgrams_counts)\n",
    "pos_TF = get_TF(train_x_obj.pos_trigram_counts)\n",
    "train_x_obj['tree_heights'] = train_x_obj.doc.apply(lambda doc: get_tree_heights(doc))\n",
    "\n",
    "# Lexical Features\n",
    "train_x_obj['words_length_4'] = train_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 4,6))\n",
    "train_x_obj['words_length_6'] = train_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 6,8))\n",
    "train_x_obj['words_length_8'] = train_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 8,10))\n",
    "train_x_obj['words_length_10'] = train_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 10,12))\n",
    "train_x_obj['words_length_12'] = train_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 12,100))\n",
    "train_x_obj['mean_word_length'] = train_x_obj.word_lengths.apply(np.mean)\n",
    "train_x_obj['variance_word_length'] = train_x_obj.word_lengths.apply(np.std)\n",
    "train_x_obj['type_token_ratio'] = train_x_obj.doc.apply(get_type_token_ratio)\n",
    "\n",
    "\n",
    "# Length Features\n",
    "train_x_obj['essay_length'] = train_x_obj.doc.apply(len)\n",
    "train_x_obj['num_words'] = train_x_obj.doc.apply(lambda doc: float(len([word for word in doc if word.is_alpha])))\n",
    "train_x_obj['num_sentences'] = train_x_obj.sentences.apply(get_sentence_lengths)\n",
    "train_x_obj['mean_sentence_length'] = train_x_obj.num_words/train_x_obj.num_sentences\n",
    "train_x_obj['num_characters'] = train_x_obj.essay.apply(len)\n",
    "train_x_obj['fourth_root_num_characters'] = train_x_obj.num_characters.apply(nth_root, n=4)\n",
    "\n",
    "# # Occurrence Features\n",
    "train_x_obj['num_commas'] = train_x_obj.essay.apply(lambda essay: float(essay.count(',')))\n",
    "train_x_obj['num_periods'] = train_x_obj.essay.apply(lambda essay: float(essay.count('.')))\n",
    "train_x_obj['num_exclaim'] = train_x_obj.essay.apply(lambda essay: float(essay.count('!')))\n",
    "train_x_obj['num_question'] = train_x_obj.essay.apply(lambda essay: float(essay.count('?')))\n",
    "train_x_obj['num_semicolon'] = train_x_obj.essay.apply(lambda essay: float(essay.count(';')))\n",
    "train_x_obj['num_colon'] = train_x_obj.essay.apply(lambda essay: float(essay.count(':')))\n",
    "\n",
    "# # Style Features\n",
    "# FIX train_x_obj['vocabulary'] = train_x_obj.word_tokens.apply(lambda word_tokens: set(word.lower() for word in word_tokens if word.isalpha()))\n",
    "train_x_obj['vocab_size'] = train_x_obj.word_counts.apply(len)\n",
    "# train_x_obj['yules_k'] = train_x_obj.word_counts.apply(get_yules_k)\n",
    "\n",
    "# # Syntactical Features\n",
    "# # the number for these lengths comes from Chen and He 2013\n",
    "train_x_obj['sentence_lengths'] = train_x_obj.sentences.apply(lambda sentences: [len(sent) for sent in sentences])\n",
    "train_x_obj['very_short_sentences'] = train_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length <= 10 for length in sentence_lengths])))\n",
    "train_x_obj['short_sentences'] = train_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 10 and length <18 for length in sentence_lengths])))\n",
    "train_x_obj['medium_sentences'] = train_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 18 and length <25 for length in sentence_lengths])))\n",
    "train_x_obj['long_sentences'] = train_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 25 for length in sentence_lengths])))\n",
    "train_x_obj['variance_sentence_length'] = train_x_obj.sentence_lengths.apply(lambda sentence_lengths: np.std(sentence_lengths))\n",
    "\n",
    "train_x_obj['max_height'] = train_x_obj.tree_heights.apply(lambda heights: float(max(heights)))\n",
    "train_x_obj['sum_heights'] = train_x_obj.tree_heights.apply(sum)\n",
    "train_x_obj['mean_heights'] = train_x_obj.tree_heights.apply(np.mean)\n",
    "\n",
    "# train_x_obj['mean_sentence_similarity'] = train_x_obj.doc.apply(get_similarity_scores)\n",
    "\n",
    "# # POS Ngrams\n",
    "train_x_obj['pos_trigram_ratio'] = train_x_obj.pos_trigrams.apply(get_posngram_ratio)\n",
    "train_x_obj['pos_fourgram_ratio'] = train_x_obj.pos_4grams.apply(get_posngram_ratio)\n",
    "train_x_obj['mean_trigram_tfTF'] = train_x_obj.pos_trigram_counts.apply(lambda pos_trigram_counts: get_mean_tfTF(pos_trigram_counts, TF=pos_TF))\n",
    "\n",
    "# # Cohesion Features\n",
    "train_x_obj['connectives'] = train_x_obj.doc.apply(get_connectives)\n",
    "\n",
    "# Readability Features\n",
    "#train_x_obj['reading_scores'] = train_x_obj.doc.apply(get_reading_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preengineering\n",
    "test_x_obj['sentences'] = test_x_obj.doc.apply(get_sentences)\n",
    "test_x_obj['word_counts'] = test_x_obj.doc.apply(get_word_counts)\n",
    "test_x_obj['word_lengths'] = test_x_obj.doc.apply(get_word_lengths)\n",
    "test_x_obj['pos'] = test_x_obj.doc.apply(get_pos)\n",
    "test_x_obj['pos_trigrams'] = test_x_obj.pos.apply(lambda pos: get_posngrams(pos, n=3))\n",
    "test_x_obj['pos_4grams'] = test_x_obj.pos.apply(lambda pos: get_posngrams(pos, n=4))\n",
    "test_x_obj['pos_trigram_counts'] = test_x_obj.pos_trigrams.apply(get_posgrams_counts)\n",
    "pos_TF = get_TF(test_x_obj.pos_trigram_counts)\n",
    "test_x_obj['tree_heights'] = test_x_obj.doc.apply(lambda doc: get_tree_heights(doc))\n",
    "\n",
    "# Lexical Features\n",
    "test_x_obj['words_length_4'] = test_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 4,6))\n",
    "test_x_obj['words_length_6'] = test_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 6,8))\n",
    "test_x_obj['words_length_8'] = test_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 8,10))\n",
    "test_x_obj['words_length_10'] = test_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 10,12))\n",
    "test_x_obj['words_length_12'] = test_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 12,100))\n",
    "test_x_obj['mean_word_length'] = test_x_obj.word_lengths.apply(np.mean)\n",
    "test_x_obj['variance_word_length'] = test_x_obj.word_lengths.apply(np.std)\n",
    "test_x_obj['type_token_ratio'] = test_x_obj.doc.apply(get_type_token_ratio)\n",
    "\n",
    "\n",
    "# Length Features\n",
    "test_x_obj['essay_length'] = test_x_obj.doc.apply(len)\n",
    "test_x_obj['num_words'] = test_x_obj.doc.apply(lambda doc: float(len([word for word in doc if word.is_alpha])))\n",
    "test_x_obj['num_sentences'] = test_x_obj.sentences.apply(get_sentence_lengths)\n",
    "test_x_obj['mean_sentence_length'] = test_x_obj.num_words/test_x_obj.num_sentences\n",
    "test_x_obj['num_characters'] = test_x_obj.essay.apply(len)\n",
    "test_x_obj['fourth_root_num_characters'] = test_x_obj.num_characters.apply(nth_root, n=4)\n",
    "\n",
    "# # Occurrence Features\n",
    "test_x_obj['num_commas'] = test_x_obj.essay.apply(lambda essay: float(essay.count(',')))\n",
    "test_x_obj['num_periods'] = test_x_obj.essay.apply(lambda essay: float(essay.count('.')))\n",
    "test_x_obj['num_exclaim'] = test_x_obj.essay.apply(lambda essay: float(essay.count('!')))\n",
    "test_x_obj['num_question'] = test_x_obj.essay.apply(lambda essay: float(essay.count('?')))\n",
    "test_x_obj['num_semicolon'] = test_x_obj.essay.apply(lambda essay: float(essay.count(';')))\n",
    "test_x_obj['num_colon'] = test_x_obj.essay.apply(lambda essay: float(essay.count(':')))\n",
    "\n",
    "# # Style Features\n",
    "# FIX test_x_obj['vocabulary'] = test_x_obj.word_tokens.apply(lambda word_tokens: set(word.lower() for word in word_tokens if word.isalpha()))\n",
    "test_x_obj['vocab_size'] = test_x_obj.word_counts.apply(len)\n",
    "# test_x_obj['yules_k'] = test_x_obj.word_counts.apply(get_yules_k)\n",
    "\n",
    "# # Syntactical Features\n",
    "# # the number for these lengths comes from Chen and He 2013\n",
    "test_x_obj['sentence_lengths'] = test_x_obj.sentences.apply(lambda sentences: [len(sent) for sent in sentences])\n",
    "test_x_obj['very_short_sentences'] = test_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length <= 10 for length in sentence_lengths])))\n",
    "test_x_obj['short_sentences'] = test_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 10 and length <18 for length in sentence_lengths])))\n",
    "test_x_obj['medium_sentences'] = test_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 18 and length <25 for length in sentence_lengths])))\n",
    "test_x_obj['long_sentences'] = test_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 25 for length in sentence_lengths])))\n",
    "test_x_obj['variance_sentence_length'] = test_x_obj.sentence_lengths.apply(lambda sentence_lengths: np.std(sentence_lengths))\n",
    "\n",
    "test_x_obj['max_height'] = test_x_obj.tree_heights.apply(lambda heights: float(max(heights)))\n",
    "test_x_obj['sum_heights'] = test_x_obj.tree_heights.apply(sum)\n",
    "test_x_obj['mean_heights'] = test_x_obj.tree_heights.apply(np.mean)\n",
    "\n",
    "# test_x_obj['mean_sentence_similarity'] = test_x_obj.doc.apply(get_similarity_scores)\n",
    "\n",
    "# # POS Ngrams\n",
    "test_x_obj['pos_trigram_ratio'] = test_x_obj.pos_trigrams.apply(get_posngram_ratio)\n",
    "test_x_obj['pos_fourgram_ratio'] = test_x_obj.pos_4grams.apply(get_posngram_ratio)\n",
    "test_x_obj['mean_trigram_tfTF'] = test_x_obj.pos_trigram_counts.apply(lambda pos_trigram_counts: get_mean_tfTF(pos_trigram_counts, TF=pos_TF))\n",
    "\n",
    "# # Cohesion Features\n",
    "test_x_obj['connectives'] = test_x_obj.doc.apply(get_connectives)\n",
    "\n",
    "# Readability Features\n",
    "#test_x_obj['reading_scores'] = test_x_obj.doc.apply(get_reading_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "colList = ['words_length_4', 'words_length_6', 'words_length_8', 'words_length_10',\n",
    "       'words_length_12', 'mean_word_length', 'variance_word_length',\n",
    "       'type_token_ratio', 'essay_length', 'num_words', 'num_sentences',\n",
    "       'mean_sentence_length', 'num_characters', 'fourth_root_num_characters',\n",
    "       'num_commas', 'num_periods', 'num_exclaim', 'num_question',\n",
    "       'num_semicolon', 'num_colon', 'vocab_size', 'very_short_sentences',\n",
    "       'short_sentences', 'medium_sentences', 'long_sentences',\n",
    "       'variance_sentence_length', 'max_height', 'sum_heights', 'mean_heights',\n",
    "       'pos_trigram_ratio', 'pos_fourgram_ratio', 'mean_trigram_tfTF',\n",
    "       'connectives']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embed_input (InputLayer)        (None, 588)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embed (Embedding)               (None, 588, 300)     10930500    embed_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "LSTM1 (LSTM)                    (None, 588, 20)      25680       embed[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "LSTM3 (LSTM)                    (None, 588, 20)      3280        LSTM1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ManualFeatureConnected_input (I (None, 33)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "LSTM2 (LSTM)                    (None, 20)           3280        LSTM3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ManualFeatureConnected (Dense)  (None, 25)           850         ManualFeatureConnected_input[0][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 45)           0           LSTM2[0][0]                      \n",
      "                                                                 ManualFeatureConnected[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "out_layer (Dense)               (None, 1)            46          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 10,963,636\n",
      "Trainable params: 33,136\n",
      "Non-trainable params: 10,930,500\n",
      "__________________________________________________________________________________________________\n",
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 212s 24ms/step - loss: 161.5773 - acc: 0.0122 - val_loss: 38.7645 - val_acc: 0.0180\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 202s 23ms/step - loss: 37.2529 - acc: 0.0152 - val_loss: 22.5825 - val_acc: 0.0193\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 204s 23ms/step - loss: 24.0880 - acc: 0.0189 - val_loss: 26.3406 - val_acc: 0.0206\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 204s 23ms/step - loss: 18.6894 - acc: 0.0233 - val_loss: 16.8341 - val_acc: 0.0276\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 210s 24ms/step - loss: 13.4731 - acc: 0.0291 - val_loss: 22.2904 - val_acc: 0.0206\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 215s 24ms/step - loss: 10.8256 - acc: 0.0266 - val_loss: 7.0709 - val_acc: 0.0366\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 213s 24ms/step - loss: 8.7850 - acc: 0.0347 - val_loss: 8.7550 - val_acc: 0.0392\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 211s 24ms/step - loss: 7.4889 - acc: 0.0368 - val_loss: 5.4166 - val_acc: 0.0462\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 211s 24ms/step - loss: 5.8926 - acc: 0.0407 - val_loss: 4.3772 - val_acc: 0.0469\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 202s 23ms/step - loss: 5.3758 - acc: 0.0422 - val_loss: 3.8938 - val_acc: 0.0405\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 199s 22ms/step - loss: 4.9175 - acc: 0.0436 - val_loss: 5.4167 - val_acc: 0.0411\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 202s 23ms/step - loss: 4.2331 - acc: 0.0441 - val_loss: 3.0491 - val_acc: 0.0546\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 200s 23ms/step - loss: 3.7506 - acc: 0.0474 - val_loss: 4.2369 - val_acc: 0.0405\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 201s 23ms/step - loss: 3.5638 - acc: 0.0479 - val_loss: 2.5191 - val_acc: 0.0584\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 204s 23ms/step - loss: 3.1075 - acc: 0.0545 - val_loss: 3.1041 - val_acc: 0.0578\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 202s 23ms/step - loss: 2.8438 - acc: 0.0501 - val_loss: 7.8314 - val_acc: 0.0270\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 205s 23ms/step - loss: 3.1808 - acc: 0.0502 - val_loss: 3.1118 - val_acc: 0.0578\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 203s 23ms/step - loss: 2.7621 - acc: 0.0530 - val_loss: 2.3598 - val_acc: 0.0546\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 205s 23ms/step - loss: 2.6965 - acc: 0.0563 - val_loss: 2.1589 - val_acc: 0.0636\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 205s 23ms/step - loss: 2.2590 - acc: 0.0587 - val_loss: 8.1529 - val_acc: 0.0199\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 207s 23ms/step - loss: 2.1473 - acc: 0.0553 - val_loss: 1.7862 - val_acc: 0.0687\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 205s 23ms/step - loss: 2.2561 - acc: 0.0568 - val_loss: 1.8093 - val_acc: 0.0668\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 207s 23ms/step - loss: 1.9256 - acc: 0.0621 - val_loss: 1.6808 - val_acc: 0.0713\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 208s 24ms/step - loss: 1.6966 - acc: 0.0644 - val_loss: 2.2843 - val_acc: 0.0604\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 214s 24ms/step - loss: 1.6252 - acc: 0.0631 - val_loss: 2.6739 - val_acc: 0.0469\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 213s 24ms/step - loss: 1.5814 - acc: 0.0638 - val_loss: 1.7650 - val_acc: 0.0706\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 210s 24ms/step - loss: 1.4711 - acc: 0.0664 - val_loss: 1.8560 - val_acc: 0.0687\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 212s 24ms/step - loss: 1.5034 - acc: 0.0688 - val_loss: 1.4610 - val_acc: 0.0796\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 214s 24ms/step - loss: 1.3764 - acc: 0.0699 - val_loss: 1.7739 - val_acc: 0.0713\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 213s 24ms/step - loss: 1.3983 - acc: 0.0677 - val_loss: 2.2102 - val_acc: 0.0629\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 212s 24ms/step - loss: 1.3113 - acc: 0.0657 - val_loss: 1.3258 - val_acc: 0.0771\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 213s 24ms/step - loss: 1.2647 - acc: 0.0697 - val_loss: 1.6078 - val_acc: 0.0751\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 212s 24ms/step - loss: 1.1904 - acc: 0.0691 - val_loss: 1.2631 - val_acc: 0.0796\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 210s 24ms/step - loss: 1.0921 - acc: 0.0725 - val_loss: 1.3329 - val_acc: 0.0803\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 213s 24ms/step - loss: 1.0785 - acc: 0.0723 - val_loss: 1.2943 - val_acc: 0.0796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd1106a5048>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "RNN_comb_best = Sequential()\n",
    "RNN_comb_best.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],\n",
    "                       input_length=max_len_class,trainable=False,name='embed'))\n",
    "RNN_comb_best.add(LSTM(20, return_sequences=True,name='LSTM1'))\n",
    "RNN_comb_best.add(LSTM(20, return_sequences=True,name='LSTM3'))\n",
    "RNN_comb_best.add(LSTM(20,name='LSTM2'))\n",
    "\n",
    "Manual_Features_comb = Sequential()\n",
    "Manual_Features_comb.add(Dense(25,input_shape=(len(colList),),name='ManualFeatureConnected'))\n",
    "\n",
    "mergedOut = Concatenate()([RNN_comb_best.output, Manual_Features_comb.output])\n",
    "mergedOut = Dense(1, name='out_layer')(mergedOut)\n",
    "\n",
    "rnn_combined_final = Model([RNN_comb_best.input, Manual_Features_comb.input], mergedOut)\n",
    "rnn_combined_final.compile(optimizer='adagrad',loss='mse', metrics=['accuracy'])\n",
    "rnn_combined_final.summary()\n",
    "\n",
    "\n",
    "rnn_combined_final.fit([sequences_matrix_class, np.array(train_x_obj[colList].fillna(0))],train_sa_y_class.adjusted_domain1_score, batch_size = 100, epochs=35, validation_split=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embed_input (InputLayer)        (None, 588)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embed (Embedding)               (None, 588, 300)     10930500    embed_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 584, 64)      96064       embed[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 292, 64)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "ManualFeatureConnected_input (I (None, 33)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 18688)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "ManualFeatureConnected (Dense)  (None, 25)           850         ManualFeatureConnected_input[0][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 18713)        0           flatten_1[0][0]                  \n",
      "                                                                 ManualFeatureConnected[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "out_layer (Dense)               (None, 1)            18714       concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 11,046,128\n",
      "Trainable params: 115,628\n",
      "Non-trainable params: 10,930,500\n",
      "__________________________________________________________________________________________________\n",
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 25s 3ms/step - loss: 3.8930 - acc: 0.0638 - val_loss: 0.3353 - val_acc: 0.1150\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.1264 - acc: 0.1153 - val_loss: 0.2985 - val_acc: 0.1169\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0856 - acc: 0.1239 - val_loss: 0.2475 - val_acc: 0.1329\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0597 - acc: 0.1302 - val_loss: 0.2343 - val_acc: 0.1317\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0481 - acc: 0.1344 - val_loss: 0.2073 - val_acc: 0.1407\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0401 - acc: 0.1360 - val_loss: 0.2015 - val_acc: 0.1464\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0347 - acc: 0.1368 - val_loss: 0.2025 - val_acc: 0.1355\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0344 - acc: 0.1365 - val_loss: 0.1785 - val_acc: 0.1464\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0320 - acc: 0.1371 - val_loss: 0.1954 - val_acc: 0.1452\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0364 - acc: 0.1370 - val_loss: 0.1639 - val_acc: 0.1464\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0234 - acc: 0.1376 - val_loss: 0.1577 - val_acc: 0.1452\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0211 - acc: 0.1383 - val_loss: 0.1525 - val_acc: 0.1452\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0205 - acc: 0.1385 - val_loss: 0.1519 - val_acc: 0.1426\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0231 - acc: 0.1380 - val_loss: 0.1475 - val_acc: 0.1439\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0182 - acc: 0.1390 - val_loss: 0.1471 - val_acc: 0.1419\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0174 - acc: 0.1393 - val_loss: 0.1501 - val_acc: 0.1471\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0156 - acc: 0.1393 - val_loss: 0.1398 - val_acc: 0.1452\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0138 - acc: 0.1397 - val_loss: 0.1449 - val_acc: 0.1477\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0236 - acc: 0.1387 - val_loss: 0.1358 - val_acc: 0.1477\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0275 - acc: 0.1390 - val_loss: 0.1470 - val_acc: 0.1464\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0227 - acc: 0.1392 - val_loss: 0.1327 - val_acc: 0.1471\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0232 - acc: 0.1399 - val_loss: 0.1423 - val_acc: 0.1394\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.1192 - acc: 0.1305 - val_loss: 0.3064 - val_acc: 0.1329\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.1306 - acc: 0.1276 - val_loss: 0.1451 - val_acc: 0.1362\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0313 - acc: 0.1380 - val_loss: 0.1204 - val_acc: 0.1464\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0186 - acc: 0.1394 - val_loss: 0.1352 - val_acc: 0.1413\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0464 - acc: 0.1380 - val_loss: 0.1451 - val_acc: 0.1368\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0860 - acc: 0.1337 - val_loss: 0.3432 - val_acc: 0.0976\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0541 - acc: 0.1365 - val_loss: 0.1139 - val_acc: 0.1452\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0197 - acc: 0.1393 - val_loss: 0.1099 - val_acc: 0.1471\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0122 - acc: 0.1402 - val_loss: 0.1696 - val_acc: 0.1413\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0894 - acc: 0.1339 - val_loss: 0.4155 - val_acc: 0.1195\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0940 - acc: 0.1319 - val_loss: 0.2615 - val_acc: 0.1015\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0392 - acc: 0.1384 - val_loss: 0.2616 - val_acc: 0.0989\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.2689 - acc: 0.1169 - val_loss: 0.5485 - val_acc: 0.0520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0ebdad3c8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "\n",
    "CNN_comb = Sequential()\n",
    "CNN_comb.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],\n",
    "                       input_length=max_len_class,trainable=False,name='embed'))\n",
    "CNN_comb.add(Conv1D(64, 5, activation = 'relu'))\n",
    "CNN_comb.add(MaxPooling1D(2))\n",
    "CNN_comb.add(Flatten())\n",
    "\n",
    "Manual_Features_comb_cnn = Sequential()\n",
    "Manual_Features_comb_cnn.add(Dense(25,input_shape=(len(colList),),name='ManualFeatureConnected'))\n",
    "\n",
    "mergedOut_cnn = Concatenate()([CNN_comb.output, Manual_Features_comb_cnn.output])\n",
    "mergedOut_cnn = Dense(1, name='out_layer')(mergedOut_cnn)\n",
    "\n",
    "cnn_combined_final = Model([CNN_comb.input, Manual_Features_comb_cnn.input], mergedOut_cnn)\n",
    "cnn_combined_final.compile(optimizer='adam',loss='mse', metrics=['accuracy'])\n",
    "cnn_combined_final.summary()\n",
    "\n",
    "#final_model.summary()\n",
    "#sahil_model_comb.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "cnn_combined_final.fit([sequences_matrix_class, np.array(train_x_obj[colList].fillna(0))],train_sa_y_class.adjusted_domain1_score, batch_size = 100, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_comb_preds = rnn_combined_final.predict([sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class), np.asarray(test_x_obj[colList].fillna(0)).reshape(sequences_test_matrix_class.shape[0],len(colList))])\n",
    "cnn_comb_preds = cnn_combined_final.predict([sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class), np.asarray(test_x_obj[colList].fillna(0)).reshape(sequences_test_matrix_class.shape[0],len(colList))])\n",
    "\n",
    "comb_pred_frame = pd.DataFrame({'RNN':rnn_comb_preds.flatten(),\n",
    "                           'CNN':cnn_comb_preds.flatten(),\n",
    "                           'actual':np.asarray(test_sa_y_class.adjusted_domain1_score), 'dataset': np.asarray(test_sa_y_class.essay_set)})\n",
    "comb_pred_frame = comb_pred_frame.merge(DivSeries, on='dataset')\n",
    "\n",
    "for colName in ['CNN', 'RNN', 'actual']:\n",
    "    comb_pred_frame[colName] = comb_pred_frame[colName] * comb_pred_frame['div']\n",
    "    comb_pred_frame[colName] = comb_pred_frame[colName].apply(round)\n",
    "\n",
    "comb_pred_frame.actual = comb_pred_frame.actual.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN    0.065429\n",
       "RNN    0.128090\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QuadKappaCalculation = pd.DataFrame(columns = ['CNN', 'RNN'],index = np.unique(comb_pred_frame.dataset))\n",
    "for essaySetValue in np.unique(comb_pred_frame.dataset):\n",
    "    temp_comb_ES = comb_pred_frame[comb_pred_frame.dataset == essaySetValue]\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'RNN'] = cohen_kappa_score(temp_comb_ES.actual, temp_comb_ES.RNN.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'CNN'] = cohen_kappa_score(temp_comb_ES.actual, temp_comb_ES.CNN.apply(round),weights='quadratic')\n",
    "QuadKappaCalculation.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN combined RMSE, Cohen, Quad Cohen, accuracy: 16.995728186204698, 0.04440576138623553, -0.42204815395281625, 0.12172573189522343\n",
      "RNN combined RMSE, Cohen, Quad Cohen, accuracy: 29.489244859841666, 0.04736246919148801, 0.47937816314859816, 0.12249614791987673\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN combined RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(comb_pred_frame.actual, comb_pred_frame.CNN.apply(round))),\n",
    "                                       cohen_kappa_score(comb_pred_frame.actual, comb_pred_frame.CNN.apply(round)),\n",
    "                                        cohen_kappa_score(comb_pred_frame.actual, comb_pred_frame.CNN.apply(round),weights='quadratic'),\n",
    "                                                 accuracy_score(comb_pred_frame.actual, comb_pred_frame.CNN.apply(round))))\n",
    "\n",
    "print(\"RNN combined RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(comb_pred_frame.actual, comb_pred_frame.RNN.apply(round))),\n",
    "                                       cohen_kappa_score(comb_pred_frame.actual, comb_pred_frame.RNN.apply(round)),\n",
    "                                        cohen_kappa_score(comb_pred_frame.actual, comb_pred_frame.RNN.apply(round),weights='quadratic'),             \n",
    "                                      accuracy_score(comb_pred_frame.actual, comb_pred_frame.RNN.apply(round))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
