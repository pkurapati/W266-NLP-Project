{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "import json, os, re, shutil, sys, time\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "from collections import defaultdict\n",
    "import xmltodict\n",
    "import untangle\n",
    "import xml.etree.ElementTree as ET\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NumPy, Pandas and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "\n",
    "from numpy.random import seed\n",
    "from pandas import read_csv, DataFrame\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Activation, Dropout, Conv1D, MaxPooling1D, Bidirectional, Flatten, TimeDistributed\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AEG Long Essay Sentence Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "aeg_long = pd.read_csv(\"../data-DNC/AEG/training_set_rel3.tsv\",sep='\\t',encoding = \"latin1\")\n",
    "#aeg_long.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test. We need to do this first to ensure that when we split to \n",
    "# sentence level, we have sentences of a given essay in either training or test but not on both.\n",
    "\n",
    "train_comb,test_comb = train_test_split(aeg_long,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10380, 5)\n",
      "(2596, 5)\n",
      "[ 3567  4233  6518 15174 18855]\n",
      "[ 9908  9872   305 12771  6839]\n"
     ]
    }
   ],
   "source": [
    "# Create train and test data frames with relevant fields\n",
    "\n",
    "x_train_df = train_comb.filter(['essay_id','essay_set','essay','domain1_score'], axis=1)\n",
    "x_test_df = test_comb.filter(['essay_id','essay_set','essay','domain1_score'], axis=1)\n",
    "\n",
    "x_train_df = x_train_df.reset_index()\n",
    "x_test_df = x_test_df.reset_index()\n",
    "print(x_train_df.shape)\n",
    "print(x_test_df.shape)\n",
    "print(x_train_df.essay_id[:5].values)\n",
    "print(x_test_df.essay_id[:5].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each essay set has a different scoring range. We need to normalize the scores to a standard scale for training.\n",
    "def normalize_score(essay):\n",
    "    \"\"\" Normalizes the domain score based on percentage\"\"\"\n",
    "    score = 0\n",
    "    score = float(essay.domain1_score)\n",
    "    essay_set = essay.essay_set\n",
    "    if essay_set == 1:\n",
    "        div = 12\n",
    "    elif essay_set == 2:\n",
    "        div = 5\n",
    "    elif essay_set == 3:\n",
    "        div = 3\n",
    "    elif essay_set == 4:\n",
    "        div = 3\n",
    "    elif essay_set == 5:\n",
    "        div = 4\n",
    "    elif essay_set == 6:\n",
    "        div = 4\n",
    "    elif essay_set == 7:\n",
    "        div = 25\n",
    "    elif essay_set == 8:\n",
    "        div = 50\n",
    "    return score/div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df['Norm_Score'] = x_train_df.apply(normalize_score,axis=1)\n",
    "x_test_df['Norm_Score'] = x_test_df.apply(normalize_score,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split essay into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will use spacy to split the essay into sentences. \n",
    "# Load spacy large english module\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split essay into sentences\n",
    "def create_sentences_df(df):\n",
    "    \"\"\" Function to split essay data into individual sentences. Returns a dataframe\"\"\"\n",
    "    start=datetime.now()\n",
    "    aeg_long_sentence = pd.DataFrame(columns=['essay_id','essay_set','sentence','domain1_score','Norm_Score'])\n",
    "    for i in range(len(df)):\n",
    "        if i%1000 == 0:\n",
    "            print(\"At iteration :\",i)\n",
    "            print(\"Duration: \",datetime.now()-start)\n",
    "        sentence = nlp(df.essay[i])\n",
    "        for s in sentence.sents:\n",
    "            aeg_long_sentence = aeg_long_sentence.append({'essay_id' : df.essay_id[i],\n",
    "                                                          'essay_set' : df.essay_set[i],'sentence' : s.text, \n",
    "                                                          'domain1_score' : df.domain1_score[i],\n",
    "                                                          'Norm_Score' : df.Norm_Score[i]},\n",
    "                                                         ignore_index=True)\n",
    "    return aeg_long_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration : 0\n",
      "Duration:  0:00:00.000932\n",
      "At iteration : 1000\n",
      "Duration:  0:01:14.121585\n",
      "At iteration : 2000\n",
      "Duration:  0:02:45.327758\n",
      "At iteration : 3000\n",
      "Duration:  0:04:51.602966\n",
      "At iteration : 4000\n",
      "Duration:  0:07:44.236425\n",
      "At iteration : 5000\n",
      "Duration:  0:11:15.450161\n",
      "At iteration : 6000\n",
      "Duration:  0:15:42.782550\n",
      "At iteration : 7000\n",
      "Duration:  0:20:41.656108\n",
      "At iteration : 8000\n",
      "Duration:  5:19:13.096613\n",
      "At iteration : 9000\n",
      "Duration:  9:07:48.924929\n",
      "At iteration : 10000\n",
      "Duration:  9:15:03.240570\n"
     ]
    }
   ],
   "source": [
    "# Split train data into sentences\n",
    "x_train_sentence_df = create_sentences_df(x_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration : 0\n",
      "Duration:  0:00:00.001320\n",
      "At iteration : 1000\n",
      "Duration:  0:01:17.885944\n",
      "At iteration : 2000\n",
      "Duration:  0:03:07.950674\n"
     ]
    }
   ],
   "source": [
    "# Split test data into sentences\n",
    "x_test_sentence_df = create_sentences_df(x_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>sentence</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>Norm_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3567</td>\n",
       "      <td>2</td>\n",
       "      <td>There are many types of reading materials for ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3567</td>\n",
       "      <td>2</td>\n",
       "      <td>You can find things on cars, trucks, sports, a...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3567</td>\n",
       "      <td>2</td>\n",
       "      <td>There are some materials in a library though t...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3567</td>\n",
       "      <td>2</td>\n",
       "      <td>But should those materials be removed from the...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3567</td>\n",
       "      <td>2</td>\n",
       "      <td>Some think that they should and others think t...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id essay_set                                           sentence  \\\n",
       "0     3567         2  There are many types of reading materials for ...   \n",
       "1     3567         2  You can find things on cars, trucks, sports, a...   \n",
       "2     3567         2  There are some materials in a library though t...   \n",
       "3     3567         2  But should those materials be removed from the...   \n",
       "4     3567         2  Some think that they should and others think t...   \n",
       "\n",
       "  domain1_score  Norm_Score  \n",
       "0             4         0.8  \n",
       "1             4         0.8  \n",
       "2             4         0.8  \n",
       "3             4         0.8  \n",
       "4             4         0.8  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_sentence_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store this dataset in pickle format so that we don't have to redo the above steps.\n",
    "x_train_sentence_df.to_pickle(\"./x_train_sentence_df.pkl\")\n",
    "x_test_sentence_df.to_pickle(\"./x_test_sentence_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147123, 5)\n",
      "(36778, 5)\n"
     ]
    }
   ],
   "source": [
    "# Open the pickled version\n",
    "x_train_sentence_df = pd.read_pickle(\"./x_train_sentence_df.pkl\")\n",
    "x_test_sentence_df = pd.read_pickle(\"./x_test_sentence_df.pkl\")\n",
    "print(x_train_sentence_df.shape)\n",
    "print(x_test_sentence_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def append_sentence(sid,sentence):\n",
    "#    sentence = sid_dict[sid] + sentence\n",
    "#    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sid_dict = {1:\"One. \",2:\"Two. \",3:\"Three. \",4:\"Four. \",5:\"Five \",6:\"Six. \",7:\"Seven. \",8:\"Eight. \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train_sentence_df['mod_sentence'] = np.vectorize(append_sentence)(x_train_sentence_df['essay_set'],\n",
    "#                                                                    x_train_sentence_df['sentence'])\n",
    "\n",
    "#x_test_sentence_df['mod_sentence'] = np.vectorize(append_sentence)(x_test_sentence_df['essay_set'],\n",
    "#                                                                    x_test_sentence_df['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train_sentence_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147123,)\n",
      "(147123,)\n",
      "(36778,)\n",
      "(36778,)\n"
     ]
    }
   ],
   "source": [
    "# Create train and test text and labels\n",
    "\n",
    "x_train = x_train_sentence_df['sentence'].values\n",
    "y_train = x_train_sentence_df['Norm_Score'].values\n",
    "x_test = x_test_sentence_df['sentence'].values\n",
    "y_test = x_test_sentence_df['Norm_Score'].values\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "train_data = pad_sequences(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len_class = train_data.shape[1]\n",
    "max_words_class = vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36778, 175)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "test_data = pad_sequences(test_seq, maxlen=max_len_class)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('/Users/kurapati/W266/data/glove.42B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common Functions:\n",
    "\n",
    "# Define function to get the multiplication factor. We can get it back from the essay set, but\n",
    "# it is better to do it from the score, because there are scores with value 0, and its MF should be 0\n",
    "\n",
    "def find_mult_factor(x):\n",
    "    \"\"\" Function to find the multiplication factor for denormalizing\"\"\"\n",
    "    if x[1] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.around(x[0]/x[1])\n",
    "    \n",
    "def denormalize(x):\n",
    "    \"\"\" Function to Denormalize the score\"\"\"\n",
    "    return np.around(x[2] * x[3])\n",
    "\n",
    "def find_max_min_mean_score(df):\n",
    "    \"\"\" Function to find the max, min and rounded mean of sentence scores\"\"\"\n",
    "    new_df = pd.DataFrame(columns=['essay_id','essay_set','Orig_Score','Norm_Orig_Score',\n",
    "                                   'Max_Score','Min_Score','Mean_Score','Norm_Mean_Score'])\n",
    "    essay_ids = np.unique(df.essay_id)\n",
    "    for e_id in essay_ids:\n",
    "        df_temp = df[df.essay_id == e_id]\n",
    "        max_score = np.max(df_temp.pred_score)\n",
    "        min_score = np.min(df_temp.pred_score)\n",
    "        # we need to round the mean so that kappa score doesnt complain\n",
    "        mean_score = np.around(np.mean(df_temp.pred_score))\n",
    "        norm_mean_score = np.mean(df_temp.norm_pred_score)\n",
    "        new_df = new_df.append({'essay_id':e_id,'essay_set':int(np.unique(df_temp.essay_set)),\n",
    "                                'Orig_Score':int(np.unique(df_temp.orig_score)),\n",
    "                                'Norm_Orig_Score' : np.unique(df_temp.norm_orig_score),\n",
    "                                'Max_Score':max_score,'Min_Score':min_score,\n",
    "                                'Mean_Score':mean_score,'Norm_Mean_Score':norm_mean_score},ignore_index=True)\n",
    "    return new_df\n",
    "\n",
    "def essay_set_metrics(df):\n",
    "    \"\"\" Calculate per essay set metrics\"\"\"\n",
    "    set_df = pd.DataFrame(columns=['essay_set','RMSE','Kappa','Kappa_Q','Accuracy','Norm_RMSE'])\n",
    "    e_sets = np.unique(df.essay_set)\n",
    "    for e_s in e_sets:\n",
    "        df_s = df[df.essay_set == e_s]\n",
    "        original_score = df_s.Orig_Score.values.astype(int)\n",
    "        norm_original_score = df_s.Norm_Orig_Score.values.astype(float)\n",
    "        predicted_score = df_s.Mean_Score.values.astype(int)\n",
    "        norm_predicted_score = df_s.Norm_Mean_Score.values.astype(float)\n",
    "        rmse = RMSE(original_score,predicted_score)\n",
    "        kappa = cohen_kappa_score(original_score,predicted_score)\n",
    "        kappa_q = cohen_kappa_score(original_score,predicted_score,weights='quadratic')\n",
    "        accuracy = accuracy_score(original_score,predicted_score)\n",
    "        norm_rmse = RMSE(norm_original_score,norm_predicted_score)\n",
    "        set_df = set_df.append({'essay_set':e_s,'RMSE':rmse,'Kappa':kappa,'Kappa_Q':kappa_q,\n",
    "                                'Accuracy':accuracy,'Norm_RMSE':norm_rmse},ignore_index=True)\n",
    "    return set_df\n",
    "\n",
    "\n",
    "def sentence_count(df):\n",
    "    \"\"\" Returns the number of sentences in an essay \"\"\"\n",
    "    essay_count = df.groupby('essay_id').count()\n",
    "    essay_count = essay_count.drop(['sentence','domain1_score','Norm_Score'],axis=1)\n",
    "    essay_count.columns = ['Number_of_Sentences']\n",
    "    return essay_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSE(actual, predict):\n",
    "    diff = actual - predict\n",
    "    diff = sum(diff**2) / len(actual)\n",
    "    return np.sqrt(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model definitions\n",
    "\n",
    "def FF_1_NN():\n",
    "    \"\"\" Simple feed forward NN\"\"\"\n",
    "    model_ff = Sequential()\n",
    "    model_ff.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_ff.add(tf.keras.layers.Flatten())\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_ff.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "    model_ff.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_ff\n",
    "\n",
    "def FF_2_NN():\n",
    "    \"\"\" Simple feed forward NN\"\"\"\n",
    "    model_ff = Sequential()\n",
    "    model_ff.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_ff.add(tf.keras.layers.Flatten())\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_ff.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "    model_ff.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_ff\n",
    "\n",
    "def FF_3_NN():\n",
    "    \"\"\" Simple feed forward NN\"\"\"\n",
    "    model_ff = Sequential()\n",
    "    model_ff.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_ff.add(tf.keras.layers.Flatten())\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_ff.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "    model_ff.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_ff\n",
    "\n",
    "def GRU_32():\n",
    "    \"\"\" Gated Recurrent Unit\"\"\"\n",
    "    model_gru = Sequential()\n",
    "    model_gru.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_gru.add(tf.keras.layers.GRU(32,activation='tanh'))\n",
    "    model_gru.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_gru.add(tf.keras.layers.Dense(1,name='out_layer'))\n",
    "    model_gru.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_gru\n",
    "\n",
    "def GRU_50():\n",
    "    \"\"\" Gated Recurrent Unit\"\"\"\n",
    "    model_gru = Sequential()\n",
    "    model_gru.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_gru.add(tf.keras.layers.GRU(32,activation='tanh'))\n",
    "    model_gru.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_gru.add(tf.keras.layers.Dense(1,name='out_layer'))\n",
    "    model_gru.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_gru\n",
    "\n",
    "def CNN_FF():\n",
    "    \"\"\" CNN with Feed Forward NN \"\"\"\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_conv.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(tf.keras.layers.Flatten())\n",
    "    model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "def CNN_lstm():\n",
    "    \"\"\" CNN with single layer LSTM & Feed Forward NN\"\"\"\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    #model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(tf.keras.layers.LSTM(100))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "def stack_lstm():\n",
    "    \"\"\" Three layered stacked LSTM.\"\"\"\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32,return_sequences=True))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32, return_sequences=True))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.2))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "def stateful_stacked_lstm():\n",
    "    # In stateful, total samples needs to be divisible by batch size\n",
    "    # we have 147026 samples, so selecting 6683 (6683*22=147026)\n",
    "    # The test sample need to be a multiple of 6683 as well\n",
    "    batch_size=2\n",
    "    model_conv = Sequential()\n",
    "    # In stateful, we have to pass batch_input_shape to the first layer\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], \n",
    "                                             trainable=False,batch_input_shape=(batch_size,max_len_class)))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32,stateful=True,return_sequences=True))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    #sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    #model_conv.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_conv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Feed Forward NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FNN 1 Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147123/147123 [==============================] - 39s 267us/step - loss: 0.0436 - acc: 0.0984\n",
      "Epoch 2/20\n",
      "147123/147123 [==============================] - 37s 250us/step - loss: 0.0296 - acc: 0.0989\n",
      "Epoch 3/20\n",
      "147123/147123 [==============================] - 36s 241us/step - loss: 0.0281 - acc: 0.0995\n",
      "Epoch 4/20\n",
      "147123/147123 [==============================] - 35s 237us/step - loss: 0.0270 - acc: 0.1000\n",
      "Epoch 5/20\n",
      "147123/147123 [==============================] - 34s 228us/step - loss: 0.0260 - acc: 0.1004\n",
      "Epoch 6/20\n",
      "147123/147123 [==============================] - 35s 237us/step - loss: 0.0252 - acc: 0.1010\n",
      "Epoch 7/20\n",
      "147123/147123 [==============================] - 34s 229us/step - loss: 0.0242 - acc: 0.1016\n",
      "Epoch 8/20\n",
      "147123/147123 [==============================] - 34s 231us/step - loss: 0.0233 - acc: 0.1019\n",
      "Epoch 9/20\n",
      "147123/147123 [==============================] - 34s 231us/step - loss: 0.0224 - acc: 0.1023\n",
      "Epoch 10/20\n",
      "147123/147123 [==============================] - 37s 250us/step - loss: 0.0215 - acc: 0.1027\n",
      "Epoch 11/20\n",
      "147123/147123 [==============================] - 34s 232us/step - loss: 0.0206 - acc: 0.1029\n",
      "Epoch 12/20\n",
      "147123/147123 [==============================] - 33s 223us/step - loss: 0.0198 - acc: 0.1032\n",
      "Epoch 13/20\n",
      "147123/147123 [==============================] - 32s 218us/step - loss: 0.0190 - acc: 0.1034\n",
      "Epoch 14/20\n",
      "147123/147123 [==============================] - 33s 222us/step - loss: 0.0184 - acc: 0.1037\n",
      "Epoch 15/20\n",
      "147123/147123 [==============================] - 32s 214us/step - loss: 0.0179 - acc: 0.1038\n",
      "Epoch 16/20\n",
      "147123/147123 [==============================] - 32s 220us/step - loss: 0.0173 - acc: 0.1038\n",
      "Epoch 17/20\n",
      "147123/147123 [==============================] - 32s 218us/step - loss: 0.0168 - acc: 0.1041\n",
      "Epoch 18/20\n",
      "147123/147123 [==============================] - 32s 216us/step - loss: 0.0163 - acc: 0.1042\n",
      "Epoch 19/20\n",
      "147123/147123 [==============================] - 32s 219us/step - loss: 0.0159 - acc: 0.1042\n",
      "Epoch 20/20\n",
      "147123/147123 [==============================] - 32s 216us/step - loss: 0.0155 - acc: 0.1043\n",
      "RMSE:  0.17892304080303573\n",
      "**** METRICS BASED ON DENORMALIZED SCORE FOR FEED FORWARD NN****\n",
      "**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\n",
      "RMSE:  2.9969939804064736\n",
      "Kappa Quadratic Weighting:  0.9714422223685433\n",
      "Accuracy:  0.30966882375333077\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator_nn = KerasRegressor(build_fn=FF_1_NN, epochs=20, batch_size=500)\n",
    "estimator_nn.fit(train_data, y_train)\n",
    "\n",
    "# Predict for test data\n",
    "prediction_nn=estimator_nn.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_nn)\n",
    "# Find the overall RMSE value. This is not very relevant considering different scale for each set.\n",
    "print(\"RMSE: \",rmse_val)\n",
    "\n",
    "# Construct pandas data frame of scores of each sentences\n",
    "nn_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_nn.astype(np.double)]).transpose()\n",
    "nn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "# Find Multiplication factor using the function we defined before\n",
    "nn_df['Mult_Factor'] = nn_df.apply(find_mult_factor,axis=1)\n",
    "# Find the denormalized predicted score\n",
    "nn_df['Denorm_Pred_Score'] = nn_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores of each sentences\n",
    "orig_score = nn_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = nn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "\n",
    "# Provide the metrics at sentence level\n",
    "rmse_nn = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score,weights='quadratic')\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "print(\"**** METRICS BASED ON DENORMALIZED SCORE FOR FEED FORWARD NN****\")\n",
    "print(\"**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\")\n",
    "print(\"RMSE: \",rmse_nn)\n",
    "print(\"Kappa Quadratic Weighting: \",cohen_kappa)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Simple FF NN : Essay level Metrics : After taking MAX score all Sentences ###\n",
      "RMSE:  3.2677746724196335\n",
      "Cohen Kappa:  0.1956655687947143\n",
      "Accuracy:  0.2854391371340524\n",
      "### Simple FF NN : Essay level Metrics : After taking Min Score of all Sentences ###\n",
      "RMSE:  3.2258943648242147\n",
      "Cohen Kappa:  0.21794020433411565\n",
      "Accuracy:  0.32126348228043144\n",
      "### Simple FF NN : Essay level Metrics : After taking Mean Score of all Sentences ###\n",
      "RMSE:  2.13859176337009\n",
      "Cohen Kappa:  0.2775055325740141\n",
      "Cohen Kappa Quadratic Weight:  0.9708101984383357\n",
      "Accuracy:  0.37057010785824346\n",
      "   essay_set      RMSE     Kappa   Kappa_Q  Accuracy  Norm_RMSE\n",
      "0        1.0  1.442902  0.042139  0.193314  0.245902   0.117325\n",
      "1        2.0  0.901573  0.150354  0.184940  0.489305   0.154051\n",
      "2        3.0  0.787249  0.063316  0.233452  0.398204   0.280280\n",
      "3        4.0  0.651920  0.387496  0.725265  0.575000   0.323910\n",
      "4        5.0  0.904097  0.120953  0.339377  0.408696   0.225493\n",
      "5        6.0  0.863731  0.088229  0.448227  0.473545   0.242584\n",
      "6        7.0  4.457674  0.000804  0.197286  0.072848   0.178450\n",
      "7        8.0  5.323135  0.004527  0.307632  0.051095   0.106872\n",
      "Mean Quadratic Kappa:  0.32868658732918754\n"
     ]
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,nn_df.Orig_Score.values,\n",
    "                          nn_df.Norm_Score,nn_df.Denorm_Pred_Score.values,\n",
    "                          nn_df.Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','norm_orig_score',\n",
    "                     'pred_score','norm_pred_score']\n",
    "nn_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "nn_test_df.essay_id = nn_test_df.essay_id.astype(int)\n",
    "nn_test_df.essay_set = nn_test_df.essay_set.astype(int)\n",
    "nn_test_df.Orig_Score = nn_test_df.Orig_Score.astype(int)\n",
    "nn_test_df.Max_Score = nn_test_df.Max_Score.astype(int)\n",
    "nn_test_df.Min_Score = nn_test_df.Min_Score.astype(int)\n",
    "nn_test_df.Mean_Score = nn_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = nn_test_df.Orig_Score.values\n",
    "max_pred_score = nn_test_df.Max_Score.values\n",
    "min_pred_score = nn_test_df.Min_Score.values\n",
    "mean_pred_score = nn_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_nn = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_nn = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_nn = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_nn = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_nn = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_nn = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_nn = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_nn = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_q_nn = cohen_kappa_score(orig_score,mean_pred_score,weights='quadratic')\n",
    "accuracy_mean_nn = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### Simple FF NN : Essay level Metrics : After taking MAX score all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_nn)\n",
    "print(\"Accuracy: \",accuracy_max_nn)\n",
    "\n",
    "print(\"### Simple FF NN : Essay level Metrics : After taking Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_nn)\n",
    "print(\"Accuracy: \",accuracy_min_nn)\n",
    "\n",
    "print(\"### Simple FF NN : Essay level Metrics : After taking Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_nn)\n",
    "print(\"Cohen Kappa Quadratic Weight: \",cohen_kappa_mean_q_nn)\n",
    "print(\"Accuracy: \",accuracy_mean_nn)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(nn_test_df)\n",
    "print(essay_set_results)\n",
    "print(\"Mean Quadratic Kappa: \",np.mean(essay_set_results.Kappa_Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FNN 2 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147123/147123 [==============================] - 33s 228us/step - loss: 0.0351 - acc: 0.0983\n",
      "Epoch 2/20\n",
      "147123/147123 [==============================] - 32s 214us/step - loss: 0.0292 - acc: 0.0988\n",
      "Epoch 3/20\n",
      "147123/147123 [==============================] - 32s 217us/step - loss: 0.0276 - acc: 0.0996\n",
      "Epoch 4/20\n",
      "147123/147123 [==============================] - 32s 220us/step - loss: 0.0265 - acc: 0.1004\n",
      "Epoch 5/20\n",
      "147123/147123 [==============================] - 32s 216us/step - loss: 0.0253 - acc: 0.1012\n",
      "Epoch 6/20\n",
      "147123/147123 [==============================] - 32s 215us/step - loss: 0.0239 - acc: 0.1017\n",
      "Epoch 7/20\n",
      "147123/147123 [==============================] - 32s 218us/step - loss: 0.0224 - acc: 0.1023\n",
      "Epoch 8/20\n",
      "147123/147123 [==============================] - 32s 215us/step - loss: 0.0209 - acc: 0.1028\n",
      "Epoch 9/20\n",
      "147123/147123 [==============================] - 32s 216us/step - loss: 0.0193 - acc: 0.1032\n",
      "Epoch 10/20\n",
      "147123/147123 [==============================] - 32s 219us/step - loss: 0.0176 - acc: 0.1036\n",
      "Epoch 11/20\n",
      "147123/147123 [==============================] - 32s 216us/step - loss: 0.0163 - acc: 0.1039\n",
      "Epoch 12/20\n",
      "147123/147123 [==============================] - 32s 218us/step - loss: 0.0151 - acc: 0.1042\n",
      "Epoch 13/20\n",
      "147123/147123 [==============================] - 32s 215us/step - loss: 0.0141 - acc: 0.1043\n",
      "Epoch 14/20\n",
      "147123/147123 [==============================] - 32s 216us/step - loss: 0.0131 - acc: 0.1045\n",
      "Epoch 15/20\n",
      "147123/147123 [==============================] - 32s 215us/step - loss: 0.0123 - acc: 0.1046\n",
      "Epoch 16/20\n",
      "147123/147123 [==============================] - 32s 214us/step - loss: 0.0115 - acc: 0.1046\n",
      "Epoch 17/20\n",
      "147123/147123 [==============================] - 32s 214us/step - loss: 0.0110 - acc: 0.1047\n",
      "Epoch 18/20\n",
      "147123/147123 [==============================] - 32s 214us/step - loss: 0.0104 - acc: 0.1048\n",
      "Epoch 19/20\n",
      "147123/147123 [==============================] - 32s 216us/step - loss: 0.0099 - acc: 0.1048\n",
      "Epoch 20/20\n",
      "147123/147123 [==============================] - 32s 215us/step - loss: 0.0094 - acc: 0.1048\n",
      "RMSE:  0.1987534469357875\n",
      "**** METRICS BASED ON DENORMALIZED SCORE FOR FEED FORWARD NN****\n",
      "**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\n",
      "RMSE:  3.4676005580065636\n",
      "Kappa Quadratic Weighting:  0.9615604623049535\n",
      "Accuracy:  0.2816085703409647\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator_nn = KerasRegressor(build_fn=FF_2_NN, epochs=20, batch_size=500)\n",
    "estimator_nn.fit(train_data, y_train)\n",
    "\n",
    "# Predict for test data\n",
    "prediction_nn=estimator_nn.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_nn)\n",
    "# Find the overall RMSE value. This is not very relevant considering different scale for each set.\n",
    "print(\"RMSE: \",rmse_val)\n",
    "\n",
    "# Construct pandas data frame of scores of each sentences\n",
    "nn_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_nn.astype(np.double)]).transpose()\n",
    "nn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "# Find Multiplication factor using the function we defined before\n",
    "nn_df['Mult_Factor'] = nn_df.apply(find_mult_factor,axis=1)\n",
    "# Find the denormalized predicted score\n",
    "nn_df['Denorm_Pred_Score'] = nn_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores of each sentences\n",
    "orig_score = nn_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = nn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "\n",
    "# Provide the metrics at sentence level\n",
    "rmse_nn = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score,weights='quadratic')\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "print(\"**** METRICS BASED ON DENORMALIZED SCORE FOR FEED FORWARD NN****\")\n",
    "print(\"**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\")\n",
    "print(\"RMSE: \",rmse_nn)\n",
    "print(\"Kappa Quadratic Weighting: \",cohen_kappa)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Simple FF NN 2 Layers : Essay level Metrics : After taking MAX score all Sentences ###\n",
      "RMSE:  3.765875486765087\n",
      "Cohen Kappa:  0.12265187146196521\n",
      "Accuracy:  0.20955315870570107\n",
      "### Simple FF NN 2 Layers : Essay level Metrics : After taking Min Score of all Sentences ###\n",
      "RMSE:  4.545947584970648\n",
      "Cohen Kappa:  0.164573051079265\n",
      "Accuracy:  0.26617873651771956\n",
      "### Simple FF NN 2 Layers : Essay level Metrics : After taking Mean Score of all Sentences ###\n",
      "RMSE:  2.138051328261422\n",
      "Cohen Kappa:  0.29140433677059774\n",
      "Cohen Kappa Quadratic Weight:  0.9704768417636869\n",
      "Accuracy:  0.38366718027734975\n",
      "   essay_set      RMSE     Kappa   Kappa_Q  Accuracy  Norm_RMSE\n",
      "0        1.0  1.389854  0.047714  0.237458  0.281421   0.115911\n",
      "1        2.0  0.860605  0.136477  0.235721  0.486631   0.151464\n",
      "2        3.0  0.831635  0.063564  0.240894  0.389222   0.284691\n",
      "3        4.0  0.670820  0.381893  0.722539  0.566667   0.314357\n",
      "4        5.0  0.932505  0.189240  0.311011  0.455072   0.229856\n",
      "5        6.0  0.884912  0.087086  0.452142  0.457672   0.247965\n",
      "6        7.0  4.395964  0.054313  0.205617  0.125828   0.174735\n",
      "7        8.0  5.442466  0.008593  0.268971  0.065693   0.107595\n",
      "Mean Quadratic Kappa:  0.3342941200245982\n"
     ]
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,nn_df.Orig_Score.values,\n",
    "                          nn_df.Norm_Score,nn_df.Denorm_Pred_Score.values,\n",
    "                          nn_df.Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','norm_orig_score',\n",
    "                     'pred_score','norm_pred_score']\n",
    "nn_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "nn_test_df.essay_id = nn_test_df.essay_id.astype(int)\n",
    "nn_test_df.essay_set = nn_test_df.essay_set.astype(int)\n",
    "nn_test_df.Orig_Score = nn_test_df.Orig_Score.astype(int)\n",
    "nn_test_df.Max_Score = nn_test_df.Max_Score.astype(int)\n",
    "nn_test_df.Min_Score = nn_test_df.Min_Score.astype(int)\n",
    "nn_test_df.Mean_Score = nn_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = nn_test_df.Orig_Score.values\n",
    "max_pred_score = nn_test_df.Max_Score.values\n",
    "min_pred_score = nn_test_df.Min_Score.values\n",
    "mean_pred_score = nn_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_nn = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_nn = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_nn = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_nn = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_nn = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_nn = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_nn = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_nn = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_q_nn = cohen_kappa_score(orig_score,mean_pred_score,weights='quadratic')\n",
    "accuracy_mean_nn = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### Simple FF NN 2 Layers : Essay level Metrics : After taking MAX score all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_nn)\n",
    "print(\"Accuracy: \",accuracy_max_nn)\n",
    "\n",
    "print(\"### Simple FF NN 2 Layers : Essay level Metrics : After taking Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_nn)\n",
    "print(\"Accuracy: \",accuracy_min_nn)\n",
    "\n",
    "print(\"### Simple FF NN 2 Layers : Essay level Metrics : After taking Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_nn)\n",
    "print(\"Cohen Kappa Quadratic Weight: \",cohen_kappa_mean_q_nn)\n",
    "print(\"Accuracy: \",accuracy_mean_nn)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(nn_test_df)\n",
    "print(essay_set_results)\n",
    "print(\"Mean Quadratic Kappa: \",np.mean(essay_set_results.Kappa_Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FNN 3 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147123/147123 [==============================] - 33s 224us/step - loss: 0.0318 - acc: 0.0985\n",
      "Epoch 2/20\n",
      "147123/147123 [==============================] - 33s 221us/step - loss: 0.0285 - acc: 0.0989\n",
      "Epoch 3/20\n",
      "147123/147123 [==============================] - 33s 222us/step - loss: 0.0272 - acc: 0.0998\n",
      "Epoch 4/20\n",
      "147123/147123 [==============================] - 33s 225us/step - loss: 0.0260 - acc: 0.1006\n",
      "Epoch 5/20\n",
      "147123/147123 [==============================] - 32s 219us/step - loss: 0.0246 - acc: 0.1014\n",
      "Epoch 6/20\n",
      "147123/147123 [==============================] - 32s 220us/step - loss: 0.0232 - acc: 0.1020\n",
      "Epoch 7/20\n",
      "147123/147123 [==============================] - 32s 221us/step - loss: 0.0214 - acc: 0.1026\n",
      "Epoch 8/20\n",
      "147123/147123 [==============================] - 33s 227us/step - loss: 0.0195 - acc: 0.1032\n",
      "Epoch 9/20\n",
      "147123/147123 [==============================] - 583s 4ms/step - loss: 0.0179 - acc: 0.1038\n",
      "Epoch 10/20\n",
      "147123/147123 [==============================] - 41s 276us/step - loss: 0.0163 - acc: 0.1042\n",
      "Epoch 11/20\n",
      "147123/147123 [==============================] - 36s 244us/step - loss: 0.0150 - acc: 0.1045\n",
      "Epoch 12/20\n",
      "147123/147123 [==============================] - 35s 239us/step - loss: 0.0138 - acc: 0.1046\n",
      "Epoch 13/20\n",
      "147123/147123 [==============================] - 35s 241us/step - loss: 0.0128 - acc: 0.1046\n",
      "Epoch 14/20\n",
      "147123/147123 [==============================] - 35s 238us/step - loss: 0.0120 - acc: 0.1047\n",
      "Epoch 15/20\n",
      "147123/147123 [==============================] - 35s 240us/step - loss: 0.0113 - acc: 0.1048\n",
      "Epoch 16/20\n",
      "147123/147123 [==============================] - 35s 240us/step - loss: 0.0106 - acc: 0.1048\n",
      "Epoch 17/20\n",
      "147123/147123 [==============================] - 35s 240us/step - loss: 0.0100 - acc: 0.1049\n",
      "Epoch 18/20\n",
      "147123/147123 [==============================] - 35s 240us/step - loss: 0.0095 - acc: 0.1049\n",
      "Epoch 19/20\n",
      "147123/147123 [==============================] - 35s 240us/step - loss: 0.0091 - acc: 0.1049\n",
      "Epoch 20/20\n",
      "147123/147123 [==============================] - 35s 240us/step - loss: 0.0088 - acc: 0.1050\n",
      "RMSE:  0.20030693721720727\n",
      "**** METRICS BASED ON DENORMALIZED SCORE FOR FEED FORWARD NN****\n",
      "**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\n",
      "RMSE:  3.5044970837010014\n",
      "Kappa Quadratic Weighting:  0.9606625047370698\n",
      "Accuracy:  0.28353907227146663\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator_nn = KerasRegressor(build_fn=FF_3_NN, epochs=20, batch_size=500)\n",
    "estimator_nn.fit(train_data, y_train)\n",
    "\n",
    "# Predict for test data\n",
    "prediction_nn=estimator_nn.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_nn)\n",
    "# Find the overall RMSE value. This is not very relevant considering different scale for each set.\n",
    "print(\"RMSE: \",rmse_val)\n",
    "\n",
    "# Construct pandas data frame of scores of each sentences\n",
    "nn_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_nn.astype(np.double)]).transpose()\n",
    "nn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "# Find Multiplication factor using the function we defined before\n",
    "nn_df['Mult_Factor'] = nn_df.apply(find_mult_factor,axis=1)\n",
    "# Find the denormalized predicted score\n",
    "nn_df['Denorm_Pred_Score'] = nn_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores of each sentences\n",
    "orig_score = nn_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = nn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "\n",
    "# Provide the metrics at sentence level\n",
    "rmse_nn = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score,weights='quadratic')\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "print(\"**** METRICS BASED ON DENORMALIZED SCORE FOR FEED FORWARD NN****\")\n",
    "print(\"**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\")\n",
    "print(\"RMSE: \",rmse_nn)\n",
    "print(\"Kappa Quadratic Weighting: \",cohen_kappa)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Simple FF NN : Essay level Metrics : After taking MAX score all Sentences ###\n",
      "RMSE:  3.663630437354458\n",
      "Cohen Kappa:  0.1387628988492543\n",
      "Accuracy:  0.22573189522342066\n",
      "### Simple FF NN : Essay level Metrics : After taking Min Score of all Sentences ###\n",
      "RMSE:  4.738663383545806\n",
      "Cohen Kappa:  0.14152613564101657\n",
      "Accuracy:  0.24499229583975346\n",
      "### Simple FF NN : Essay level Metrics : After taking Mean Score of all Sentences ###\n",
      "RMSE:  2.107200217845796\n",
      "Cohen Kappa:  0.2899175128646725\n",
      "Cohen Kappa Quadratic Weight:  0.9711809537478122\n",
      "Accuracy:  0.3825115562403698\n",
      "   essay_set      RMSE     Kappa   Kappa_Q  Accuracy  Norm_RMSE\n",
      "0        1.0  1.377017  0.041800  0.256676  0.275956   0.112741\n",
      "1        2.0  0.827340  0.152816  0.290354  0.497326   0.148604\n",
      "2        3.0  0.831635  0.061087  0.226526  0.389222   0.279158\n",
      "3        4.0  0.662487  0.385855  0.729390  0.569444   0.307384\n",
      "4        5.0  0.915249  0.160521  0.332653  0.437681   0.228587\n",
      "5        6.0  0.889385  0.138091  0.457819  0.481481   0.242676\n",
      "6        7.0  4.316149  0.030521  0.220636  0.105960   0.172726\n",
      "7        8.0  5.385842 -0.009563  0.288884  0.043796   0.107261\n",
      "Mean Quadratic Kappa:  0.35036729738194816\n"
     ]
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,nn_df.Orig_Score.values,\n",
    "                          nn_df.Norm_Score,nn_df.Denorm_Pred_Score.values,\n",
    "                          nn_df.Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','norm_orig_score',\n",
    "                     'pred_score','norm_pred_score']\n",
    "nn_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "nn_test_df.essay_id = nn_test_df.essay_id.astype(int)\n",
    "nn_test_df.essay_set = nn_test_df.essay_set.astype(int)\n",
    "nn_test_df.Orig_Score = nn_test_df.Orig_Score.astype(int)\n",
    "nn_test_df.Max_Score = nn_test_df.Max_Score.astype(int)\n",
    "nn_test_df.Min_Score = nn_test_df.Min_Score.astype(int)\n",
    "nn_test_df.Mean_Score = nn_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = nn_test_df.Orig_Score.values\n",
    "max_pred_score = nn_test_df.Max_Score.values\n",
    "min_pred_score = nn_test_df.Min_Score.values\n",
    "mean_pred_score = nn_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_nn = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_nn = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_nn = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_nn = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_nn = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_nn = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_nn = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_nn = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_q_nn = cohen_kappa_score(orig_score,mean_pred_score,weights='quadratic')\n",
    "accuracy_mean_nn = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### Simple FF NN : Essay level Metrics : After taking MAX score all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_nn)\n",
    "print(\"Accuracy: \",accuracy_max_nn)\n",
    "\n",
    "print(\"### Simple FF NN : Essay level Metrics : After taking Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_nn)\n",
    "print(\"Accuracy: \",accuracy_min_nn)\n",
    "\n",
    "print(\"### Simple FF NN : Essay level Metrics : After taking Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_nn)\n",
    "print(\"Cohen Kappa Quadratic Weight: \",cohen_kappa_mean_q_nn)\n",
    "print(\"Accuracy: \",accuracy_mean_nn)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(nn_test_df)\n",
    "print(essay_set_results)\n",
    "print(\"Mean Quadratic Kappa: \",np.mean(essay_set_results.Kappa_Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Dim=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147123/147123 [==============================] - 124s 844us/step - loss: 0.0517 - acc: 0.0923\n",
      "Epoch 2/20\n",
      "147123/147123 [==============================] - 121s 824us/step - loss: 0.0315 - acc: 0.0991\n",
      "Epoch 3/20\n",
      "147123/147123 [==============================] - 120s 818us/step - loss: 0.0297 - acc: 0.0996\n",
      "Epoch 4/20\n",
      "147123/147123 [==============================] - 122s 829us/step - loss: 0.0286 - acc: 0.0997\n",
      "Epoch 5/20\n",
      "147123/147123 [==============================] - 123s 833us/step - loss: 0.0281 - acc: 0.1000\n",
      "Epoch 6/20\n",
      "147123/147123 [==============================] - 121s 822us/step - loss: 0.0277 - acc: 0.1000\n",
      "Epoch 7/20\n",
      "147123/147123 [==============================] - 120s 818us/step - loss: 0.0273 - acc: 0.1002\n",
      "Epoch 8/20\n",
      "147123/147123 [==============================] - 915s 6ms/step - loss: 0.0270 - acc: 0.1003\n",
      "Epoch 9/20\n",
      "147123/147123 [==============================] - 129s 876us/step - loss: 0.0267 - acc: 0.1004\n",
      "Epoch 10/20\n",
      "147123/147123 [==============================] - 124s 843us/step - loss: 0.0265 - acc: 0.1004\n",
      "Epoch 11/20\n",
      "147123/147123 [==============================] - 135s 919us/step - loss: 0.0263 - acc: 0.1006\n",
      "Epoch 12/20\n",
      "147123/147123 [==============================] - 135s 915us/step - loss: 0.0260 - acc: 0.1005\n",
      "Epoch 13/20\n",
      "147123/147123 [==============================] - 134s 910us/step - loss: 0.0258 - acc: 0.1007\n",
      "Epoch 14/20\n",
      "147123/147123 [==============================] - 132s 899us/step - loss: 0.0256 - acc: 0.1008\n",
      "Epoch 15/20\n",
      "147123/147123 [==============================] - 130s 884us/step - loss: 0.0255 - acc: 0.1007\n",
      "Epoch 16/20\n",
      "147123/147123 [==============================] - 122s 832us/step - loss: 0.0253 - acc: 0.1008\n",
      "Epoch 17/20\n",
      "147123/147123 [==============================] - 120s 818us/step - loss: 0.0251 - acc: 0.1010\n",
      "Epoch 18/20\n",
      "147123/147123 [==============================] - 121s 825us/step - loss: 0.0250 - acc: 0.1012\n",
      "Epoch 19/20\n",
      "147123/147123 [==============================] - 120s 816us/step - loss: 0.0249 - acc: 0.1011\n",
      "Epoch 20/20\n",
      "147123/147123 [==============================] - 125s 846us/step - loss: 0.0246 - acc: 0.1013\n",
      "RMSE:  0.16365551566355543\n",
      "**** METRICS BASED ON DENORMALIZED SCORE FOR SIMPLE RNN****\n",
      "**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\n",
      "RMSE:  2.6811852130169274\n",
      "Kappa Quadratic Weighting:  0.9766798798950704\n",
      "Accuracy:  0.331013105660993\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator = KerasRegressor(build_fn=GRU_32, epochs=20, batch_size=500)\n",
    "estimator.fit(train_data, y_train)\n",
    "\n",
    "# Predict for test data\n",
    "prediction_rnn=estimator.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_rnn)\n",
    "# Find the overall RMSE value. This is not very relevant considering different scale for each set.\n",
    "print(\"RMSE: \",rmse_val)\n",
    "\n",
    "# Construct pandas data frame of scores of each sentences\n",
    "rnn_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_rnn.astype(np.double)]).transpose()\n",
    "rnn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "# Find Multiplication factor using the function we defined before\n",
    "rnn_df['Mult_Factor'] = rnn_df.apply(find_mult_factor,axis=1)\n",
    "# Find the denormalized predicted score\n",
    "rnn_df['Denorm_Pred_Score'] = rnn_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores of each sentences\n",
    "orig_score = rnn_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = rnn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "\n",
    "# Provide the metrics at sentence level\n",
    "rmse_rnn = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score,weights='quadratic')\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "\n",
    "print(\"**** METRICS BASED ON DENORMALIZED SCORE FOR SIMPLE RNN****\")\n",
    "print(\"**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\")\n",
    "print(\"RMSE: \",rmse_rnn)\n",
    "print(\"Kappa Quadratic Weighting: \",cohen_kappa)\n",
    "print(\"Accuracy: \",accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### RNN GRU Results : MAX Score of all Sentences ###\n",
      "RMSE:  2.6864953599349906\n",
      "Cohen Kappa:  0.23309685332559504\n",
      "Accuracy:  0.32550077041602465\n",
      "### RNN GRU Results : Min Score of all Sentences ###\n",
      "RMSE:  2.5121430822999735\n",
      "Cohen Kappa:  0.2906816168095623\n",
      "Accuracy:  0.38751926040061635\n",
      "### RNN GRU Results : Mean Score of all Sentences ###\n",
      "RMSE:  2.050123074963098\n",
      "Cohen Kappa:  0.2888152272008384\n",
      "Cohen Kappa Quadratic:  0.972806765680875\n",
      "Accuracy:  0.38212634822804314\n",
      "   essay_set      RMSE     Kappa   Kappa_Q  Accuracy  Norm_RMSE\n",
      "0        1.0  1.413247  0.068420  0.207540  0.289617   0.112577\n",
      "1        2.0  0.814311  0.200855  0.318971  0.524064   0.144942\n",
      "2        3.0  0.777683  0.071328  0.245712  0.404192   0.262931\n",
      "3        4.0  0.649786  0.394100  0.732489  0.577778   0.278861\n",
      "4        5.0  0.905699  0.103213  0.337147  0.397101   0.218914\n",
      "5        6.0  0.834127  0.097415  0.479666  0.478836   0.221532\n",
      "6        7.0  4.157894 -0.015868  0.269119  0.062914   0.166398\n",
      "7        8.0  5.274232  0.023077  0.301236  0.072993   0.105103\n",
      "Mean Quadratic Kappa:  0.36148501769705255\n"
     ]
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,rnn_df.Orig_Score.values,\n",
    "                          rnn_df.Norm_Score,rnn_df.Denorm_Pred_Score.values,\n",
    "                          rnn_df.Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','norm_orig_score',\n",
    "                     'pred_score','norm_pred_score']\n",
    "rnn_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "rnn_test_df.essay_id = rnn_test_df.essay_id.astype(int)\n",
    "rnn_test_df.essay_set = rnn_test_df.essay_set.astype(int)\n",
    "rnn_test_df.Orig_Score = rnn_test_df.Orig_Score.astype(int)\n",
    "rnn_test_df.Max_Score = rnn_test_df.Max_Score.astype(int)\n",
    "rnn_test_df.Min_Score = rnn_test_df.Min_Score.astype(int)\n",
    "rnn_test_df.Mean_Score = rnn_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = rnn_test_df.Orig_Score.values\n",
    "max_pred_score = rnn_test_df.Max_Score.values\n",
    "min_pred_score = rnn_test_df.Min_Score.values\n",
    "mean_pred_score = rnn_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_rnn = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_rnn = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_rnn = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_rnn = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_rnn = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_rnn = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_rnn = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_rnn = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_q_rnn = cohen_kappa_score(orig_score,mean_pred_score,weights='quadratic')\n",
    "accuracy_mean_rnn = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### RNN GRU Results : MAX Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_rnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_rnn)\n",
    "print(\"Accuracy: \",accuracy_max_rnn)\n",
    "\n",
    "print(\"### RNN GRU Results : Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_rnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_rnn)\n",
    "print(\"Accuracy: \",accuracy_min_rnn)\n",
    "\n",
    "print(\"### RNN GRU Results : Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_rnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_rnn)\n",
    "print(\"Cohen Kappa Quadratic: \",cohen_kappa_mean_q_rnn)\n",
    "print(\"Accuracy: \",accuracy_mean_rnn)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(rnn_test_df)\n",
    "print(essay_set_results)\n",
    "print(\"Mean Quadratic Kappa: \",np.mean(essay_set_results.Kappa_Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Dim=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147123/147123 [==============================] - 127s 866us/step - loss: 0.1100 - acc: 0.0834\n",
      "Epoch 2/20\n",
      "147123/147123 [==============================] - 126s 854us/step - loss: 0.0397 - acc: 0.0961\n",
      "Epoch 3/20\n",
      "147123/147123 [==============================] - 128s 869us/step - loss: 0.0338 - acc: 0.0984\n",
      "Epoch 4/20\n",
      "147123/147123 [==============================] - 121s 820us/step - loss: 0.0315 - acc: 0.0992\n",
      "Epoch 5/20\n",
      "147123/147123 [==============================] - 146s 992us/step - loss: 0.0303 - acc: 0.0994\n",
      "Epoch 6/20\n",
      "147123/147123 [==============================] - 129s 874us/step - loss: 0.0295 - acc: 0.0996\n",
      "Epoch 7/20\n",
      "147123/147123 [==============================] - 133s 907us/step - loss: 0.0290 - acc: 0.0996\n",
      "Epoch 8/20\n",
      "147123/147123 [==============================] - 128s 867us/step - loss: 0.0285 - acc: 0.1000\n",
      "Epoch 9/20\n",
      "147123/147123 [==============================] - 130s 882us/step - loss: 0.0282 - acc: 0.1001\n",
      "Epoch 10/20\n",
      "147123/147123 [==============================] - 132s 899us/step - loss: 0.0278 - acc: 0.1002\n",
      "Epoch 11/20\n",
      "147123/147123 [==============================] - 10867s 74ms/step - loss: 0.0275 - acc: 0.1002\n",
      "Epoch 12/20\n",
      "147123/147123 [==============================] - 372s 3ms/step - loss: 0.0272 - acc: 0.1003\n",
      "Epoch 13/20\n",
      "147123/147123 [==============================] - 131s 888us/step - loss: 0.0270 - acc: 0.1004\n",
      "Epoch 14/20\n",
      "147123/147123 [==============================] - 128s 872us/step - loss: 0.0268 - acc: 0.1005\n",
      "Epoch 15/20\n",
      "147123/147123 [==============================] - 132s 897us/step - loss: 0.0265 - acc: 0.1006\n",
      "Epoch 16/20\n",
      "147123/147123 [==============================] - 126s 859us/step - loss: 0.0262 - acc: 0.1006\n",
      "Epoch 17/20\n",
      "147123/147123 [==============================] - 134s 910us/step - loss: 0.0260 - acc: 0.1007\n",
      "Epoch 18/20\n",
      "147123/147123 [==============================] - 137s 928us/step - loss: 0.0258 - acc: 0.1008\n",
      "Epoch 19/20\n",
      "147123/147123 [==============================] - 126s 855us/step - loss: 0.0257 - acc: 0.1008\n",
      "Epoch 20/20\n",
      "147123/147123 [==============================] - 131s 892us/step - loss: 0.0254 - acc: 0.1010\n",
      "RMSE:  0.16643325815132692\n",
      "**** METRICS BASED ON DENORMALIZED SCORE FOR SIMPLE RNN****\n",
      "**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\n",
      "RMSE:  2.6847727623524444\n",
      "Kappa Quadratic Weighting:  0.9772108742704733\n",
      "Accuracy:  0.32399804230790147\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator = KerasRegressor(build_fn=GRU_50, epochs=20, batch_size=500)\n",
    "estimator.fit(train_data, y_train)\n",
    "\n",
    "# Predict for test data\n",
    "prediction_rnn=estimator.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_rnn)\n",
    "# Find the overall RMSE value. This is not very relevant considering different scale for each set.\n",
    "print(\"RMSE: \",rmse_val)\n",
    "\n",
    "# Construct pandas data frame of scores of each sentences\n",
    "rnn_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_rnn.astype(np.double)]).transpose()\n",
    "rnn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "# Find Multiplication factor using the function we defined before\n",
    "rnn_df['Mult_Factor'] = rnn_df.apply(find_mult_factor,axis=1)\n",
    "# Find the denormalized predicted score\n",
    "rnn_df['Denorm_Pred_Score'] = rnn_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores of each sentences\n",
    "orig_score = rnn_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = rnn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "\n",
    "# Provide the metrics at sentence level\n",
    "rmse_rnn = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score,weights='quadratic')\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "\n",
    "print(\"**** METRICS BASED ON DENORMALIZED SCORE FOR SIMPLE RNN****\")\n",
    "print(\"**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\")\n",
    "print(\"RMSE: \",rmse_rnn)\n",
    "print(\"Kappa Quadratic Weighting: \",cohen_kappa)\n",
    "print(\"Accuracy: \",accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### RNN GRU Results : MAX Score of all Sentences ###\n",
      "RMSE:  2.8784438742372678\n",
      "Cohen Kappa:  0.22072116917554718\n",
      "Accuracy:  0.31240369799691836\n",
      "### RNN GRU Results : Min Score of all Sentences ###\n",
      "RMSE:  2.3368983697959584\n",
      "Cohen Kappa:  0.3229052481778496\n",
      "Accuracy:  0.41486902927580893\n",
      "### RNN GRU Results : Mean Score of all Sentences ###\n",
      "RMSE:  2.155546112864239\n",
      "Cohen Kappa:  0.264022917076957\n",
      "Cohen Kappa Quadratic:  0.9707118844163327\n",
      "Accuracy:  0.3578582434514638\n",
      "   essay_set      RMSE     Kappa   Kappa_Q  Accuracy  Norm_RMSE\n",
      "0        1.0  1.483976  0.030455  0.161689  0.224044   0.119711\n",
      "1        2.0  0.922099  0.054183  0.161778  0.427807   0.156690\n",
      "2        3.0  0.783436  0.056695  0.230449  0.395210   0.272872\n",
      "3        4.0  0.636832  0.417011  0.740798  0.594444   0.284176\n",
      "4        5.0  0.930949  0.062430  0.297812  0.368116   0.228753\n",
      "5        6.0  0.854493  0.100703  0.450928  0.484127   0.232807\n",
      "6        7.0  4.415131 -0.000082  0.222391  0.069536   0.176454\n",
      "7        8.0  5.487211  0.009507  0.285091  0.072993   0.109436\n",
      "Mean Quadratic Kappa:  0.318866902066941\n"
     ]
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,rnn_df.Orig_Score.values,\n",
    "                          rnn_df.Norm_Score,rnn_df.Denorm_Pred_Score.values,\n",
    "                          rnn_df.Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','norm_orig_score',\n",
    "                     'pred_score','norm_pred_score']\n",
    "rnn_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "rnn_test_df.essay_id = rnn_test_df.essay_id.astype(int)\n",
    "rnn_test_df.essay_set = rnn_test_df.essay_set.astype(int)\n",
    "rnn_test_df.Orig_Score = rnn_test_df.Orig_Score.astype(int)\n",
    "rnn_test_df.Max_Score = rnn_test_df.Max_Score.astype(int)\n",
    "rnn_test_df.Min_Score = rnn_test_df.Min_Score.astype(int)\n",
    "rnn_test_df.Mean_Score = rnn_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = rnn_test_df.Orig_Score.values\n",
    "max_pred_score = rnn_test_df.Max_Score.values\n",
    "min_pred_score = rnn_test_df.Min_Score.values\n",
    "mean_pred_score = rnn_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_rnn = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_rnn = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_rnn = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_rnn = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_rnn = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_rnn = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_rnn = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_rnn = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_q_rnn = cohen_kappa_score(orig_score,mean_pred_score,weights='quadratic')\n",
    "accuracy_mean_rnn = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### RNN GRU Results : MAX Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_rnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_rnn)\n",
    "print(\"Accuracy: \",accuracy_max_rnn)\n",
    "\n",
    "print(\"### RNN GRU Results : Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_rnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_rnn)\n",
    "print(\"Accuracy: \",accuracy_min_rnn)\n",
    "\n",
    "print(\"### RNN GRU Results : Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_rnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_rnn)\n",
    "print(\"Cohen Kappa Quadratic: \",cohen_kappa_mean_q_rnn)\n",
    "print(\"Accuracy: \",accuracy_mean_rnn)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(rnn_test_df)\n",
    "print(essay_set_results)\n",
    "print(\"Mean Quadratic Kappa: \",np.mean(essay_set_results.Kappa_Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147123/147123 [==============================] - 142s 966us/step - loss: 0.4231 - acc: 0.0799\n",
      "Epoch 2/20\n",
      "147123/147123 [==============================] - 138s 940us/step - loss: 0.0428 - acc: 0.0961\n",
      "Epoch 3/20\n",
      "147123/147123 [==============================] - 133s 906us/step - loss: 0.0365 - acc: 0.0976\n",
      "Epoch 4/20\n",
      "147123/147123 [==============================] - 138s 941us/step - loss: 0.0341 - acc: 0.0981\n",
      "Epoch 5/20\n",
      "147123/147123 [==============================] - 137s 930us/step - loss: 0.0332 - acc: 0.0985\n",
      "Epoch 6/20\n",
      "147123/147123 [==============================] - 136s 926us/step - loss: 0.0324 - acc: 0.0985\n",
      "Epoch 7/20\n",
      "147123/147123 [==============================] - 142s 966us/step - loss: 0.0321 - acc: 0.0986\n",
      "Epoch 8/20\n",
      "147123/147123 [==============================] - 141s 957us/step - loss: 0.0317 - acc: 0.0986\n",
      "Epoch 9/20\n",
      "147123/147123 [==============================] - 137s 931us/step - loss: 0.0316 - acc: 0.0987\n",
      "Epoch 10/20\n",
      "147123/147123 [==============================] - 142s 962us/step - loss: 0.0314 - acc: 0.0988\n",
      "Epoch 11/20\n",
      "147123/147123 [==============================] - 143s 971us/step - loss: 0.0308 - acc: 0.0988\n",
      "Epoch 12/20\n",
      "147123/147123 [==============================] - 149s 1ms/step - loss: 0.0304 - acc: 0.0988\n",
      "Epoch 13/20\n",
      "147123/147123 [==============================] - 150s 1ms/step - loss: 0.0300 - acc: 0.0988\n",
      "Epoch 14/20\n",
      "147123/147123 [==============================] - 149s 1ms/step - loss: 0.0297 - acc: 0.0990\n",
      "Epoch 15/20\n",
      "147123/147123 [==============================] - 138s 941us/step - loss: 0.0296 - acc: 0.0990\n",
      "Epoch 16/20\n",
      "147123/147123 [==============================] - 137s 934us/step - loss: 0.0295 - acc: 0.0989\n",
      "Epoch 17/20\n",
      "147123/147123 [==============================] - 138s 937us/step - loss: 0.0294 - acc: 0.0991\n",
      "Epoch 18/20\n",
      "147123/147123 [==============================] - 146s 993us/step - loss: 0.0299 - acc: 0.0990\n",
      "Epoch 19/20\n",
      "147123/147123 [==============================] - 142s 968us/step - loss: 0.0298 - acc: 0.0991\n",
      "Epoch 20/20\n",
      "147123/147123 [==============================] - 136s 928us/step - loss: 0.0296 - acc: 0.0993\n",
      "RMSE BEFORE DENORMALIZING:  0.1758869827106889\n",
      "**** METRICS BASED ON DENORMALIZED SCORE FOR SIMPLE CNN****\n",
      "**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\n",
      "RMSE:  2.8358339918405613\n",
      "Kappa Quadratic Weighting:  0.9737685459253101\n",
      "Accuracy:  0.2984392843547773\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator = KerasRegressor(build_fn=CNN_FF, epochs=20, batch_size=500)\n",
    "estimator.fit(train_data, y_train)\n",
    "\n",
    "# Precit for test data\n",
    "prediction_cnn_glove=estimator.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_cnn_glove)\n",
    "print(\"RMSE BEFORE DENORMALIZING: \",rmse_val)\n",
    "\n",
    "# Construct data frame for denormalizing the score\n",
    "cnn_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_cnn_glove.astype(np.double)]).transpose()\n",
    "cnn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "cnn_df['Mult_Factor'] = cnn_df.apply(find_mult_factor,axis=1)\n",
    "cnn_df['Denorm_Pred_Score'] = cnn_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = cnn_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = cnn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "rmse_cnn = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score,weights='quadratic')\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "\n",
    "print(\"**** METRICS BASED ON DENORMALIZED SCORE FOR SIMPLE CNN****\")\n",
    "print(\"**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\")\n",
    "print(\"RMSE: \",rmse_cnn)\n",
    "print(\"Kappa Quadratic Weighting: \",cohen_kappa)\n",
    "print(\"Accuracy: \",accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### CNN FF Results : MAX Score of all Sentences ###\n",
      "RMSE:  3.058683866531078\n",
      "Cohen Kappa:  0.21222482149594712\n",
      "Accuracy:  0.3050847457627119\n",
      "### CNN FF Results : Min Score of all Sentences ###\n",
      "RMSE:  2.7251468345110523\n",
      "Cohen Kappa:  0.22569578709087035\n",
      "Accuracy:  0.3320493066255778\n",
      "### CNN FF Results : Mean Score of all Sentences ###\n",
      "RMSE:  2.2783059421644176\n",
      "Cohen Kappa:  0.24075520769163195\n",
      "Cohen Kappa Quadratic:  0.9664854808356727\n",
      "Accuracy:  0.33705701078582434\n",
      "   essay_set      RMSE     Kappa   Kappa_Q  Accuracy  Norm_RMSE\n",
      "0        1.0  1.545308  0.005206  0.041260  0.191257   0.120954\n",
      "1        2.0  1.015916  0.004141  0.001950  0.393048   0.162239\n",
      "2        3.0  0.777683  0.055626  0.228426  0.395210   0.290574\n",
      "3        4.0  0.674949  0.344481  0.708150  0.544444   0.359828\n",
      "4        5.0  1.025755  0.026208  0.147858  0.339130   0.249424\n",
      "5        6.0  0.863731  0.098845  0.436751  0.484127   0.245049\n",
      "6        7.0  4.736431  0.011247  0.081560  0.079470   0.188964\n",
      "7        8.0  5.685813 -0.002682  0.156830  0.043796   0.112877\n",
      "Mean Quadratic Kappa:  0.22534813662645037\n"
     ]
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,cnn_df.Orig_Score.values,\n",
    "                          cnn_df.Norm_Score,cnn_df.Denorm_Pred_Score.values,\n",
    "                          cnn_df.Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','norm_orig_score',\n",
    "                     'pred_score','norm_pred_score']\n",
    "cnn_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "cnn_test_df.essay_id = cnn_test_df.essay_id.astype(int)\n",
    "cnn_test_df.essay_set = cnn_test_df.essay_set.astype(int)\n",
    "cnn_test_df.Orig_Score = cnn_test_df.Orig_Score.astype(int)\n",
    "cnn_test_df.Max_Score = cnn_test_df.Max_Score.astype(int)\n",
    "cnn_test_df.Min_Score = cnn_test_df.Min_Score.astype(int)\n",
    "cnn_test_df.Mean_Score = cnn_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = cnn_test_df.Orig_Score.values\n",
    "max_pred_score = cnn_test_df.Max_Score.values\n",
    "min_pred_score = cnn_test_df.Min_Score.values\n",
    "mean_pred_score = cnn_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_cnn = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_cnn = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_cnn = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_cnn = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_cnn = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_cnn = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_cnn = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_cnn = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_q_cnn = cohen_kappa_score(orig_score,mean_pred_score,weights='quadratic')\n",
    "accuracy_mean_cnn = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### CNN FF Results : MAX Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_cnn)\n",
    "print(\"Accuracy: \",accuracy_max_cnn)\n",
    "\n",
    "print(\"### CNN FF Results : Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_cnn)\n",
    "print(\"Accuracy: \",accuracy_min_cnn)\n",
    "\n",
    "print(\"### CNN FF Results : Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_cnn)\n",
    "print(\"Cohen Kappa Quadratic: \",cohen_kappa_mean_q_cnn)\n",
    "print(\"Accuracy: \",accuracy_mean_cnn)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(cnn_test_df)\n",
    "print(essay_set_results)\n",
    "print(\"Mean Quadratic Kappa: \",np.mean(essay_set_results.Kappa_Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147123/147123 [==============================] - 201s 1ms/step - loss: 0.0994 - acc: 0.0915\n",
      "Epoch 2/20\n",
      "147123/147123 [==============================] - 188s 1ms/step - loss: 0.0312 - acc: 0.0987\n",
      "Epoch 3/20\n",
      "147123/147123 [==============================] - 189s 1ms/step - loss: 0.0297 - acc: 0.0989\n",
      "Epoch 4/20\n",
      "147123/147123 [==============================] - 188s 1ms/step - loss: 0.0289 - acc: 0.0991\n",
      "Epoch 5/20\n",
      "147123/147123 [==============================] - 188s 1ms/step - loss: 0.0281 - acc: 0.0994\n",
      "Epoch 6/20\n",
      "147123/147123 [==============================] - 189s 1ms/step - loss: 0.0274 - acc: 0.0996\n",
      "Epoch 7/20\n",
      "147123/147123 [==============================] - 14627s 99ms/step - loss: 0.0268 - acc: 0.0999\n",
      "Epoch 8/20\n",
      "147123/147123 [==============================] - 6937s 47ms/step - loss: 0.0262 - acc: 0.1002\n",
      "Epoch 9/20\n",
      "147123/147123 [==============================] - 223s 2ms/step - loss: 0.0257 - acc: 0.1005\n",
      "Epoch 10/20\n",
      "147123/147123 [==============================] - 208s 1ms/step - loss: 0.0251 - acc: 0.1008\n",
      "Epoch 11/20\n",
      "147123/147123 [==============================] - 200s 1ms/step - loss: 0.0245 - acc: 0.1010\n",
      "Epoch 12/20\n",
      "147123/147123 [==============================] - 204s 1ms/step - loss: 0.0240 - acc: 0.1012\n",
      "Epoch 13/20\n",
      "147123/147123 [==============================] - 194s 1ms/step - loss: 0.0234 - acc: 0.1014\n",
      "Epoch 14/20\n",
      "147123/147123 [==============================] - 192s 1ms/step - loss: 0.0230 - acc: 0.1017\n",
      "Epoch 15/20\n",
      "147123/147123 [==============================] - 195s 1ms/step - loss: 0.0224 - acc: 0.1019\n",
      "Epoch 16/20\n",
      "147123/147123 [==============================] - 196s 1ms/step - loss: 0.0220 - acc: 0.1020\n",
      "Epoch 17/20\n",
      "147123/147123 [==============================] - 201s 1ms/step - loss: 0.0214 - acc: 0.1023\n",
      "Epoch 18/20\n",
      "147123/147123 [==============================] - 197s 1ms/step - loss: 0.0211 - acc: 0.1023\n",
      "Epoch 19/20\n",
      "147123/147123 [==============================] - 197s 1ms/step - loss: 0.0206 - acc: 0.1024\n",
      "Epoch 20/20\n",
      "147123/147123 [==============================] - 197s 1ms/step - loss: 0.0203 - acc: 0.1025\n",
      "RMSE BEFORE DENORMALIZING:  0.1806041868547733\n",
      "**** METRICS BASED ON DENORMALIZED SCORE FOR CNN+LSTM****\n",
      "**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\n",
      "RMSE:  3.110587926129043\n",
      "Kappa Quadratic Weighting:  0.9678594490860415\n",
      "Accuracy:  0.30618848224482026\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator = KerasRegressor(build_fn=CNN_lstm, epochs=20, batch_size=500)\n",
    "estimator.fit(train_data, y_train)\n",
    "\n",
    "# Precit for test data\n",
    "prediction_cnn_lstm=estimator.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_cnn_lstm)\n",
    "print(\"RMSE BEFORE DENORMALIZING: \",rmse_val)\n",
    "\n",
    "# Construct data frame for denormalizing the score\n",
    "cnn_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_cnn_lstm.astype(np.double)]).transpose()\n",
    "cnn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "cnn_df['Mult_Factor'] = cnn_df.apply(find_mult_factor,axis=1)\n",
    "cnn_df['Denorm_Pred_Score'] = cnn_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = cnn_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = cnn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "rmse_cnn = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score,weights='quadratic')\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "\n",
    "print(\"**** METRICS BASED ON DENORMALIZED SCORE FOR CNN+LSTM****\")\n",
    "print(\"**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\")\n",
    "print(\"RMSE: \",rmse_cnn)\n",
    "print(\"Kappa Quadratic Weighting: \",cohen_kappa)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### CNN LSTM Results : MAX Score of all Sentences ###\n",
      "RMSE:  3.2244013789688863\n",
      "Cohen Kappa:  0.2216361717035582\n",
      "Accuracy:  0.31278890600924497\n",
      "### CNN LSTM Results : Min Score of all Sentences ###\n",
      "RMSE:  3.7567094574390114\n",
      "Cohen Kappa:  0.18549105763328888\n",
      "Accuracy:  0.2862095531587057\n",
      "### CNN LSTM Results : Mean Score of all Sentences ###\n",
      "RMSE:  2.0855176017430965\n",
      "Cohen Kappa:  0.3079471136540092\n",
      "Cohen Kappa Quadratic:  0.971148842517859\n",
      "Accuracy:  0.40177195685670264\n",
      "   essay_set      RMSE     Kappa   Kappa_Q  Accuracy  Norm_RMSE\n",
      "0        1.0  1.362054  0.039019  0.247982  0.333333   0.112168\n",
      "1        2.0  0.745754  0.257717  0.385655  0.569519   0.143538\n",
      "2        3.0  0.785345  0.065146  0.229398  0.401198   0.262401\n",
      "3        4.0  0.625833  0.448674  0.749393  0.616667   0.285012\n",
      "4        5.0  0.947919  0.093976  0.270812  0.394203   0.223022\n",
      "5        6.0  0.788475  0.157477  0.548532  0.497354   0.219715\n",
      "6        7.0  4.272583 -0.012110  0.207912  0.066225   0.170708\n",
      "7        8.0  5.402757  0.002427  0.270151  0.058394   0.107979\n",
      "Mean Quadratic Kappa:  0.3637293601285294\n"
     ]
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,cnn_df.Orig_Score.values,\n",
    "                          cnn_df.Norm_Score,cnn_df.Denorm_Pred_Score.values,\n",
    "                          cnn_df.Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','norm_orig_score',\n",
    "                     'pred_score','norm_pred_score']\n",
    "cnn_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "cnn_test_df.essay_id = cnn_test_df.essay_id.astype(int)\n",
    "cnn_test_df.essay_set = cnn_test_df.essay_set.astype(int)\n",
    "cnn_test_df.Orig_Score = cnn_test_df.Orig_Score.astype(int)\n",
    "cnn_test_df.Max_Score = cnn_test_df.Max_Score.astype(int)\n",
    "cnn_test_df.Min_Score = cnn_test_df.Min_Score.astype(int)\n",
    "cnn_test_df.Mean_Score = cnn_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = cnn_test_df.Orig_Score.values\n",
    "max_pred_score = cnn_test_df.Max_Score.values\n",
    "min_pred_score = cnn_test_df.Min_Score.values\n",
    "mean_pred_score = cnn_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_cnn = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_cnn = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_cnn = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_cnn = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_cnn = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_cnn = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_cnn = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_cnn = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_q_cnn = cohen_kappa_score(orig_score,mean_pred_score,weights='quadratic')\n",
    "accuracy_mean_cnn = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### CNN LSTM Results : MAX Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_cnn)\n",
    "print(\"Accuracy: \",accuracy_max_cnn)\n",
    "\n",
    "print(\"### CNN LSTM Results : Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_cnn)\n",
    "print(\"Accuracy: \",accuracy_min_cnn)\n",
    "\n",
    "print(\"### CNN LSTM Results : Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_cnn)\n",
    "print(\"Cohen Kappa Quadratic: \",cohen_kappa_mean_q_cnn)\n",
    "print(\"Accuracy: \",accuracy_mean_cnn)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(cnn_test_df)\n",
    "print(essay_set_results)\n",
    "print(\"Mean Quadratic Kappa: \",np.mean(essay_set_results.Kappa_Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147123/147123 [==============================] - 264s 2ms/step - loss: 0.0374 - acc: 0.0971\n",
      "Epoch 2/20\n",
      "147123/147123 [==============================] - 260s 2ms/step - loss: 0.0290 - acc: 0.0990\n",
      "Epoch 3/20\n",
      "147123/147123 [==============================] - 258s 2ms/step - loss: 0.0282 - acc: 0.0993\n",
      "Epoch 4/20\n",
      "147123/147123 [==============================] - 254s 2ms/step - loss: 0.0278 - acc: 0.0995\n",
      "Epoch 5/20\n",
      "147123/147123 [==============================] - 262s 2ms/step - loss: 0.0274 - acc: 0.0998\n",
      "Epoch 6/20\n",
      "147123/147123 [==============================] - 257s 2ms/step - loss: 0.0271 - acc: 0.1001\n",
      "Epoch 7/20\n",
      "147123/147123 [==============================] - 271s 2ms/step - loss: 0.0267 - acc: 0.1002\n",
      "Epoch 8/20\n",
      "147123/147123 [==============================] - 261s 2ms/step - loss: 0.0265 - acc: 0.1005\n",
      "Epoch 9/20\n",
      "147123/147123 [==============================] - 258s 2ms/step - loss: 0.0261 - acc: 0.1006\n",
      "Epoch 10/20\n",
      "147123/147123 [==============================] - 247s 2ms/step - loss: 0.0259 - acc: 0.1008\n",
      "Epoch 11/20\n",
      "147123/147123 [==============================] - 248s 2ms/step - loss: 0.0256 - acc: 0.1009\n",
      "Epoch 12/20\n",
      "147123/147123 [==============================] - 252s 2ms/step - loss: 0.0254 - acc: 0.1012\n",
      "Epoch 13/20\n",
      "147123/147123 [==============================] - 250s 2ms/step - loss: 0.0251 - acc: 0.1014\n",
      "Epoch 14/20\n",
      "147123/147123 [==============================] - 251s 2ms/step - loss: 0.0249 - acc: 0.1014\n",
      "Epoch 15/20\n",
      "147123/147123 [==============================] - 247s 2ms/step - loss: 0.0246 - acc: 0.1016\n",
      "Epoch 16/20\n",
      "147123/147123 [==============================] - 260s 2ms/step - loss: 0.0245 - acc: 0.1017\n",
      "Epoch 17/20\n",
      "147123/147123 [==============================] - 254s 2ms/step - loss: 0.0242 - acc: 0.1018\n",
      "Epoch 18/20\n",
      "147123/147123 [==============================] - 246s 2ms/step - loss: 0.0239 - acc: 0.1021\n",
      "Epoch 19/20\n",
      "147123/147123 [==============================] - 285s 2ms/step - loss: 0.0237 - acc: 0.1020\n",
      "Epoch 20/20\n",
      "147123/147123 [==============================] - 306s 2ms/step - loss: 0.0235 - acc: 0.1021\n",
      "RMSE:  0.1667374396558812\n",
      "**** METRICS BASED ON DENORMALIZED SCORE FOR SIMPLE CNN****\n",
      "**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\n",
      "RMSE:  2.670780538038357\n",
      "Kappa Quadratic Weighting:  0.977294121470357\n",
      "Accuracy:  0.32943607591494917\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator_lstm = KerasRegressor(build_fn=stack_lstm, epochs=20, batch_size=500)\n",
    "estimator_lstm.fit(train_data, y_train)\n",
    "\n",
    "# Predict for test data\n",
    "prediction_lstm=estimator_lstm.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_lstm)\n",
    "# Find the overall RMSE value. This is not very relevant considering different scale for each set.\n",
    "print(\"RMSE: \",rmse_val)\n",
    "\n",
    "# Construct pandas data frame of scores of each sentences\n",
    "lstm_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_lstm.astype(np.double)]).transpose()\n",
    "lstm_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "# Find Multiplication factor using the function we defined before\n",
    "lstm_df['Mult_Factor'] = lstm_df.apply(find_mult_factor,axis=1)\n",
    "# Find the denormalized predicted score\n",
    "lstm_df['Denorm_Pred_Score'] = lstm_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores of each sentences\n",
    "orig_score = lstm_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = lstm_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "\n",
    "# Provide the metrics at sentence level\n",
    "rmse_lstm = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score,weights='quadratic')\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "\n",
    "print(\"**** METRICS BASED ON DENORMALIZED SCORE FOR SIMPLE CNN****\")\n",
    "print(\"**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\")\n",
    "print(\"RMSE: \",rmse_lstm)\n",
    "print(\"Kappa Quadratic Weighting: \",cohen_kappa)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Stacked LSTM Results : MAX Score of all Sentences ###\n",
      "RMSE:  2.827950413977182\n",
      "Cohen Kappa:  0.20716939224632758\n",
      "Accuracy:  0.3020030816640986\n",
      "### Stacked LSTM Results : Min Score of all Sentences ###\n",
      "RMSE:  2.4956047650414077\n",
      "Cohen Kappa:  0.30119786959949246\n",
      "Accuracy:  0.39599383667180277\n",
      "### Stacked LSTM Results : Mean Score of all Sentences ###\n",
      "RMSE:  2.0995083436495148\n",
      "Cohen Kappa:  0.2767664946759322\n",
      "Cohen Kappa Quadratic:  0.9720147097476838\n",
      "Accuracy:  0.3701848998459168\n",
      "   essay_set      RMSE     Kappa   Kappa_Q  Accuracy  Norm_RMSE\n",
      "0        1.0  1.442902  0.037363  0.186567  0.243169   0.115721\n",
      "1        2.0  0.862157  0.140598  0.252677  0.483957   0.150902\n",
      "2        3.0  0.769944  0.094725  0.293567  0.416168   0.269017\n",
      "3        4.0  0.664580  0.371431  0.730797  0.558333   0.308396\n",
      "4        5.0  0.915249  0.128036  0.319211  0.414493   0.219347\n",
      "5        6.0  0.843588  0.099913  0.474886  0.478836   0.236111\n",
      "6        7.0  4.285352 -0.010010  0.252494  0.062914   0.169518\n",
      "7        8.0  5.364114 -0.001360  0.307573  0.058394   0.108026\n",
      "Mean Quadratic Kappa:  0.352221652399816\n"
     ]
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,lstm_df.Orig_Score.values,\n",
    "                          lstm_df.Norm_Score,lstm_df.Denorm_Pred_Score.values,\n",
    "                          lstm_df.Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','norm_orig_score',\n",
    "                     'pred_score','norm_pred_score']\n",
    "lstm_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "lstm_test_df.essay_id = lstm_test_df.essay_id.astype(int)\n",
    "lstm_test_df.essay_set = lstm_test_df.essay_set.astype(int)\n",
    "lstm_test_df.Orig_Score = lstm_test_df.Orig_Score.astype(int)\n",
    "lstm_test_df.Max_Score = lstm_test_df.Max_Score.astype(int)\n",
    "lstm_test_df.Min_Score = lstm_test_df.Min_Score.astype(int)\n",
    "lstm_test_df.Mean_Score = lstm_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = lstm_test_df.Orig_Score.values\n",
    "max_pred_score = lstm_test_df.Max_Score.values\n",
    "min_pred_score = lstm_test_df.Min_Score.values\n",
    "mean_pred_score = lstm_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_lstm = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_lstm = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_lstm = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_lstm = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_lstm = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_lstm = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_lstm = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_lstm = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_q_lstm = cohen_kappa_score(orig_score,mean_pred_score,weights='quadratic')\n",
    "accuracy_mean_lstm = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### Stacked LSTM Results : MAX Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_lstm)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_lstm)\n",
    "print(\"Accuracy: \",accuracy_max_lstm)\n",
    "\n",
    "print(\"### Stacked LSTM Results : Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_lstm)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_lstm)\n",
    "print(\"Accuracy: \",accuracy_min_lstm)\n",
    "\n",
    "print(\"### Stacked LSTM Results : Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_lstm)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_lstm)\n",
    "print(\"Cohen Kappa Quadratic: \",cohen_kappa_mean_q_lstm)\n",
    "print(\"Accuracy: \",accuracy_mean_lstm)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(lstm_test_df)\n",
    "print(essay_set_results)\n",
    "print(\"Mean Quadratic Kappa: \",np.mean(essay_set_results.Kappa_Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 175, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 52500)             0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 50)                2625050   \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 17,625,101\n",
      "Trainable params: 2,625,101\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ff_1 = FF_1_NN()\n",
    "ff_2 = FF_2_NN()\n",
    "ff_3 = FF_3_NN()\n",
    "ff_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 175, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 52500)             0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 50)                2625050   \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 17,630,201\n",
      "Trainable params: 2,630,201\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ff_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru_32 = GRU_32()\n",
    "gru_50 = GRU_50()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 175, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 32)                31968     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 15,032,001\n",
      "Trainable params: 32,001\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_32.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 175, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 32)                31968     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 15,032,001\n",
      "Trainable params: 32,001\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_ff = CNN_FF()\n",
    "cnn_lstm = CNN_lstm()\n",
    "s_lstm = stack_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 175, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 171, 64)           96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 42, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 100)               268900    \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,365,065\n",
      "Trainable params: 365,065\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_ff.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 175, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 171, 64)           96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 42, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,172,265\n",
      "Trainable params: 172,265\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 175, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 175, 32)           42624     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 175, 32)           8320      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 175, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 15,059,297\n",
      "Trainable params: 59,297\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "s_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
