{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Beau\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Beau\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Beau\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Pool size changed, may indicate binary incompatibility. Expected 48 from C header, got 64 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Beau\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: cymem.cymem.Address size changed, may indicate binary incompatibility. Expected 24 from C header, got 40 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# BEFORE RUNNING OPEN NEW BASH WINDOW AND NAVIGATE TO WHERE CORENLP FILES ARE THEN RUN THIS CODE TO SE \n",
    "\n",
    "# java -mx1g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9010 -timeout 15000\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "import spacy\n",
    "from spacy.attrs import ORTH\n",
    "import textacy\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_excel(\"long_answer/training_set_rel3.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_lg',disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['doc'] = raw_data.essay.apply(lambda essay: nlp(essay.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_height(root):\n",
    "    \"\"\"\n",
    "    Find the maximum depth (height) of the dependency parse of a spacy sentence by starting with its root\n",
    "    Code adapted from https://stackoverflow.com/questions/35920826/how-to-find-height-for-non-binary-tree\n",
    "    :param root: spacy.tokens.token.Token\n",
    "    :return: int, maximum height of sentence's dependency parse tree\n",
    "    \"\"\"\n",
    "    if not list(root.children):\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 + max(tree_height(x) for x in root.children)\n",
    "    \n",
    "def get_average_heights(paragraph):\n",
    "    \"\"\"\n",
    "    Computes average height of parse trees for each sentence in paragraph.\n",
    "    :param paragraph: spacy doc object or str\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    if type(paragraph) == str:\n",
    "        doc = nlp(paragraph)\n",
    "    else:\n",
    "        doc = paragraph\n",
    "    roots = [sent.root for sent in doc.sents]\n",
    "    return np.mean([tree_height(root) for root in roots])\n",
    "\n",
    "def get_variance_heights(paragraph):\n",
    "    \"\"\"\n",
    "    Computes average height of parse trees for each sentence in paragraph.\n",
    "    :param paragraph: spacy doc object or str\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    if type(paragraph) == str:\n",
    "        doc = nlp(paragraph)\n",
    "    else:\n",
    "        doc = paragraph\n",
    "    roots = [sent.root for sent in doc.sents]\n",
    "    return np.std([tree_height(root) for root in roots])\n",
    "\n",
    "def get_tree_heights(paragraph):\n",
    "    \"\"\"\n",
    "    Computes average height of parse trees for each sentence in paragraph.\n",
    "    :param paragraph: spacy doc object or str\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    if type(paragraph) == str:\n",
    "        doc = nlp(paragraph)\n",
    "    else:\n",
    "        doc = paragraph\n",
    "    roots = [sent.root for sent in doc.sents]\n",
    "    return [tree_height(root) for root in roots]\n",
    "\n",
    "def get_sentences(doc):\n",
    "    sents = list(doc.sents)\n",
    "    return sents\n",
    "\n",
    "def get_sentence_lengths(sentences):\n",
    "    return float(len(sentences))\n",
    "\n",
    "def get_word_counts(doc):\n",
    "    return doc.count_by(ORTH)\n",
    "\n",
    "def get_connectives(doc):\n",
    "    text = doc.text.lower()\n",
    "    connectives = [\n",
    "    'after',\n",
    "    'earlier',\n",
    "    'before',\n",
    "    'during',\n",
    "    'while',\n",
    "    'later',\n",
    "    'because',\n",
    "    'consequently',\n",
    "    'thus',\n",
    "    'both',\n",
    "    'additionally',\n",
    "    'furthermore',\n",
    "    'moreover',\n",
    "    'actually',\n",
    "    'as a result',\n",
    "    'due to',\n",
    "    'but',\n",
    "    'yet',\n",
    "    'however',\n",
    "    'although',\n",
    "    'nevertheless'\n",
    "    ]\n",
    "    total = 0\n",
    "    for connector in connectives:\n",
    "        total += text.count(connector)\n",
    "    return float((total/len(doc)))\n",
    "\n",
    "def get_pos(doc):\n",
    "    return [token.pos_ for token in doc]\n",
    "\n",
    "\n",
    "def get_posngrams(poslist,n):\n",
    "    posngrams = []\n",
    "    for item in range(len(poslist) - n + 1):\n",
    "        posngrams.append(tuple([poslist[item+i] for i in range(n)]))\n",
    "    return posngrams\n",
    "\n",
    "def get_posgrams_counts(list_grams):\n",
    "    posgrams_counts = defaultdict(int)\n",
    "    for gram in list_grams:\n",
    "        posgrams_counts[gram] += 1\n",
    "    return posgrams_counts\n",
    "\n",
    "def get_TF(list_dicts):\n",
    "    TF_dict = defaultdict(int)\n",
    "    for dictionary in list_dicts:\n",
    "        for gram in dictionary:\n",
    "            TF_dict[gram] += dictionary[gram]\n",
    "    return TF_dict\n",
    "\n",
    "def get_mean_tfTF(posgram_counts,TF):\n",
    "    tfTF_ratios = list()\n",
    "    for key, value in posgram_counts.items():\n",
    "        tfTF_ratios.append(value/TF[key])\n",
    "    return np.mean(tfTF_ratios)\n",
    "\n",
    "def get_posngram_ratio(posngrams):\n",
    "    if len(posngrams) > 0:\n",
    "        return float(len(set(posngrams))/len(posngrams))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_reading_scores(doc):\n",
    "    scores = textacy.TextStats(doc).readability_stats\n",
    "    del scores['smog_index']\n",
    "    return scores\n",
    "\n",
    "def get_word_lengths(doc):\n",
    "    lengths = list()\n",
    "    for word in doc:\n",
    "        if word.is_alpha:\n",
    "            lengths.append(float(len(word)))\n",
    "    return lengths\n",
    "\n",
    "def get_words_of_length(lengths, n, p):\n",
    "    count = 0\n",
    "    for length in lengths:\n",
    "        if length > n and length < p:\n",
    "            count += 1\n",
    "    return float(count)\n",
    "\n",
    "def get_type_token_ratio(doc):\n",
    "    unique_words = set(word for word in doc if word.is_alpha)\n",
    "    total_words = [word for word in doc if word.is_alpha]\n",
    "    return float(len(unique_words)/len(total_words))\n",
    "\n",
    "def get_similarity_scores(doc):\n",
    "    sents = [sent for sent in doc.sents]\n",
    "    similarity_scores = list()\n",
    "    for i in range(1,len(sents)):\n",
    "        sent1 = sents[i-1]\n",
    "        sent2 = sents[i]\n",
    "        similarity_scores.append(sent1.similarity(sent2))\n",
    "    return np.mean(similarity_scores)\n",
    "\n",
    "def nth_root(x,n):\n",
    "    return x ** (1/float(n))\n",
    "\n",
    "def get_yules_k(word_counts):\n",
    "    m1 =  sum(word_counts.values())\n",
    "    m2 = sum([freq ** 2 for freq in word_counts.values()])\n",
    "    if m1 == m2:\n",
    "        k = 0 \n",
    "    else:\n",
    "        i = (m1*m1) / (m2-m1)\n",
    "        k = 1/i * 10000\n",
    "        return float(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_data['word_tokens'] = raw_data.essay.apply(lambda essay: word_tokenize(essay))\n",
    "#raw_data['sentence_tokens'] = raw_data.essay.apply(lambda essay: sent_tokenize(essay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preengineering\n",
    "raw_data['sentences'] = raw_data.doc.apply(get_sentences)\n",
    "raw_data['word_counts'] = raw_data.doc.apply(get_word_counts)\n",
    "raw_data['word_lengths'] = raw_data.doc.apply(get_word_lengths)\n",
    "raw_data['pos'] = raw_data.doc.apply(get_pos)\n",
    "raw_data['pos_trigrams'] = raw_data.pos.apply(lambda pos: get_posngrams(pos, n=3))\n",
    "raw_data['pos_4grams'] = raw_data.pos.apply(lambda pos: get_posngrams(pos, n=4))\n",
    "raw_data['pos_trigram_counts'] = raw_data.pos_trigrams.apply(get_posgrams_counts)\n",
    "pos_TF = get_TF(raw_data.pos_trigram_counts)\n",
    "raw_data['tree_heights'] = raw_data.doc.apply(lambda doc: get_tree_heights(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Beau\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Lexical Features\n",
    "raw_data['words_length_4'] = raw_data.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 4,6))\n",
    "raw_data['words_length_6'] = raw_data.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 6,8))\n",
    "raw_data['words_length_8'] = raw_data.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 8,10))\n",
    "raw_data['words_length_10'] = raw_data.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 10,12))\n",
    "raw_data['words_length_12'] = raw_data.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 12,100))\n",
    "raw_data['mean_word_length'] = raw_data.word_lengths.apply(np.mean)\n",
    "raw_data['variance_word_length'] = raw_data.word_lengths.apply(np.std)\n",
    "raw_data['type_token_ratio'] = raw_data.doc.apply(get_type_token_ratio)\n",
    "\n",
    "\n",
    "# Length Features\n",
    "raw_data['essay_length'] = raw_data.doc.apply(len)\n",
    "raw_data['num_words'] = raw_data.doc.apply(lambda doc: float(len([word for word in doc if word.is_alpha])))\n",
    "raw_data['num_sentences'] = raw_data.sentences.apply(get_sentence_lengths)\n",
    "raw_data['mean_sentence_length'] = raw_data.num_words/raw_data.num_sentences\n",
    "raw_data['num_characters'] = raw_data.essay.apply(len)\n",
    "raw_data['fourth_root_num_characters'] = raw_data.num_characters.apply(nth_root, n=4)\n",
    "\n",
    "# # Occurrence Features\n",
    "raw_data['num_commas'] = raw_data.essay.apply(lambda essay: float(essay.count(',')))\n",
    "raw_data['num_periods'] = raw_data.essay.apply(lambda essay: float(essay.count('.')))\n",
    "raw_data['num_exclaim'] = raw_data.essay.apply(lambda essay: float(essay.count('!')))\n",
    "raw_data['num_question'] = raw_data.essay.apply(lambda essay: float(essay.count('?')))\n",
    "raw_data['num_semicolon'] = raw_data.essay.apply(lambda essay: float(essay.count(';')))\n",
    "raw_data['num_colon'] = raw_data.essay.apply(lambda essay: float(essay.count(':')))\n",
    "\n",
    "# # Style Features\n",
    "# FIX raw_data['vocabulary'] = raw_data.word_tokens.apply(lambda word_tokens: set(word.lower() for word in word_tokens if word.isalpha()))\n",
    "raw_data['vocab_size'] = raw_data.word_counts.apply(len)\n",
    "# raw_data['yules_k'] = raw_data.word_counts.apply(get_yules_k)\n",
    "\n",
    "# # Syntactical Features\n",
    "# # the number for these lengths comes from Chen and He 2013\n",
    "raw_data['sentence_lengths'] = raw_data.sentences.apply(lambda sentences: [len(sent) for sent in sentences])\n",
    "raw_data['very_short_sentences'] = raw_data.sentence_lengths.apply(lambda sentence_lengths: float(sum([length <= 10 for length in sentence_lengths])))\n",
    "raw_data['short_sentences'] = raw_data.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 10 and length <18 for length in sentence_lengths])))\n",
    "raw_data['medium_sentences'] = raw_data.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 18 and length <25 for length in sentence_lengths])))\n",
    "raw_data['long_sentences'] = raw_data.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 25 for length in sentence_lengths])))\n",
    "raw_data['variance_sentence_length'] = raw_data.sentence_lengths.apply(lambda sentence_lengths: np.std(sentence_lengths))\n",
    "\n",
    "raw_data['max_height'] = raw_data.tree_heights.apply(lambda heights: float(max(heights)))\n",
    "raw_data['sum_heights'] = raw_data.tree_heights.apply(sum)\n",
    "raw_data['mean_heights'] = raw_data.tree_heights.apply(np.mean)\n",
    "\n",
    "# raw_data['mean_sentence_similarity'] = raw_data.doc.apply(get_similarity_scores)\n",
    "\n",
    "# # POS Ngrams\n",
    "raw_data['pos_trigram_ratio'] = raw_data.pos_trigrams.apply(get_posngram_ratio)\n",
    "raw_data['pos_fourgram_ratio'] = raw_data.pos_4grams.apply(get_posngram_ratio)\n",
    "raw_data['mean_trigram_tfTF'] = raw_data.pos_trigram_counts.apply(lambda pos_trigram_counts: get_mean_tfTF(pos_trigram_counts, TF=pos_TF))\n",
    "\n",
    "# # Cohesion Features\n",
    "raw_data['connectives'] = raw_data.doc.apply(get_connectives)\n",
    "\n",
    "# Readability Features\n",
    "#raw_data['reading_scores'] = raw_data.doc.apply(get_reading_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "DivSeries = pd.DataFrame({'div': [12,5,3,3,4,4,25,50],'essay_set':[1,2,3,4,5,6,7,8]})\n",
    "eng_data = raw_data.merge(DivSeries, on='essay_set')\n",
    "eng_data['score'] = eng_data.domain1_score/eng_data['div']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata = eng_data.iloc[:,37:]\n",
    "trainingdata = trainingdata.drop(['sentence_lengths','div'], axis=1)\n",
    "# trainingdata = feature_data[feature_data.essay_set < 7]\n",
    "# trainingdata  = trainingdata.dropna(subset=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata['essay_set'] = raw_data.essay_set\n",
    "trainingdata['essay_id'] = raw_data.essay_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata.to_pickle(\"./engineered_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = dict()\n",
    "# for i in range(1,9):\n",
    "#     d[i] = raw_data[raw_data.essay_set==i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1,9):\n",
    "#     plt.figure()\n",
    "#     sns.distplot(d[i].essay_length)\n",
    "#     plt.title(\"Distribution of Essay Length Set{}\".format(i))\n",
    "\n",
    "# for i in range(1,9):\n",
    "#     plt.figure()\n",
    "#     sns.stripplot(x=\"words_length_10\", y=\"domain1_score\",\n",
    "#                 sizes=(1, 12), linewidth=0,\n",
    "#                 data=d[i], jitter=True)\n",
    "#     plt.title(\"Score by Number of Sentences Set{}\".format(i))\n",
    "\n",
    "# for i in range(1,9):\n",
    "#     plt.figure()\n",
    "#     sns.scatterplot(x=\"yules_k\", y=\"domain1_score\",\n",
    "#                 sizes=(1, 8), linewidth=0,\n",
    "#                 data=d[i])\n",
    "#     plt.title(\"Score Words Set{}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
