{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import randint as sp_randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_df = pd.read_pickle(\"./engineered_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['words_length_4', 'words_length_6', 'words_length_8', 'words_length_10',\n",
       "       'words_length_12', 'mean_word_length', 'variance_word_length',\n",
       "       'type_token_ratio', 'essay_length', 'num_words', 'num_sentences',\n",
       "       'mean_sentence_length', 'num_characters', 'fourth_root_num_characters',\n",
       "       'num_commas', 'num_periods', 'num_exclaim', 'num_question',\n",
       "       'num_semicolon', 'num_colon', 'vocab_size', 'yules_k',\n",
       "       'very_short_sentences', 'short_sentences', 'medium_sentences',\n",
       "       'long_sentences', 'variance_sentence_length', 'max_height',\n",
       "       'sum_heights', 'mean_heights', 'pos_trigram_ratio',\n",
       "       'pos_fourgram_ratio', 'mean_trigram_tfTF', 'mean_fourgram_tfTF',\n",
       "       'connectives', 'flesch_kincaid_grade_level', 'flesch_reading_ease',\n",
       "       'gunning_fog_index', 'coleman_liau_index',\n",
       "       'automated_readability_index', 'lix', 'gulpease_index',\n",
       "       'wiener_sachtextformel', 'score', 'essay_set', 'essay_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpickled_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingdata = unpickled_df[unpickled_df.essay_set < 7]\n",
    "trainingdata = unpickled_df\n",
    "DivSeries = pd.DataFrame({'div': [12,5,3,3,4,4,25,50],'essay_set':[1,2,3,4,5,6,7,8]})\n",
    "trainingdata = trainingdata.merge(DivSeries, on='essay_set')\n",
    "cols_at_end = ['score']\n",
    "trainingdata = trainingdata[[c for c in trainingdata if c not in cols_at_end] + [c for c in cols_at_end if c in trainingdata]]\n",
    "# trainingdata = trainingdata.drop(['tree_heights'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_trigram_tfTF     1\n",
      "mean_fourgram_tfTF    1\n",
      "score                 1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_columns=trainingdata.columns[trainingdata.isnull().any()]\n",
    "print(trainingdata[null_columns].isnull().sum())\n",
    "trainingdata = trainingdata.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, Y = trainingdata.iloc[:,:-3], trainingdata.loc[:,['score','essay_set']]\n",
    "# X, Y = trainingdata.drop(['score','essay_id'], axis=0), trainingdata.loc[:,['score','essay_set']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traincomp, testcomp = train_test_split(trainingdata,random_state=42,test_size=0.2)\n",
    "\n",
    "set_train = traincomp.loc[:,['essay_set','essay_id','div']]  # setting asside essay set and scale\n",
    "set_test = testcomp.loc[:,['essay_set','essay_id','div']] # setting asside essay set and scale\n",
    "\n",
    "x_train = np.asarray(traincomp.iloc[:,:-3])\n",
    "y_train = np.reshape(np.asarray(traincomp.loc[:,['score']]),(-1,))\n",
    "\n",
    "x_test = np.asarray(testcomp.iloc[:,:-3])\n",
    "y_test = np.reshape(np.asarray(testcomp.loc[:,['score']]),(-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null_columns=classification_data.columns[classification_data.isnull().any()]\n",
    "# classification_data[null_columns].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10380, 44)\n",
      "(2596, 44)\n",
      "(10380,)\n",
      "(2596,)\n"
     ]
    }
   ],
   "source": [
    "# print(sum(np.isnan(y_train)))\n",
    "# print(sum(np.isnan(y_test)))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(fit_intercept=True)\n",
    "rf = RandomForestRegressor(n_estimators=500, min_samples_split=10, min_samples_leaf=3, max_features='auto',max_depth=44, bootstrap=True, random_state=42)\n",
    "sv = svm.SVR(kernel='rbf',C=0.1,gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=0.1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x_train, y_train)\n",
    "rf.fit(x_train, y_train)\n",
    "sv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoringFrame_baselines = pd.DataFrame([y_test,set_test[:,0],set_test[:,1],rf.predict(x_test), sv.predict(x_test), lr.predict(x_test)]).transpose()\n",
    "# scoringFrame_baselines.columns = ['actual','essay_set','div','rf','sv','lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE is one possible eval score\n",
    "def RMSE(actual, predict):\n",
    "    diff = actual - predict\n",
    "    diff = sum(diff**2) / len(actual)\n",
    "    return np.sqrt(diff)\n",
    "\n",
    "def evaluate(model, x_test, y_test, div, essay_set):\n",
    "    predictions = model.predict(x_test)\n",
    "    y_rescaled = y_test * div\n",
    "    y_rescaled = y_rescaled.astype(int)\n",
    "    predictions_rescaled = predictions * div\n",
    "    predictions_round = predictions_rescaled.round()\n",
    "    predictions_round = predictions_round.astype(int)\n",
    "    rmse = RMSE(y_rescaled, predictions_round)\n",
    "    cohen = cohen_kappa_score(y_rescaled, predictions_round, weights=None)\n",
    "    quad_cohen = cohen_kappa_score(y_rescaled, predictions_round, weights='quadratic')\n",
    "    accuracy = accuracy_score(y_rescaled, predictions_round)\n",
    "    return {'rmse':rmse,'kappa':cohen,'quad_kappa':quad_cohen,'accuracy':accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 2.02250435917868 Cohen Kappa: 0.4001262423530352 Cohen QW-Kappa: 0.9741565966034074 Accuracy: 0.4761171032357473\n",
      "Random Forests RMSE: 1.5422903345729375 Cohen Kappa: 0.4665936551347196 Cohen QW-Kappa: 0.9843781904283819 Accuracy: 0.5342835130970724\n",
      "Support Vector RMSE: 1.8743257647535838 Cohen Kappa: 0.40089386711048913 Cohen QW-Kappa: 0.9774461651725512 Accuracy: 0.4761171032357473\n"
     ]
    }
   ],
   "source": [
    "lr_metrics = evaluate(lr,x_test,y_test,div=set_test.loc[:,'div'],essay_set=set_test.loc[:,'essay_set'])\n",
    "rf_metrics = evaluate(rf,x_test,y_test,div=set_test.loc[:,'div'],essay_set=set_test.loc[:,'essay_set'])\n",
    "sv_metrics = evaluate(sv,x_test,y_test,div=set_test.loc[:,'div'],essay_set=set_test.loc[:,'essay_set'])\n",
    "print(\"Linear Regression RMSE: {0} Cohen Kappa: {1} Cohen QW-Kappa: {2} Accuracy: {3}\".format(lr_metrics['rmse'], lr_metrics['kappa'],lr_metrics['quad_kappa'],lr_metrics['accuracy']))\n",
    "print(\"Random Forests RMSE: {0} Cohen Kappa: {1} Cohen QW-Kappa: {2} Accuracy: {3}\".format(rf_metrics['rmse'], rf_metrics['kappa'], rf_metrics['quad_kappa'],rf_metrics['accuracy']))\n",
    "print(\"Support Vector RMSE: {0} Cohen Kappa: {1} Cohen QW-Kappa: {2} Accuracy: {3}\".format(sv_metrics['rmse'], sv_metrics['kappa'],sv_metrics['quad_kappa'],sv_metrics['accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_set      RMSE     Kappa  Accuracy  QW-Kappa\n",
      "0        1.0  0.837965  0.337357  0.500000  0.337357\n",
      "1        2.0  0.573478  0.525508  0.708556  0.525508\n",
      "2        3.0  0.670151  0.427363  0.628743  0.427363\n",
      "3        4.0  0.654047  0.451525  0.627778  0.451525\n",
      "4        5.0  0.594662  0.524375  0.663768  0.524375\n",
      "5        6.0  0.747129  0.328884  0.560847  0.328884\n",
      "6        7.0  3.053063  0.101793  0.162252  0.101793\n",
      "7        8.0  4.135479  0.047277  0.094891  0.047277\n",
      "   essay_set      RMSE     Kappa  Accuracy  QW-Kappa\n",
      "0        1.0  1.113749  0.185354  0.374317  0.185354\n",
      "1        2.0  0.685994  0.341279  0.598930  0.341279\n",
      "2        3.0  0.689962  0.338168  0.577844  0.338168\n",
      "3        4.0  0.679052  0.364771  0.586111  0.364771\n",
      "4        5.0  0.623222  0.438439  0.620290  0.438439\n",
      "5        6.0  0.755929  0.275904  0.558201  0.275904\n",
      "6        7.0  3.162278  0.049620  0.115894  0.049620\n",
      "7        8.0  6.778024  0.032291  0.080292  0.032291\n",
      "   essay_set      RMSE     Kappa  Accuracy  QW-Kappa\n",
      "0        1.0  1.166471  0.178044  0.374317  0.178044\n",
      "1        2.0  0.763471  0.244300  0.548128  0.244300\n",
      "2        3.0  0.674603  0.403380  0.616766  0.403380\n",
      "3        4.0  0.699206  0.343629  0.575000  0.343629\n",
      "4        5.0  0.637022  0.445113  0.620290  0.445113\n",
      "5        6.0  0.748897  0.344330  0.568783  0.344330\n",
      "6        7.0  3.431930  0.071546  0.135762  0.071546\n",
      "7        8.0  5.515076  0.022703  0.080292  0.022703\n"
     ]
    }
   ],
   "source": [
    "rf_scoring_df = pd.DataFrame([y_test,set_test.loc[:,'essay_set'],set_test.loc[:,'div'],rf.predict(x_test)]).transpose()\n",
    "rf_scoring_df.columns = ['actual','essay_set','div','prediction']\n",
    "\n",
    "lr_scoring_df = pd.DataFrame([y_test,set_test.loc[:,'essay_set'],set_test.loc[:,'div'],lr.predict(x_test)]).transpose()\n",
    "lr_scoring_df.columns = ['actual','essay_set','div','prediction']\n",
    "\n",
    "sv_scoring_df = pd.DataFrame([y_test,set_test.loc[:,'essay_set'],set_test.loc[:,'div'],sv.predict(x_test)]).transpose()\n",
    "sv_scoring_df.columns = ['actual','essay_set','div','prediction']\n",
    "\n",
    "def essay_set_metrics(df):\n",
    "    df['y_rescaled'] = df.actual * df['div']\n",
    "    df.y_rescaled = df.y_rescaled.astype(int)\n",
    "    df['prediction_rescaled'] = df.prediction * df['div']\n",
    "    df['prediction_round'] = df.prediction_rescaled.round()\n",
    "    df.prediction_round = df.prediction_round.astype(int)\n",
    "    set_df = pd.DataFrame(columns=['essay_set','RMSE','Kappa','Accuracy'])\n",
    "    e_sets = np.unique(df.essay_set)\n",
    "    for e_s in e_sets:\n",
    "        df_s = df[df.essay_set == e_s]\n",
    "        original_score = df_s.y_rescaled.values\n",
    "        predicted_score = df_s.prediction_round.values\n",
    "        rmse = RMSE(original_score,predicted_score)\n",
    "        kappa = cohen_kappa_score(original_score,predicted_score, weights=None)\n",
    "        quad_kappa = cohen_kappa_score(original_score,predicted_score, weights='quadratic')\n",
    "        accuracy = accuracy_score(original_score,predicted_score)\n",
    "        set_df = set_df.append({'essay_set':e_s,'RMSE':rmse,'Kappa':kappa,'QW-Kappa':kappa,'Accuracy':accuracy},\n",
    "                             ignore_index=True)\n",
    "    return set_df\n",
    "\n",
    "print(essay_set_metrics(rf_scoring_df))\n",
    "print(essay_set_metrics(lr_scoring_df))\n",
    "print(essay_set_metrics(sv_scoring_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01548982, 0.00414467, 0.01638003, 0.00832988, 0.01119178,\n",
       "       0.0111116 , 0.0137667 , 0.01336969, 0.01160804, 0.00466289,\n",
       "       0.0081578 , 0.00689729, 0.01037676, 0.00478127, 0.00709852,\n",
       "       0.00502207, 0.00808653, 0.0558265 ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vocab_size</th>\n",
       "      <td>0.252920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>essay_length</th>\n",
       "      <td>0.209961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fourth_root_num_characters</th>\n",
       "      <td>0.095460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_characters</th>\n",
       "      <td>0.083960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>essay_set</th>\n",
       "      <td>0.055826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_word_length</th>\n",
       "      <td>0.020335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variance_word_length</th>\n",
       "      <td>0.016607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_heights</th>\n",
       "      <td>0.016380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variance_sentence_length</th>\n",
       "      <td>0.015490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_trigram_tfTF</th>\n",
       "      <td>0.013767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_fourgram_tfTF</th>\n",
       "      <td>0.013370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yules_k</th>\n",
       "      <td>0.013289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_token_ratio</th>\n",
       "      <td>0.011928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>connectives</th>\n",
       "      <td>0.011608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_trigram_ratio</th>\n",
       "      <td>0.011192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_fourgram_ratio</th>\n",
       "      <td>0.011112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <td>0.010377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_periods</th>\n",
       "      <td>0.010331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            importances\n",
       "vocab_size                     0.252920\n",
       "essay_length                   0.209961\n",
       "fourth_root_num_characters     0.095460\n",
       "num_characters                 0.083960\n",
       "essay_set                      0.055826\n",
       "mean_word_length               0.020335\n",
       "variance_word_length           0.016607\n",
       "sum_heights                    0.016380\n",
       "variance_sentence_length       0.015490\n",
       "mean_trigram_tfTF              0.013767\n",
       "mean_fourgram_tfTF             0.013370\n",
       "yules_k                        0.013289\n",
       "type_token_ratio               0.011928\n",
       "connectives                    0.011608\n",
       "pos_trigram_ratio              0.011192\n",
       "pos_fourgram_ratio             0.011112\n",
       "coleman_liau_index             0.010377\n",
       "num_periods                    0.010331"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = trainingdata.columns[:-3]\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "# plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "# plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "# plt.xlabel('Relative Importance')\n",
    "# plt.show()\n",
    "\n",
    "a = pd.DataFrame(data=importances,index=features,columns=['importances']).sort_values(by=['importances'],ascending=False)\n",
    "a[a.importances > 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Create the parameter grid based on the results of random search \n",
    "# param_grid = {\n",
    "#     'n_estimators': [500],\n",
    "#     'min_samples_split': [10, 12],\n",
    "#     'min_samples_leaf': [2,3],\n",
    "#     'max_features': ['auto'],\n",
    "#     'max_depth': [40, 44],\n",
    "#     'bootstrap': [True]  \n",
    "# }\n",
    "# # Create a based model\n",
    "# rf = RandomForestRegressor()\n",
    "# # Instantiate the grid search model\n",
    "# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "#                           cv = 3, n_jobs = -1, verbose = 2)\n",
    "# grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = {\n",
    "#     'kernel': ['linear','rbf'],\n",
    "#     'gamma': ['auto'],\n",
    "#     'C':[0.1,1]\n",
    "# }\n",
    "\n",
    "# sv = svm.SVR()\n",
    "\n",
    "# sv_grid = GridSearchCV(estimator = sv, param_grid= grid, cv = 3, verbose=4, n_jobs = -1)\n",
    "# sv_grid.fit(x_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
