{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Activation, Dropout, Conv1D, MaxPooling1D, Bidirectional, Flatten, TimeDistributed\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv(r'training_set_rel3.tsv',sep='\\t', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentenceEnders=r'!.?'\n",
    "SentenceContinuation=r',:;-'\n",
    "\n",
    "def EssayLength(Essay):\n",
    "    return len(Essay.split())\n",
    "\n",
    "def CountSentences(Essay):\n",
    "    count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "    return count(Essay, set(SentenceEnders))\n",
    "\n",
    "def CountContinuation(Essay):\n",
    "    count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
    "    return count(Essay, set(SentenceContinuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data = all_data\n",
    "DivSeries = pd.DataFrame({'div': [12,5,3,3,4,4,25,50],'dataset':[1,2,3,4,5,6,7,8]})\n",
    "\n",
    "# score normalization\n",
    "for i in all_data.essay_set.unique():\n",
    "    if(i==1):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 12\n",
    "    elif (i==2):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 5\n",
    "    elif (i in (3,4)):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 3\n",
    "    elif (i in (5,6)):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 4\n",
    "    elif (i == 7):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 25\n",
    "    else:\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 50\n",
    "\n",
    "\n",
    "train_sa_x_class,test_sa_x_class,train_sa_y_class,test_sa_y_class = train_test_split(np.asarray(classification_data.essay), classification_data[['adjusted_domain1_score','essay_set']],test_size=0.2, random_state=42)\n",
    "\n",
    "max_len_class = all_data.essay.apply(EssayLength).sort_values(ascending=True).iloc[int(np.floor(len(all_data)*.95))]\n",
    "\n",
    "tok_class = Tokenizer()\n",
    "tok_class.fit_on_texts(pd.Series(train_sa_x_class))\n",
    "sequences_class = tok_class.texts_to_sequences(train_sa_x_class)\n",
    "sequences_matrix_class = sequence.pad_sequences(sequences_class,maxlen=max_len_class)\n",
    "\n",
    "sequences_test_class = tok_class.texts_to_sequences(test_sa_x_class)\n",
    "sequences_test_matrix_class = sequence.pad_sequences(sequences_test_class,maxlen=max_len_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_data = all_data\n",
    "DivSeries = pd.DataFrame({'div': [12,5,3,3,4,4,25,50],'dataset':[1,2,3,4,5,6,7,8]})\n",
    "\n",
    "# score normalization\n",
    "for i in all_data.essay_set.unique():\n",
    "    if(i==1):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 12\n",
    "    elif (i==2):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 5\n",
    "    elif (i in (3,4)):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 3\n",
    "    elif (i in (5,6)):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 4\n",
    "    elif (i == 7):\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 25\n",
    "    else:\n",
    "        classification_data.loc[classification_data.essay_set == i, 'adjusted_domain1_score'] = classification_data.loc[classification_data.essay_set == i, 'domain1_score'] / 50\n",
    "\n",
    "\n",
    "train_sa_x_class,test_sa_x_class,train_sa_y_class,test_sa_y_class = train_test_split(np.asarray(classification_data.essay), classification_data[['adjusted_domain1_score','essay_set']],test_size=0.2, random_state=42)\n",
    "\n",
    "max_len_class = all_data.essay.apply(EssayLength).sort_values(ascending=True).iloc[int(np.floor(len(all_data)*.95))]\n",
    "\n",
    "tok_class = Tokenizer()\n",
    "tok_class.fit_on_texts(pd.Series(train_sa_x_class))\n",
    "sequences_class = tok_class.texts_to_sequences(train_sa_x_class)\n",
    "sequences_matrix_class = sequence.pad_sequences(sequences_class,maxlen=max_len_class)\n",
    "\n",
    "sequences_test_class = tok_class.texts_to_sequences(test_sa_x_class)\n",
    "sequences_test_matrix_class = sequence.pad_sequences(sequences_test_class,maxlen=max_len_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c8c51211564b4895f362a84dc19e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36434), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#looking to pull glove embeddings so no embedding training required.\n",
    "import csv\n",
    "gloves = pd.read_table(r\"glove.42B.300d.txt\", sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((len(tok_class.word_index)+1, 300))\n",
    "for word, i in tqdm_notebook(tok_class.word_index.items()):\n",
    "    if word in gloves.index:\n",
    "        embedding_matrix[i] = np.asarray(gloves.loc[word])\n",
    "    else:\n",
    "        embedding_matrix[i] = np.zeros(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed-forward networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 2.8057 - acc: 0.1944 - val_loss: 2.4925 - val_acc: 0.1908\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 8s 957us/step - loss: 2.1753 - acc: 0.2095 - val_loss: 2.3643 - val_acc: 0.2338\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 8s 962us/step - loss: 1.6923 - acc: 0.3398 - val_loss: 2.1719 - val_acc: 0.3571\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 9s 983us/step - loss: 0.9683 - acc: 0.6758 - val_loss: 2.0156 - val_acc: 0.4586\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 9s 987us/step - loss: 0.4367 - acc: 0.8649 - val_loss: 2.0072 - val_acc: 0.4798\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 9s 972us/step - loss: 0.2051 - acc: 0.9512 - val_loss: 2.0725 - val_acc: 0.4920\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 9s 974us/step - loss: 0.1221 - acc: 0.9833 - val_loss: 2.2077 - val_acc: 0.4862\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 9s 974us/step - loss: 0.0877 - acc: 0.9899 - val_loss: 2.2846 - val_acc: 0.4804\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 9s 966us/step - loss: 0.0697 - acc: 0.9939 - val_loss: 2.3308 - val_acc: 0.4843\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 8s 948us/step - loss: 0.0575 - acc: 0.9957 - val_loss: 2.3813 - val_acc: 0.4843\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 8s 953us/step - loss: 0.0491 - acc: 0.9963 - val_loss: 2.4637 - val_acc: 0.4811\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 9s 1ms/step - loss: 0.0439 - acc: 0.9973 - val_loss: 2.4803 - val_acc: 0.4855\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 9s 968us/step - loss: 0.0385 - acc: 0.9976 - val_loss: 2.5291 - val_acc: 0.4817\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 9s 967us/step - loss: 0.0345 - acc: 0.9978 - val_loss: 2.5675 - val_acc: 0.4811\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 9s 983us/step - loss: 0.0316 - acc: 0.9984 - val_loss: 2.6000 - val_acc: 0.4759\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 8s 931us/step - loss: 0.0292 - acc: 0.9985 - val_loss: 2.6435 - val_acc: 0.4753\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 8s 922us/step - loss: 0.0272 - acc: 0.9989 - val_loss: 2.6603 - val_acc: 0.4785\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 9s 970us/step - loss: 0.0255 - acc: 0.9990 - val_loss: 2.7199 - val_acc: 0.4785\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 9s 971us/step - loss: 0.0244 - acc: 0.9990 - val_loss: 2.7488 - val_acc: 0.4804\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 9s 965us/step - loss: 0.0230 - acc: 0.9989 - val_loss: 2.7743 - val_acc: 0.4753\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 9s 965us/step - loss: 0.0220 - acc: 0.9991 - val_loss: 2.7828 - val_acc: 0.4753\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 9s 970us/step - loss: 0.0211 - acc: 0.9990 - val_loss: 2.8065 - val_acc: 0.4740\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 8s 962us/step - loss: 0.0203 - acc: 0.9990 - val_loss: 2.8259 - val_acc: 0.4733\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 9s 985us/step - loss: 0.0197 - acc: 0.9992 - val_loss: 2.8437 - val_acc: 0.4733\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 9s 999us/step - loss: 0.0191 - acc: 0.9992 - val_loss: 2.8844 - val_acc: 0.4721\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 9s 980us/step - loss: 0.0187 - acc: 0.9992 - val_loss: 2.8955 - val_acc: 0.4733\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 9s 974us/step - loss: 0.0182 - acc: 0.9992 - val_loss: 2.9216 - val_acc: 0.4727\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 9s 966us/step - loss: 0.0178 - acc: 0.9992 - val_loss: 2.9338 - val_acc: 0.4733\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 8s 948us/step - loss: 0.0174 - acc: 0.9992 - val_loss: 2.9435 - val_acc: 0.4695\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 8s 952us/step - loss: 0.0172 - acc: 0.9992 - val_loss: 2.9600 - val_acc: 0.4714\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 8s 959us/step - loss: 0.0168 - acc: 0.9992 - val_loss: 2.9903 - val_acc: 0.4714\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 8s 962us/step - loss: 0.0166 - acc: 0.9992 - val_loss: 3.0134 - val_acc: 0.4714\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 9s 968us/step - loss: 0.0164 - acc: 0.9992 - val_loss: 3.0196 - val_acc: 0.4721\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 9s 969us/step - loss: 0.0161 - acc: 0.9992 - val_loss: 3.0471 - val_acc: 0.4740\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 8s 941us/step - loss: 0.0159 - acc: 0.9992 - val_loss: 3.0577 - val_acc: 0.4714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd09c047ef0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_1 = Sequential()\n",
    "ff_1.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "ff_1.add(Dense(100,name='deep1'))\n",
    "ff_1.add(Flatten())\n",
    "#Regression\n",
    "# ff_1.add(Dense(1,activation='sigmoid',name='out_layer'))\n",
    "# ff_1.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "# ff_1.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#Classification\n",
    "ff_1.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),activation='sigmoid',name='out_layer'))\n",
    "ff_1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "ff_1.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 12s 1ms/step - loss: 2.7084 - acc: 0.1779 - val_loss: 2.3053 - val_acc: 0.2171\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 1.9089 - acc: 0.2373 - val_loss: 2.1948 - val_acc: 0.2563\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 1.3949 - acc: 0.4735 - val_loss: 2.0170 - val_acc: 0.4200\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.7450 - acc: 0.7462 - val_loss: 1.9146 - val_acc: 0.4701\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.4104 - acc: 0.8646 - val_loss: 2.0374 - val_acc: 0.4611\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.2568 - acc: 0.9250 - val_loss: 2.2464 - val_acc: 0.4554\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.1774 - acc: 0.9583 - val_loss: 2.3598 - val_acc: 0.4637\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.1294 - acc: 0.9736 - val_loss: 2.5062 - val_acc: 0.4611\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.1026 - acc: 0.9821 - val_loss: 2.6110 - val_acc: 0.4573\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0832 - acc: 0.9876 - val_loss: 2.7347 - val_acc: 0.4663\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0701 - acc: 0.9888 - val_loss: 2.8129 - val_acc: 0.4650\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0583 - acc: 0.9918 - val_loss: 2.8923 - val_acc: 0.4624\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0466 - acc: 0.9956 - val_loss: 2.9675 - val_acc: 0.4566\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0404 - acc: 0.9963 - val_loss: 3.0776 - val_acc: 0.4656\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0351 - acc: 0.9972 - val_loss: 3.1137 - val_acc: 0.4560\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0300 - acc: 0.9981 - val_loss: 3.1824 - val_acc: 0.4560\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0265 - acc: 0.9985 - val_loss: 3.2578 - val_acc: 0.4579\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0240 - acc: 0.9989 - val_loss: 3.3173 - val_acc: 0.4554\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0220 - acc: 0.9990 - val_loss: 3.3605 - val_acc: 0.4522\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0202 - acc: 0.9990 - val_loss: 3.4236 - val_acc: 0.4502\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0188 - acc: 0.9991 - val_loss: 3.4741 - val_acc: 0.4541\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0176 - acc: 0.9991 - val_loss: 3.5182 - val_acc: 0.4541\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0165 - acc: 0.9993 - val_loss: 3.5634 - val_acc: 0.4541\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0155 - acc: 0.9993 - val_loss: 3.6068 - val_acc: 0.4509\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0148 - acc: 0.9994 - val_loss: 3.6417 - val_acc: 0.4541\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0141 - acc: 0.9994 - val_loss: 3.6828 - val_acc: 0.4444\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0135 - acc: 0.9994 - val_loss: 3.7137 - val_acc: 0.4457\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0130 - acc: 0.9994 - val_loss: 3.7359 - val_acc: 0.4470\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0126 - acc: 0.9994 - val_loss: 3.7775 - val_acc: 0.4457\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0121 - acc: 0.9995 - val_loss: 3.8076 - val_acc: 0.4457\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0118 - acc: 0.9995 - val_loss: 3.8422 - val_acc: 0.4451\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0114 - acc: 0.9995 - val_loss: 3.8716 - val_acc: 0.4457\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0111 - acc: 0.9995 - val_loss: 3.8924 - val_acc: 0.4489\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0110 - acc: 0.9995 - val_loss: 3.9182 - val_acc: 0.4457\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 10s 1ms/step - loss: 0.0107 - acc: 0.9995 - val_loss: 3.9524 - val_acc: 0.4457\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0743b0d68>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_2 = Sequential()\n",
    "ff_2.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "ff_2.add(Dense(100,name='deep1'))\n",
    "ff_2.add(Dense(50,name='deep2'))\n",
    "ff_2.add(Flatten())\n",
    "#Regression\n",
    "# ff_2.add(Dense(1,activation='sigmoid',name='out_layer'))\n",
    "# ff_2.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "# ff_2.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#Classification\n",
    "ff_2.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),activation='sigmoid',name='out_layer'))\n",
    "ff_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "ff_2.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 14s 2ms/step - loss: 2.7220 - acc: 0.1944 - val_loss: 2.3549 - val_acc: 0.2017\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 2.0527 - acc: 0.2339 - val_loss: 2.1401 - val_acc: 0.2428\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 1.6547 - acc: 0.3376 - val_loss: 2.1438 - val_acc: 0.3365\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 1.1305 - acc: 0.5826 - val_loss: 1.9307 - val_acc: 0.4496\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.6820 - acc: 0.7537 - val_loss: 2.0448 - val_acc: 0.4297\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.4409 - acc: 0.8480 - val_loss: 2.4225 - val_acc: 0.4541\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.3022 - acc: 0.9030 - val_loss: 2.5805 - val_acc: 0.4522\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.2281 - acc: 0.9340 - val_loss: 2.7223 - val_acc: 0.4509\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.1735 - acc: 0.9547 - val_loss: 2.8927 - val_acc: 0.4425\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.1385 - acc: 0.9692 - val_loss: 3.0716 - val_acc: 0.4464\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.1132 - acc: 0.9777 - val_loss: 3.1853 - val_acc: 0.4438\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 12s 1ms/step - loss: 0.0958 - acc: 0.9833 - val_loss: 3.3346 - val_acc: 0.4387\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 12s 1ms/step - loss: 0.0890 - acc: 0.9832 - val_loss: 3.4575 - val_acc: 0.4438\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0748 - acc: 0.9870 - val_loss: 3.5774 - val_acc: 0.4374\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0632 - acc: 0.9905 - val_loss: 3.7125 - val_acc: 0.4374\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0620 - acc: 0.9899 - val_loss: 3.7754 - val_acc: 0.4387\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0526 - acc: 0.9921 - val_loss: 3.8752 - val_acc: 0.4303\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0463 - acc: 0.9952 - val_loss: 3.9767 - val_acc: 0.4367\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0433 - acc: 0.9955 - val_loss: 4.0631 - val_acc: 0.4297\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0414 - acc: 0.9954 - val_loss: 4.1385 - val_acc: 0.4329\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0393 - acc: 0.9961 - val_loss: 4.2281 - val_acc: 0.4232\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0368 - acc: 0.9968 - val_loss: 4.2852 - val_acc: 0.4290\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0342 - acc: 0.9974 - val_loss: 4.3529 - val_acc: 0.4277\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0324 - acc: 0.9978 - val_loss: 4.4177 - val_acc: 0.4329\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0312 - acc: 0.9978 - val_loss: 4.4881 - val_acc: 0.4284\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0304 - acc: 0.9980 - val_loss: 4.5443 - val_acc: 0.4265\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0292 - acc: 0.9982 - val_loss: 4.5909 - val_acc: 0.4284\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0285 - acc: 0.9982 - val_loss: 4.6417 - val_acc: 0.4303\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0279 - acc: 0.9982 - val_loss: 4.6925 - val_acc: 0.4290\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0276 - acc: 0.9982 - val_loss: 4.7334 - val_acc: 0.4297\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0271 - acc: 0.9982 - val_loss: 4.7775 - val_acc: 0.4277\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0266 - acc: 0.9982 - val_loss: 4.8247 - val_acc: 0.4258\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0262 - acc: 0.9982 - val_loss: 4.8631 - val_acc: 0.4239\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0258 - acc: 0.9982 - val_loss: 4.8981 - val_acc: 0.4277\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 11s 1ms/step - loss: 0.0255 - acc: 0.9982 - val_loss: 4.9329 - val_acc: 0.4271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd06dde0b00>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_3 = Sequential()\n",
    "ff_3.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "ff_3.add(Dense(100,name='deep1'))\n",
    "ff_3.add(Dense(50,name='deep2'))\n",
    "ff_3.add(Dense(25,name='deep3'))\n",
    "ff_3.add(Flatten())\n",
    "#Regression\n",
    "# ff_3.add(Dense(1,activation='sigmoid',name='out_layer'))\n",
    "# ff_3.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "# ff_3.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#Classification\n",
    "ff_3.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),activation='sigmoid',name='out_layer'))\n",
    "ff_3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "ff_3.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification only\n",
    "#required to convert from cat_CE output back to single prediction\n",
    "NormalizeSeries = pd.Series(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns)\n",
    "def RetrieveNormalize(IndexVal):\n",
    "    return NormalizeSeries.loc[IndexVal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 588, 300)          10930500  \n",
      "_________________________________________________________________\n",
      "deep1 (Dense)                (None, 588, 100)          30100     \n",
      "_________________________________________________________________\n",
      "deep2 (Dense)                (None, 588, 50)           5050      \n",
      "_________________________________________________________________\n",
      "deep3 (Dense)                (None, 588, 25)           1275      \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 14700)             0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 51)                749751    \n",
      "=================================================================\n",
      "Total params: 11,716,676\n",
      "Trainable params: 786,176\n",
      "Non-trainable params: 10,930,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ff_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b39a909c9764a93b2d0ba53592e8974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2596), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Weighted Quadrating Kappa scores: \n",
      "ff1    0.595457\n",
      "ff2    0.596776\n",
      "ff3    0.543137\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Classification\n",
    "scoringFrame_FF = pd.DataFrame(columns=['actual','dataset','feedforward','ff1','ff2', 'ff3'],index=range(0,len(test_sa_y_class)))\n",
    "for i in tqdm_notebook(scoringFrame_FF.index):\n",
    "    scoringFrame_FF.loc[i, 'actual'] = test_sa_y_class.adjusted_domain1_score.iloc[i]\n",
    "    scoringFrame_FF.loc[i, 'dataset'] = test_sa_y_class.essay_set.iloc[i]\n",
    "scoringFrame_FF.dataset = scoringFrame_FF.dataset.astype(int)\n",
    "\n",
    "ff_pred1 = ff_1.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))#.flatten()\n",
    "ff_pred2 = ff_2.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))#.flatten()\n",
    "ff_pred3 = ff_3.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))#.flatten()\n",
    "\n",
    "scoringFrame_FF['ff1'] = np.argmax(ff_pred1,axis=1)\n",
    "scoringFrame_FF['ff2'] = np.argmax(ff_pred2,axis=1)\n",
    "scoringFrame_FF['ff3'] = np.argmax(ff_pred3,axis=1)\n",
    "\n",
    "#NOTE - this way of scoring is much slower than the batch scoring done for CNN/RNN. Including it for completion's sake\n",
    "#regression\n",
    "# scoringFrame_FF = pd.DataFrame(columns=['actual','dataset','feedforward','ff1','ff2', 'ff3'],index=range(0,len(test_sa_y_class)))\n",
    "# for i in tqdm_notebook(scoringFrame_FF.index):\n",
    "#     scoringFrame_FF.loc[i, 'actual'] = test_sa_y_class.adjusted_domain1_score.iloc[i]\n",
    "#     scoringFrame_FF.loc[i, 'dataset'] = test_sa_y_class.essay_set.iloc[i]\n",
    "#     scoringFrame_FF.loc[i, 'ff1'] = ff_1.predict(sequences_test_matrix_class[i].reshape(1,max_len_class))[0][0]\n",
    "#     scoringFrame_FF.loc[i, 'ff2'] = ff_2.predict(sequences_test_matrix_class[i].reshape(1,max_len_class))[0][0]\n",
    "#     scoringFrame_FF.loc[i, 'ff3'] = ff_3.predict(sequences_test_matrix_class[i].reshape(1,max_len_class))[0][0]\n",
    "# scoringFrame_FF.dataset = scoringFrame_FF.dataset.astype(int)\n",
    "\n",
    "# scoringFrame_FF = scoringFrame_FF.merge(DivSeries, on='dataset')\n",
    "\n",
    "scoringFrame_FF = scoringFrame_FF.merge(DivSeries, on='dataset')\n",
    "\n",
    "for colName in ['actual','ff1','ff2','ff3']:\n",
    "    #classification only\n",
    "    if(colName != 'actual'):\n",
    "        scoringFrame_FF[colName] = scoringFrame_FF[colName].apply(RetrieveNormalize)\n",
    "    scoringFrame_FF[colName] = scoringFrame_FF[colName] * scoringFrame_FF['div']\n",
    "    scoringFrame_FF[colName] = scoringFrame_FF[colName].apply(round)\n",
    "\n",
    "scoringFrame_FF.actual = scoringFrame_FF.actual.astype(int)\n",
    "\n",
    "QuadKappaCalculation = pd.DataFrame(columns = ['ff1', 'ff2', 'ff3'],index = np.unique(scoringFrame_FF.dataset))\n",
    "for essaySetValue in np.unique(scoringFrame_FF.dataset):\n",
    "    temp_ff_ES = scoringFrame_FF[scoringFrame_FF.dataset == essaySetValue]\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'ff1'] = cohen_kappa_score(temp_ff_ES.actual, temp_ff_ES.ff1.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'ff2'] = cohen_kappa_score(temp_ff_ES.actual, temp_ff_ES.ff2.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'ff3'] = cohen_kappa_score(temp_ff_ES.actual, temp_ff_ES.ff3.apply(round),weights='quadratic')\n",
    "print(\"Mean Weighted Quadrating Kappa scores: \")\n",
    "print(QuadKappaCalculation.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff1 RMSE, Cohen, Quad Cohen, accuracy: 2.2775449707943762, 0.42612668433682255, 0.9680540716672278, 0.49768875192604006\n",
      "ff2 RMSE, Cohen, Quad Cohen, accuracy: 2.399280837294699, 0.41132930150606306, 0.9632877332798625, 0.48382126348228044\n",
      "ff3 RMSE, Cohen, Quad Cohen, accuracy: 2.4943696227905265, 0.3581157175355486, 0.9612553378657894, 0.43682588597842836\n"
     ]
    }
   ],
   "source": [
    "print(\"ff1 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(scoringFrame_FF.actual, scoringFrame_FF.ff1.apply(round))),\n",
    "                                       cohen_kappa_score(scoringFrame_FF.actual, scoringFrame_FF.ff1.apply(round)),\n",
    "                                        cohen_kappa_score(scoringFrame_FF.actual, scoringFrame_FF.ff1.apply(round),weights='quadratic'),\n",
    "                                                 accuracy_score(scoringFrame_FF.actual, scoringFrame_FF.ff1.apply(round))))\n",
    "\n",
    "print(\"ff2 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(scoringFrame_FF.actual, scoringFrame_FF.ff2.apply(round))),\n",
    "                                       cohen_kappa_score(scoringFrame_FF.actual, scoringFrame_FF.ff2.apply(round)),\n",
    "                                        cohen_kappa_score(scoringFrame_FF.actual, scoringFrame_FF.ff2.apply(round),weights='quadratic'),             \n",
    "                                      accuracy_score(scoringFrame_FF.actual, scoringFrame_FF.ff2.apply(round))))\n",
    "\n",
    "print(\"ff3 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(scoringFrame_FF.actual, scoringFrame_FF.ff3.apply(round))),\n",
    "                                       cohen_kappa_score(scoringFrame_FF.actual, scoringFrame_FF.ff3.apply(round)),\n",
    "                                       cohen_kappa_score(scoringFrame_FF.actual, scoringFrame_FF.ff3.apply(round),weights='quadratic'),               \n",
    "                                      accuracy_score(scoringFrame_FF.actual, scoringFrame_FF.ff3.apply(round))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 30s 3ms/step - loss: 0.6062 - acc: 0.0842 - val_loss: 0.0489 - val_acc: 0.1381\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 22s 2ms/step - loss: 0.0606 - acc: 0.1207 - val_loss: 0.0450 - val_acc: 0.1445\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0370 - acc: 0.1354 - val_loss: 0.0369 - val_acc: 0.1407\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0301 - acc: 0.1360 - val_loss: 0.0341 - val_acc: 0.1432\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0277 - acc: 0.1365 - val_loss: 0.0330 - val_acc: 0.1432\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0259 - acc: 0.1368 - val_loss: 0.0324 - val_acc: 0.1452\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0244 - acc: 0.1371 - val_loss: 0.0318 - val_acc: 0.1452\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0232 - acc: 0.1377 - val_loss: 0.0314 - val_acc: 0.1452\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 22s 2ms/step - loss: 0.0221 - acc: 0.1377 - val_loss: 0.0310 - val_acc: 0.1452\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0210 - acc: 0.1379 - val_loss: 0.0308 - val_acc: 0.1452\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0200 - acc: 0.1380 - val_loss: 0.0305 - val_acc: 0.1452\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0192 - acc: 0.1382 - val_loss: 0.0303 - val_acc: 0.1445\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0183 - acc: 0.1382 - val_loss: 0.0301 - val_acc: 0.1445\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0176 - acc: 0.1385 - val_loss: 0.0300 - val_acc: 0.1445\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0168 - acc: 0.1386 - val_loss: 0.0299 - val_acc: 0.1445\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0161 - acc: 0.1388 - val_loss: 0.0299 - val_acc: 0.1445\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0155 - acc: 0.1388 - val_loss: 0.0298 - val_acc: 0.1445\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0148 - acc: 0.1391 - val_loss: 0.0300 - val_acc: 0.1452\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0143 - acc: 0.1391 - val_loss: 0.0298 - val_acc: 0.1445\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0136 - acc: 0.1393 - val_loss: 0.0298 - val_acc: 0.1445\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0131 - acc: 0.1395 - val_loss: 0.0299 - val_acc: 0.1452\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0125 - acc: 0.1394 - val_loss: 0.0300 - val_acc: 0.1445\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0120 - acc: 0.1397 - val_loss: 0.0300 - val_acc: 0.1452\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0115 - acc: 0.1397 - val_loss: 0.0301 - val_acc: 0.1452\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0110 - acc: 0.1397 - val_loss: 0.0302 - val_acc: 0.1452\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0106 - acc: 0.1397 - val_loss: 0.0303 - val_acc: 0.1445\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0101 - acc: 0.1400 - val_loss: 0.0303 - val_acc: 0.1458\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0097 - acc: 0.1400 - val_loss: 0.0304 - val_acc: 0.1445\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 22s 2ms/step - loss: 0.0093 - acc: 0.1400 - val_loss: 0.0306 - val_acc: 0.1445\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0089 - acc: 0.1400 - val_loss: 0.0306 - val_acc: 0.1439\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0084 - acc: 0.1400 - val_loss: 0.0308 - val_acc: 0.1439\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0081 - acc: 0.1401 - val_loss: 0.0310 - val_acc: 0.1445\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0077 - acc: 0.1401 - val_loss: 0.0311 - val_acc: 0.1439\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 22s 3ms/step - loss: 0.0074 - acc: 0.1401 - val_loss: 0.0314 - val_acc: 0.1445\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0071 - acc: 0.1401 - val_loss: 0.0314 - val_acc: 0.1439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce8a8edc18>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_1 = Sequential()\n",
    "cnn_1.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "cnn_1.add(Conv1D(64, 5, activation='relu'))\n",
    "cnn_1.add(MaxPooling1D(pool_size=4))\n",
    "cnn_1.add(Flatten())\n",
    "#regression\n",
    "cnn_1.add(Dense(1,name='out_layer'))\n",
    "cnn_1.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "cnn_1.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#classification\n",
    "# cnn_1.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),name='out_layer'))\n",
    "# cnn_1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# cnn_1.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 30s 3ms/step - loss: 0.1055 - acc: 0.1125 - val_loss: 0.0429 - val_acc: 0.1362\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0369 - acc: 0.1350 - val_loss: 0.0344 - val_acc: 0.1452\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 25s 3ms/step - loss: 0.0264 - acc: 0.1367 - val_loss: 0.0321 - val_acc: 0.1464\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0214 - acc: 0.1375 - val_loss: 0.0308 - val_acc: 0.1452\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0181 - acc: 0.1383 - val_loss: 0.0304 - val_acc: 0.1458\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0158 - acc: 0.1387 - val_loss: 0.0298 - val_acc: 0.1458\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0139 - acc: 0.1392 - val_loss: 0.0298 - val_acc: 0.1458\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0122 - acc: 0.1397 - val_loss: 0.0302 - val_acc: 0.1458\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0109 - acc: 0.1401 - val_loss: 0.0298 - val_acc: 0.1471\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0097 - acc: 0.1403 - val_loss: 0.0298 - val_acc: 0.1464\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0086 - acc: 0.1403 - val_loss: 0.0304 - val_acc: 0.1471\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0077 - acc: 0.1403 - val_loss: 0.0305 - val_acc: 0.1464\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0068 - acc: 0.1403 - val_loss: 0.0308 - val_acc: 0.1464\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0061 - acc: 0.1403 - val_loss: 0.0310 - val_acc: 0.1452\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0054 - acc: 0.1403 - val_loss: 0.0314 - val_acc: 0.1464\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0048 - acc: 0.1403 - val_loss: 0.0317 - val_acc: 0.1452\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0042 - acc: 0.1405 - val_loss: 0.0320 - val_acc: 0.1458\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0037 - acc: 0.1408 - val_loss: 0.0324 - val_acc: 0.1458\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0033 - acc: 0.1408 - val_loss: 0.0328 - val_acc: 0.1458\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0029 - acc: 0.1408 - val_loss: 0.0329 - val_acc: 0.1458\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0026 - acc: 0.1408 - val_loss: 0.0331 - val_acc: 0.1458\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0022 - acc: 0.1408 - val_loss: 0.0335 - val_acc: 0.1458\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0020 - acc: 0.1410 - val_loss: 0.0337 - val_acc: 0.1458\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0017 - acc: 0.1410 - val_loss: 0.0342 - val_acc: 0.1458\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0015 - acc: 0.1410 - val_loss: 0.0343 - val_acc: 0.1452\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0013 - acc: 0.1410 - val_loss: 0.0348 - val_acc: 0.1452\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 0.0011 - acc: 0.1410 - val_loss: 0.0349 - val_acc: 0.1445\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 9.8658e-04 - acc: 0.1410 - val_loss: 0.0351 - val_acc: 0.1452\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 8.6914e-04 - acc: 0.1410 - val_loss: 0.0354 - val_acc: 0.1445\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 7.5698e-04 - acc: 0.1410 - val_loss: 0.0357 - val_acc: 0.1445\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 6.5524e-04 - acc: 0.1410 - val_loss: 0.0360 - val_acc: 0.1445\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 5.7445e-04 - acc: 0.1410 - val_loss: 0.0362 - val_acc: 0.1445\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 5.0414e-04 - acc: 0.1410 - val_loss: 0.0362 - val_acc: 0.1445\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 4.7829e-04 - acc: 0.1410 - val_loss: 0.0364 - val_acc: 0.1445\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 23s 3ms/step - loss: 4.5283e-04 - acc: 0.1410 - val_loss: 0.0368 - val_acc: 0.1445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce8b388978>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_2 = Sequential()\n",
    "cnn_2.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "cnn_2.add(Conv1D(64, 5, activation='relu'))\n",
    "cnn_2.add(MaxPooling1D(pool_size=4))\n",
    "cnn_2.add(Conv1D(20, 5, activation='relu'))\n",
    "cnn_2.add(MaxPooling1D(pool_size=2))\n",
    "cnn_2.add(Flatten())\n",
    "#regression\n",
    "cnn_2.add(Dense(1,name='out_layer'))\n",
    "cnn_2.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "cnn_2.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#classification\n",
    "# cnn_2.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),name='out_layer'))\n",
    "# cnn_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# cnn_2.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 31s 4ms/step - loss: 0.0641 - acc: 0.1305 - val_loss: 0.0352 - val_acc: 0.1452\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0287 - acc: 0.1368 - val_loss: 0.0286 - val_acc: 0.1471\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0233 - acc: 0.1373 - val_loss: 0.0268 - val_acc: 0.1458\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0201 - acc: 0.1375 - val_loss: 0.0264 - val_acc: 0.1458\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0180 - acc: 0.1379 - val_loss: 0.0261 - val_acc: 0.1464\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0163 - acc: 0.1384 - val_loss: 0.0261 - val_acc: 0.1464\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0148 - acc: 0.1388 - val_loss: 0.0260 - val_acc: 0.1464\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0136 - acc: 0.1392 - val_loss: 0.0263 - val_acc: 0.1464\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0123 - acc: 0.1393 - val_loss: 0.0262 - val_acc: 0.1458\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0112 - acc: 0.1394 - val_loss: 0.0263 - val_acc: 0.1458\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0101 - acc: 0.1397 - val_loss: 0.0265 - val_acc: 0.1464\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0092 - acc: 0.1399 - val_loss: 0.0270 - val_acc: 0.1458\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0084 - acc: 0.1401 - val_loss: 0.0272 - val_acc: 0.1458\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0077 - acc: 0.1401 - val_loss: 0.0276 - val_acc: 0.1458\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0068 - acc: 0.1401 - val_loss: 0.0277 - val_acc: 0.1458\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0060 - acc: 0.1401 - val_loss: 0.0282 - val_acc: 0.1452\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0054 - acc: 0.1402 - val_loss: 0.0286 - val_acc: 0.1445\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0049 - acc: 0.1402 - val_loss: 0.0292 - val_acc: 0.1445\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0044 - acc: 0.1403 - val_loss: 0.0297 - val_acc: 0.1445\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0039 - acc: 0.1403 - val_loss: 0.0300 - val_acc: 0.1445\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0035 - acc: 0.1403 - val_loss: 0.0304 - val_acc: 0.1445\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0031 - acc: 0.1404 - val_loss: 0.0307 - val_acc: 0.1445\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0028 - acc: 0.1404 - val_loss: 0.0312 - val_acc: 0.1445\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0025 - acc: 0.1404 - val_loss: 0.0314 - val_acc: 0.1445\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0022 - acc: 0.1405 - val_loss: 0.0318 - val_acc: 0.1439\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0020 - acc: 0.1405 - val_loss: 0.0321 - val_acc: 0.1452\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0019 - acc: 0.1407 - val_loss: 0.0324 - val_acc: 0.1452\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0017 - acc: 0.1407 - val_loss: 0.0328 - val_acc: 0.1445\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0015 - acc: 0.1407 - val_loss: 0.0330 - val_acc: 0.1445\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0014 - acc: 0.1407 - val_loss: 0.0331 - val_acc: 0.1439\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0013 - acc: 0.1408 - val_loss: 0.0334 - val_acc: 0.1439\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0013 - acc: 0.1408 - val_loss: 0.0334 - val_acc: 0.1439\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0012 - acc: 0.1408 - val_loss: 0.0338 - val_acc: 0.1445\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0012 - acc: 0.1409 - val_loss: 0.0338 - val_acc: 0.1439\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 0.0011 - acc: 0.1409 - val_loss: 0.0335 - val_acc: 0.1445\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce8ae203c8>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_3 = Sequential()\n",
    "cnn_3.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "cnn_3.add(Conv1D(64, 5, activation='relu'))\n",
    "cnn_3.add(MaxPooling1D(pool_size=4))\n",
    "cnn_3.add(Conv1D(20, 5, activation='relu'))\n",
    "cnn_3.add(MaxPooling1D(pool_size=2))\n",
    "cnn_3.add(Conv1D(20, 5, activation='relu'))\n",
    "cnn_3.add(MaxPooling1D(pool_size=2))\n",
    "cnn_3.add(Flatten())\n",
    "#regression\n",
    "cnn_3.add(Dense(1,name='out_layer'))\n",
    "cnn_3.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "cnn_3.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#classification\n",
    "# cnn_3.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),name='out_layer'))\n",
    "# cnn_3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# cnn_3.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN1    0.585416\n",
       "CNN2    0.512286\n",
       "CNN3    0.570201\n",
       "dtype: float64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_pred1 = cnn_1.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))#.flatten()\n",
    "cnn_pred2 = cnn_2.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))#.flatten()\n",
    "cnn_pred3 = cnn_3.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))#.flatten()\n",
    "\n",
    "#Uncomment in classification case\n",
    "# cnn_pred1 = np.argmax(cnn_pred1,axis=1)\n",
    "# cnn_pred2 = np.argmax(cnn_pred2,axis=1)\n",
    "# cnn_pred3 = np.argmax(cnn_pred3,axis=1)\n",
    "\n",
    "CNN_pred_frame = pd.DataFrame({'actual':np.asarray(test_sa_y_class.adjusted_domain1_score),\n",
    "                           'dataset': np.asarray(test_sa_y_class.essay_set),\n",
    "                           'CNN1':cnn_pred1.flatten(),\n",
    "                          'CNN2':cnn_pred2.flatten(),\n",
    "                          'CNN3':cnn_pred3.flatten()})\n",
    "\n",
    "cnn_scoring = CNN_pred_frame.merge(DivSeries, on='dataset')\n",
    "\n",
    "for colName in ['actual', 'CNN1', 'CNN2', 'CNN3']:\n",
    "    #Uncomment in classification case\n",
    "#     if(colName != 'actual'):\n",
    "#         cnn_scoring[colName] = cnn_scoring[colName].apply(RetrieveNormalize)\n",
    "    cnn_scoring[colName] = cnn_scoring[colName] * cnn_scoring['div']\n",
    "    cnn_scoring[colName] = cnn_scoring[colName].apply(round)\n",
    "\n",
    "cnn_scoring.actual = cnn_scoring.actual.astype(int)\n",
    "\n",
    "QuadKappaCalculation = pd.DataFrame(columns = ['CNN1', 'CNN2', 'CNN3'],index = np.unique(cnn_scoring.dataset))\n",
    "for essaySetValue in np.unique(cnn_scoring.dataset):\n",
    "    temp_CNN_ES = cnn_scoring[cnn_scoring.dataset == essaySetValue]\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'CNN1'] = cohen_kappa_score(temp_CNN_ES.actual, temp_CNN_ES.CNN1.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'CNN2'] = cohen_kappa_score(temp_CNN_ES.actual, temp_CNN_ES.CNN2.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'CNN3'] = cohen_kappa_score(temp_CNN_ES.actual, temp_CNN_ES.CNN3.apply(round),weights='quadratic')\n",
    "QuadKappaCalculation.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN1 RMSE, Cohen, Quad Cohen, accuracy: 2.5552447649759165, 0.36098051010302346, 0.961227187232454, 0.4391371340523883\n",
      "CNN2 RMSE, Cohen, Quad Cohen, accuracy: 3.072067105712875, 0.3304538502606913, 0.9441076119949335, 0.41140215716486905\n",
      "CNN3 RMSE, Cohen, Quad Cohen, accuracy: 2.4663373091793894, 0.3396593965310497, 0.9624113723352307, 0.4206471494607088\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN1 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(cnn_scoring.actual, cnn_scoring.CNN1.apply(round))),\n",
    "                                       cohen_kappa_score(cnn_scoring.actual, cnn_scoring.CNN1.apply(round)),\n",
    "                                        cohen_kappa_score(cnn_scoring.actual, cnn_scoring.CNN1.apply(round),weights='quadratic'),\n",
    "                                                 accuracy_score(cnn_scoring.actual, cnn_scoring.CNN1.apply(round))))\n",
    "\n",
    "print(\"CNN2 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(cnn_scoring.actual, cnn_scoring.CNN2.apply(round))),\n",
    "                                       cohen_kappa_score(cnn_scoring.actual, cnn_scoring.CNN2.apply(round)),\n",
    "                                        cohen_kappa_score(cnn_scoring.actual, cnn_scoring.CNN2.apply(round),weights='quadratic'),\n",
    "                                                 accuracy_score(cnn_scoring.actual, cnn_scoring.CNN2.apply(round))))\n",
    "\n",
    "print(\"CNN3 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(cnn_scoring.actual, cnn_scoring.CNN3.apply(round))),\n",
    "                                       cohen_kappa_score(cnn_scoring.actual, cnn_scoring.CNN3.apply(round)),\n",
    "                                        cohen_kappa_score(cnn_scoring.actual, cnn_scoring.CNN3.apply(round),weights='quadratic'),             \n",
    "                                      accuracy_score(cnn_scoring.actual, cnn_scoring.CNN3.apply(round))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 37s 4ms/step - loss: 0.1413 - acc: 0.0817 - val_loss: 0.0795 - val_acc: 0.0938\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 31s 4ms/step - loss: 0.0689 - acc: 0.0943 - val_loss: 0.0664 - val_acc: 0.1118\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0581 - acc: 0.1062 - val_loss: 0.0614 - val_acc: 0.1079\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0541 - acc: 0.1099 - val_loss: 0.0594 - val_acc: 0.1118\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0519 - acc: 0.1127 - val_loss: 0.0582 - val_acc: 0.1137\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0504 - acc: 0.1149 - val_loss: 0.0574 - val_acc: 0.1130\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0492 - acc: 0.1171 - val_loss: 0.0565 - val_acc: 0.1130\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0482 - acc: 0.1178 - val_loss: 0.0558 - val_acc: 0.1137\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0473 - acc: 0.1188 - val_loss: 0.0553 - val_acc: 0.1130\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0464 - acc: 0.1187 - val_loss: 0.0546 - val_acc: 0.1162\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0456 - acc: 0.1205 - val_loss: 0.0540 - val_acc: 0.1150\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 28s 3ms/step - loss: 0.0445 - acc: 0.1220 - val_loss: 0.0528 - val_acc: 0.1188\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0434 - acc: 0.1232 - val_loss: 0.0514 - val_acc: 0.1227\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0420 - acc: 0.1252 - val_loss: 0.0504 - val_acc: 0.1259\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0403 - acc: 0.1260 - val_loss: 0.0485 - val_acc: 0.1259\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0371 - acc: 0.1303 - val_loss: 0.0423 - val_acc: 0.1368\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0323 - acc: 0.1345 - val_loss: 0.0376 - val_acc: 0.1381\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0288 - acc: 0.1374 - val_loss: 0.0347 - val_acc: 0.1439\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0271 - acc: 0.1375 - val_loss: 0.0322 - val_acc: 0.1439\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0242 - acc: 0.1385 - val_loss: 0.0304 - val_acc: 0.1458\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0226 - acc: 0.1386 - val_loss: 0.0293 - val_acc: 0.1464\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0214 - acc: 0.1393 - val_loss: 0.0279 - val_acc: 0.1464\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0201 - acc: 0.1393 - val_loss: 0.0271 - val_acc: 0.1464\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0193 - acc: 0.1396 - val_loss: 0.0267 - val_acc: 0.1458\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0187 - acc: 0.1395 - val_loss: 0.0261 - val_acc: 0.1464\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0182 - acc: 0.1397 - val_loss: 0.0257 - val_acc: 0.1464\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0173 - acc: 0.1401 - val_loss: 0.0251 - val_acc: 0.1464\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0168 - acc: 0.1401 - val_loss: 0.0256 - val_acc: 0.1464\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0165 - acc: 0.1401 - val_loss: 0.0246 - val_acc: 0.1471\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 30s 3ms/step - loss: 0.0158 - acc: 0.1401 - val_loss: 0.0245 - val_acc: 0.1471\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0156 - acc: 0.1401 - val_loss: 0.0245 - val_acc: 0.1471\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 30s 3ms/step - loss: 0.0152 - acc: 0.1400 - val_loss: 0.0242 - val_acc: 0.1471\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0149 - acc: 0.1402 - val_loss: 0.0241 - val_acc: 0.1471\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0146 - acc: 0.1401 - val_loss: 0.0241 - val_acc: 0.1464\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 29s 3ms/step - loss: 0.0144 - acc: 0.1402 - val_loss: 0.0239 - val_acc: 0.1471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce8f537860>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_1 = Sequential()\n",
    "rnn_1.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "rnn_1.add(LSTM(20))\n",
    "#Regression\n",
    "rnn_1.add(Dense(1,name='out_layer'))\n",
    "rnn_1.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "rnn_1.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#Classification\n",
    "# rnn_1.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),name='out_layer'))\n",
    "# rnn_1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# rnn_1.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 62s 7ms/step - loss: 0.2962 - acc: 0.0553 - val_loss: 0.0928 - val_acc: 0.1130\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0649 - acc: 0.1114 - val_loss: 0.0628 - val_acc: 0.1130\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 51s 6ms/step - loss: 0.0564 - acc: 0.1116 - val_loss: 0.0591 - val_acc: 0.1130\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 51s 6ms/step - loss: 0.0547 - acc: 0.1115 - val_loss: 0.0577 - val_acc: 0.1130\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 51s 6ms/step - loss: 0.0540 - acc: 0.1116 - val_loss: 0.0572 - val_acc: 0.1130\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 51s 6ms/step - loss: 0.0533 - acc: 0.1115 - val_loss: 0.0568 - val_acc: 0.1130\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 51s 6ms/step - loss: 0.0527 - acc: 0.1116 - val_loss: 0.0563 - val_acc: 0.1130\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 51s 6ms/step - loss: 0.0519 - acc: 0.1116 - val_loss: 0.0559 - val_acc: 0.1130\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 51s 6ms/step - loss: 0.0511 - acc: 0.1119 - val_loss: 0.0550 - val_acc: 0.1130\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 54s 6ms/step - loss: 0.0501 - acc: 0.1119 - val_loss: 0.0542 - val_acc: 0.1143\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 55s 6ms/step - loss: 0.0488 - acc: 0.1149 - val_loss: 0.0529 - val_acc: 0.1150\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 54s 6ms/step - loss: 0.0472 - acc: 0.1187 - val_loss: 0.0510 - val_acc: 0.1246\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0448 - acc: 0.1234 - val_loss: 0.0488 - val_acc: 0.1285\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0420 - acc: 0.1282 - val_loss: 0.0467 - val_acc: 0.1310\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0400 - acc: 0.1291 - val_loss: 0.0433 - val_acc: 0.1342\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0375 - acc: 0.1327 - val_loss: 0.0417 - val_acc: 0.1362\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0341 - acc: 0.1356 - val_loss: 0.0373 - val_acc: 0.1407\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0298 - acc: 0.1373 - val_loss: 0.0308 - val_acc: 0.1439\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0266 - acc: 0.1369 - val_loss: 0.0279 - val_acc: 0.1464\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0238 - acc: 0.1374 - val_loss: 0.0257 - val_acc: 0.1477\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0223 - acc: 0.1383 - val_loss: 0.0249 - val_acc: 0.1477\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 51s 6ms/step - loss: 0.0213 - acc: 0.1383 - val_loss: 0.0251 - val_acc: 0.1477\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0217 - acc: 0.1388 - val_loss: 0.0234 - val_acc: 0.1477\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0204 - acc: 0.1387 - val_loss: 0.0229 - val_acc: 0.1471\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 51s 6ms/step - loss: 0.0202 - acc: 0.1386 - val_loss: 0.0237 - val_acc: 0.1471\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 51s 6ms/step - loss: 0.0198 - acc: 0.1392 - val_loss: 0.0223 - val_acc: 0.1464\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0189 - acc: 0.1391 - val_loss: 0.0224 - val_acc: 0.1464\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0187 - acc: 0.1392 - val_loss: 0.0226 - val_acc: 0.1477\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0189 - acc: 0.1393 - val_loss: 0.0223 - val_acc: 0.1471\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0184 - acc: 0.1394 - val_loss: 0.0221 - val_acc: 0.1471\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0180 - acc: 0.1395 - val_loss: 0.0217 - val_acc: 0.1471\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 51s 6ms/step - loss: 0.0176 - acc: 0.1396 - val_loss: 0.0215 - val_acc: 0.1471\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 51s 6ms/step - loss: 0.0173 - acc: 0.1395 - val_loss: 0.0214 - val_acc: 0.1471\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0172 - acc: 0.1393 - val_loss: 0.0213 - val_acc: 0.1477\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 50s 6ms/step - loss: 0.0171 - acc: 0.1396 - val_loss: 0.0214 - val_acc: 0.1471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce8e738438>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_2 = Sequential()\n",
    "rnn_2.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "rnn_2.add(LSTM(20, return_sequences=True))\n",
    "rnn_2.add(LSTM(20))\n",
    "#regression\n",
    "rnn_2.add(Dense(1,name='out_layer'))\n",
    "rnn_2.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "rnn_2.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#Classification\n",
    "# rnn_2.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),name='out_layer'))\n",
    "# rnn_2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# rnn_2.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 82s 9ms/step - loss: 0.2394 - acc: 0.0602 - val_loss: 0.0891 - val_acc: 0.1130\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 70s 8ms/step - loss: 0.0626 - acc: 0.1124 - val_loss: 0.0599 - val_acc: 0.1143\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0550 - acc: 0.1120 - val_loss: 0.0575 - val_acc: 0.1130\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0539 - acc: 0.1122 - val_loss: 0.0571 - val_acc: 0.1130\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0532 - acc: 0.1122 - val_loss: 0.0562 - val_acc: 0.1137\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0520 - acc: 0.1122 - val_loss: 0.0548 - val_acc: 0.1137\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0491 - acc: 0.1142 - val_loss: 0.0507 - val_acc: 0.1246\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0442 - acc: 0.1232 - val_loss: 0.0450 - val_acc: 0.1329\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 70s 8ms/step - loss: 0.0402 - acc: 0.1282 - val_loss: 0.0410 - val_acc: 0.1362\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0364 - acc: 0.1331 - val_loss: 0.0360 - val_acc: 0.1419\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0314 - acc: 0.1360 - val_loss: 0.0313 - val_acc: 0.1464\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0277 - acc: 0.1376 - val_loss: 0.0278 - val_acc: 0.1477\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0250 - acc: 0.1382 - val_loss: 0.0253 - val_acc: 0.1477\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0229 - acc: 0.1386 - val_loss: 0.0238 - val_acc: 0.1477\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0216 - acc: 0.1390 - val_loss: 0.0231 - val_acc: 0.1484\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0206 - acc: 0.1390 - val_loss: 0.0229 - val_acc: 0.1484\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0201 - acc: 0.1391 - val_loss: 0.0216 - val_acc: 0.1484\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 70s 8ms/step - loss: 0.0192 - acc: 0.1393 - val_loss: 0.0217 - val_acc: 0.1484\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0192 - acc: 0.1394 - val_loss: 0.0215 - val_acc: 0.1490\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0183 - acc: 0.1396 - val_loss: 0.0207 - val_acc: 0.1484\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0179 - acc: 0.1393 - val_loss: 0.0206 - val_acc: 0.1484\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0175 - acc: 0.1396 - val_loss: 0.0205 - val_acc: 0.1484\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0175 - acc: 0.1394 - val_loss: 0.0244 - val_acc: 0.1490\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0175 - acc: 0.1396 - val_loss: 0.0204 - val_acc: 0.1490\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0169 - acc: 0.1394 - val_loss: 0.0207 - val_acc: 0.1490\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0165 - acc: 0.1397 - val_loss: 0.0201 - val_acc: 0.1496\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 70s 8ms/step - loss: 0.0162 - acc: 0.1397 - val_loss: 0.0201 - val_acc: 0.1496\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0162 - acc: 0.1395 - val_loss: 0.0204 - val_acc: 0.1496\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 69s 8ms/step - loss: 0.0160 - acc: 0.1396 - val_loss: 0.0201 - val_acc: 0.1496\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0158 - acc: 0.1397 - val_loss: 0.0202 - val_acc: 0.1496\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0160 - acc: 0.1396 - val_loss: 0.0202 - val_acc: 0.1490\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0156 - acc: 0.1397 - val_loss: 0.0208 - val_acc: 0.1496\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0158 - acc: 0.1397 - val_loss: 0.0202 - val_acc: 0.1496\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0149 - acc: 0.1397 - val_loss: 0.0209 - val_acc: 0.1496\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 68s 8ms/step - loss: 0.0149 - acc: 0.1397 - val_loss: 0.0208 - val_acc: 0.1496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce8b6b0e80>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_3 = Sequential()\n",
    "rnn_3.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],input_length=max_len_class,trainable=False))\n",
    "rnn_3.add(LSTM(20, return_sequences=True))\n",
    "rnn_3.add(LSTM(20, return_sequences=True))\n",
    "rnn_3.add(LSTM(20))\n",
    "#Regression\n",
    "rnn_3.add(Dense(1,name='out_layer'))\n",
    "rnn_3.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "rnn_3.fit(sequences_matrix_class,train_sa_y_class.adjusted_domain1_score, batch_size = 500, epochs=35, validation_split=0.15)\n",
    "#Classification\n",
    "# rnn_3.add(Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),name='out_layer'))\n",
    "# rnn_3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# rnn_3.fit(sequences_matrix_class,np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 500, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN1    0.651340\n",
       "RNN2    0.674097\n",
       "RNN3    0.686735\n",
       "dtype: float64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_pred1 = rnn_1.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))\n",
    "rnn_pred2 = rnn_2.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))\n",
    "rnn_pred3 = rnn_3.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))\n",
    "\n",
    "#Uncomment for classification case with cat_ce loss\n",
    "# rnn_pred1 = np.argmax(rnn_pred1,axis=1)\n",
    "# rnn_pred2 = np.argmax(rnn_pred2,axis=1)\n",
    "# rnn_pred3 = np.argmax(rnn_pred3,axis=1)\n",
    "\n",
    "RNN_pred_frame = pd.DataFrame({'actual':np.asarray(test_sa_y_class.adjusted_domain1_score),\n",
    "                           'dataset': np.asarray(test_sa_y_class.essay_set),\n",
    "                           'RNN1':rnn_pred1.flatten(),\n",
    "                          'RNN2':rnn_pred2.flatten(),\n",
    "                          'RNN3':rnn_pred3.flatten()})\n",
    "\n",
    "rnn_scoring = RNN_pred_frame.merge(DivSeries, on='dataset')\n",
    "\n",
    "for colName in ['actual', 'RNN1', 'RNN2', 'RNN3']:\n",
    "    #Uncomment for classification case with cat_ce loss\n",
    "#     if(colName != 'actual'):\n",
    "#         rnn_scoring[colName] = rnn_scoring[colName].apply(RetrieveNormalize)\n",
    "    rnn_scoring[colName] = rnn_scoring[colName] * rnn_scoring['div']\n",
    "    rnn_scoring[colName] = rnn_scoring[colName].apply(round)\n",
    "\n",
    "rnn_scoring.actual = rnn_scoring.actual.astype(int)\n",
    "\n",
    "QuadKappaCalculation = pd.DataFrame(columns = ['RNN1', 'RNN2', 'RNN3'],index = np.unique(rnn_scoring.dataset))\n",
    "for essaySetValue in np.unique(rnn_scoring.dataset):\n",
    "    temp_RNN_ES = rnn_scoring[rnn_scoring.dataset == essaySetValue]\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'RNN1'] = cohen_kappa_score(temp_RNN_ES.actual, temp_RNN_ES.RNN1.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'RNN2'] = cohen_kappa_score(temp_RNN_ES.actual, temp_RNN_ES.RNN2.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'RNN3'] = cohen_kappa_score(temp_RNN_ES.actual, temp_RNN_ES.RNN3.apply(round),weights='quadratic')\n",
    "QuadKappaCalculation.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN RMSE, Cohen, Quad Cohen, accuracy: 1.8575005158572262, 0.4045031908930591, 0.9778191799343129, 0.47804314329738057\n",
      "RNN2 RMSE, Cohen, Quad Cohen, accuracy: 1.773803553013729, 0.4326413283024191, 0.9803193058298769, 0.5034668721109399\n",
      "RNN3 RMSE, Cohen, Quad Cohen, accuracy: 1.8914086841499005, 0.44518106208561037, 0.9778448202840749, 0.5119414483821263\n"
     ]
    }
   ],
   "source": [
    "print(\"RNN RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(rnn_scoring.actual, rnn_scoring.RNN1.apply(round))),\n",
    "                                       cohen_kappa_score(rnn_scoring.actual, rnn_scoring.RNN1.apply(round)),\n",
    "                                        cohen_kappa_score(rnn_scoring.actual, rnn_scoring.RNN1.apply(round),weights='quadratic'),\n",
    "                                                 accuracy_score(rnn_scoring.actual, rnn_scoring.RNN1.apply(round))))\n",
    "\n",
    "print(\"RNN2 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(rnn_scoring.actual, rnn_scoring.RNN2.apply(round))),\n",
    "                                       cohen_kappa_score(rnn_scoring.actual, rnn_scoring.RNN2.apply(round)),\n",
    "                                        cohen_kappa_score(rnn_scoring.actual, rnn_scoring.RNN2.apply(round),weights='quadratic'),\n",
    "                                                 accuracy_score(rnn_scoring.actual, rnn_scoring.RNN2.apply(round))))\n",
    "\n",
    "print(\"RNN3 RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(rnn_scoring.actual, rnn_scoring.RNN3.apply(round))),\n",
    "                                       cohen_kappa_score(rnn_scoring.actual, rnn_scoring.RNN3.apply(round)),\n",
    "                                        cohen_kappa_score(rnn_scoring.actual, rnn_scoring.RNN3.apply(round),weights='quadratic'),             \n",
    "                                      accuracy_score(rnn_scoring.actual, rnn_scoring.RNN3.apply(round))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation split by dataset - top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  UnweightedKappa QuadKappa  Accuracy  UnweightedRMSE\n",
      "1        0.438732  0.438732  0.942623        0.085908\n",
      "2        0.559002  0.559002  0.930481        0.120366\n",
      "3        0.589805  0.589805  0.805389        0.198272\n",
      "4        0.781063  0.781063  0.891667        0.185394\n",
      "5        0.523465  0.523465  0.756522        0.144108\n",
      "6        0.614299  0.614299  0.838624        0.139351\n",
      "7        0.548825  0.548825  0.854305        0.120696\n",
      "8        0.270501  0.270501  0.963504        0.125150\n"
     ]
    }
   ],
   "source": [
    "preds_to_analyze = rnn_3.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))\n",
    "\n",
    "byset_analysis = pd.DataFrame({'actual':np.asarray(test_sa_y_class.adjusted_domain1_score),\n",
    "                           'dataset': np.asarray(test_sa_y_class.essay_set),\n",
    "                          'RNN3':preds_to_analyze.flatten()})\n",
    "\n",
    "preds_to_analyze = byset_analysis.merge(DivSeries, on='dataset')\n",
    "\n",
    "for colName in ['actual', 'RNN3']:\n",
    "    newColName = colName + '_adjusted'\n",
    "    preds_to_analyze[newColName] = preds_to_analyze[colName] * preds_to_analyze['div']\n",
    "    preds_to_analyze[newColName] = preds_to_analyze[colName].apply(round)\n",
    "\n",
    "preds_to_analyze['right'] = (preds_to_analyze.RNN3_adjusted == preds_to_analyze.actual_adjusted)\n",
    "preds_to_analyze['delta'] = (preds_to_analyze.RNN3_adjusted - preds_to_analyze.actual_adjusted)**2\n",
    "\n",
    "aggregates = pd.concat([preds_to_analyze.groupby('dataset')['right'].sum(),\n",
    "           preds_to_analyze.groupby('dataset')['actual'].count(),\n",
    "                        preds_to_analyze.groupby('dataset')['delta'].mean()],axis=1)\n",
    "aggregates.columns = ['correct', 'test_samples','RMSE']\n",
    "\n",
    "aggregates = pd.DataFrame(index=range(1,9),columns = ['UnweightedKappa', 'QuadKappa', 'Accuracy'])\n",
    "\n",
    "for i in aggregates.index:\n",
    "    temp = preds_to_analyze[preds_to_analyze.dataset == i]\n",
    "    aggregates.loc[i,'UnweightedKappa'] = cohen_kappa_score(temp.actual_adjusted, temp.RNN3_adjusted.apply(round))\n",
    "    aggregates.loc[i,'QuadKappa'] = cohen_kappa_score(temp.actual_adjusted, temp.RNN3_adjusted.apply(round),weights='quadratic')\n",
    "    aggregates.loc[i,'Accuracy'] = accuracy_score(temp.actual_adjusted, temp.RNN3_adjusted.apply(round))\n",
    "    aggregates.loc[i,'UnweightedRMSE'] = mean_squared_error(temp.actual, temp.RNN3)\n",
    "    aggregates.loc[i,'RMSE'] = mean_squared_error(temp.actual_adjusted, temp.RNN3_adjusted)\n",
    "aggregates.UnweightedRMSE = aggregates.UnweightedRMSE.apply(sqrt)\n",
    "\n",
    "print(aggregates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>dataset</th>\n",
       "      <th>RNN3</th>\n",
       "      <th>div</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.593101</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>0.722121</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.921658</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>0.853335</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>0.725540</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.242303</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.198401</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.096411</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.211319</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>0.753628</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.270942</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.857472</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.173292</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.059308</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.138383</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>0.588066</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>0.670716</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.354181</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.405553</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>0.202880</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.246701</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>0.527093</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.428495</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.314995</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>0.480423</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>0.351572</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.193518</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.129854</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.109845</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.115069</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.740101</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2567</th>\n",
       "      <td>0.920000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.877711</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>0.560000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.441133</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2569</th>\n",
       "      <td>0.840000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.759920</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2570</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.620959</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2571</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.548781</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2572</th>\n",
       "      <td>0.320000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.278966</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2573</th>\n",
       "      <td>0.880000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.852314</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2574</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.829730</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.615167</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>0.560000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.541100</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.685103</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2578</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.783571</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2579</th>\n",
       "      <td>0.360000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.443459</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2580</th>\n",
       "      <td>0.560000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.549514</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581</th>\n",
       "      <td>0.840000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.769810</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.560294</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>0.320000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.453746</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2584</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.692956</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2585</th>\n",
       "      <td>0.520000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.729590</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.898065</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587</th>\n",
       "      <td>0.840000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.755342</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>0.920000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.890934</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2589</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.775421</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.606336</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591</th>\n",
       "      <td>0.840000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.953229</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>0.520000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.603978</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.643152</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2594</th>\n",
       "      <td>0.920000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.852497</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>7</td>\n",
       "      <td>0.727144</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2596 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        actual  dataset      RNN3  div\n",
       "0     0.333333        4  0.593101    3\n",
       "1     0.666667        4  0.722121    3\n",
       "2     1.000000        4  0.921658    3\n",
       "3     0.666667        4  0.853335    3\n",
       "4     0.666667        4  0.725540    3\n",
       "5     0.333333        4  0.242303    3\n",
       "6     0.333333        4  0.198401    3\n",
       "7     0.000000        4  0.096411    3\n",
       "8     0.000000        4  0.211319    3\n",
       "9     0.666667        4  0.753628    3\n",
       "10    0.333333        4  0.270942    3\n",
       "11    1.000000        4  0.857472    3\n",
       "12    0.333333        4  0.173292    3\n",
       "13    0.000000        4  0.059308    3\n",
       "14    0.333333        4  0.138383    3\n",
       "15    0.666667        4  0.588066    3\n",
       "16    0.666667        4  0.670716    3\n",
       "17    0.333333        4  0.354181    3\n",
       "18    0.333333        4  0.405553    3\n",
       "19    0.666667        4  0.202880    3\n",
       "20    0.333333        4  0.246701    3\n",
       "21    0.666667        4  0.527093    3\n",
       "22    0.333333        4  0.428495    3\n",
       "23    0.333333        4  0.314995    3\n",
       "24    0.666667        4  0.480423    3\n",
       "25    0.333333        4  0.351572    3\n",
       "26    0.000000        4  0.193518    3\n",
       "27    0.000000        4  0.129854    3\n",
       "28    0.000000        4  0.109845    3\n",
       "29    0.000000        4  0.115069    3\n",
       "...        ...      ...       ...  ...\n",
       "2566  0.800000        7  0.740101   25\n",
       "2567  0.920000        7  0.877711   25\n",
       "2568  0.560000        7  0.441133   25\n",
       "2569  0.840000        7  0.759920   25\n",
       "2570  0.800000        7  0.620959   25\n",
       "2571  0.600000        7  0.548781   25\n",
       "2572  0.320000        7  0.278966   25\n",
       "2573  0.880000        7  0.852314   25\n",
       "2574  0.640000        7  0.829730   25\n",
       "2575  0.640000        7  0.615167   25\n",
       "2576  0.560000        7  0.541100   25\n",
       "2577  0.720000        7  0.685103   25\n",
       "2578  0.600000        7  0.783571   25\n",
       "2579  0.360000        7  0.443459   25\n",
       "2580  0.560000        7  0.549514   25\n",
       "2581  0.840000        7  0.769810   25\n",
       "2582  0.640000        7  0.560294   25\n",
       "2583  0.320000        7  0.453746   25\n",
       "2584  0.640000        7  0.692956   25\n",
       "2585  0.520000        7  0.729590   25\n",
       "2586  0.720000        7  0.898065   25\n",
       "2587  0.840000        7  0.755342   25\n",
       "2588  0.920000        7  0.890934   25\n",
       "2589  0.640000        7  0.775421   25\n",
       "2590  0.640000        7  0.606336   25\n",
       "2591  0.840000        7  0.953229   25\n",
       "2592  0.520000        7  0.603978   25\n",
       "2593  0.800000        7  0.643152   25\n",
       "2594  0.920000        7  0.852497   25\n",
       "2595  0.800000        7  0.727144   25\n",
       "\n",
       "[2596 rows x 4 columns]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_to_analyze = rnn_3.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))\n",
    "\n",
    "byset_analysis = pd.DataFrame({'actual':np.asarray(test_sa_y_class.adjusted_domain1_score),\n",
    "                           'dataset': np.asarray(test_sa_y_class.essay_set),\n",
    "                          'RNN3':preds_to_analyze.flatten()})\n",
    "\n",
    "preds_to_analyze = byset_analysis.merge(DivSeries, on='dataset')\n",
    "preds_to_analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  UnweightedKappa QuadKappa  Accuracy  UnweightedRMSE      RMSE\n",
      "1        0.228632  0.719381  0.393443        0.085908  1.068680\n",
      "2         0.40637  0.648741  0.628342        0.120366  0.652023\n",
      "3        0.467729  0.648761  0.646707        0.198272  0.670151\n",
      "4        0.491536   0.76076  0.638889        0.185394  0.636832\n",
      "5        0.447882  0.762493  0.608696        0.144108  0.639293\n",
      "6        0.489322  0.787484  0.642857        0.139351  0.642416\n",
      "7       0.0743891  0.766861   0.13245        0.120696  3.020899\n",
      "8       0.0226475  0.399399  0.080292        0.125150  6.255509\n"
     ]
    }
   ],
   "source": [
    "for colName in ['actual', 'RNN3']:\n",
    "    newColName = colName + '_adjusted'\n",
    "    preds_to_analyze[newColName] = preds_to_analyze[colName] * preds_to_analyze['div']\n",
    "    preds_to_analyze[newColName] = preds_to_analyze[newColName].apply(round)\n",
    "\n",
    "preds_to_analyze['right'] = (preds_to_analyze.RNN3_adjusted == preds_to_analyze.actual_adjusted)\n",
    "preds_to_analyze['delta'] = (preds_to_analyze.RNN3_adjusted - preds_to_analyze.actual_adjusted)**2\n",
    "\n",
    "aggregates = pd.concat([preds_to_analyze.groupby('dataset')['right'].sum(),\n",
    "           preds_to_analyze.groupby('dataset')['actual'].count(),\n",
    "                        preds_to_analyze.groupby('dataset')['delta'].mean()],axis=1)\n",
    "aggregates.columns = ['correct', 'test_samples','RMSE']\n",
    "\n",
    "aggregates = pd.DataFrame(index=range(1,9),columns = ['UnweightedKappa', 'QuadKappa', 'Accuracy'])\n",
    "\n",
    "for i in aggregates.index:\n",
    "    temp = preds_to_analyze[preds_to_analyze.dataset == i]\n",
    "    aggregates.loc[i,'UnweightedKappa'] = cohen_kappa_score(temp.actual_adjusted, temp.RNN3_adjusted.apply(round))\n",
    "    aggregates.loc[i,'QuadKappa'] = cohen_kappa_score(temp.actual_adjusted, temp.RNN3_adjusted.apply(round),weights='quadratic')\n",
    "    aggregates.loc[i,'Accuracy'] = accuracy_score(temp.actual_adjusted, temp.RNN3_adjusted.apply(round))\n",
    "    aggregates.loc[i,'UnweightedRMSE'] = mean_squared_error(temp.actual, temp.RNN3)\n",
    "    aggregates.loc[i,'RMSE'] = mean_squared_error(temp.actual_adjusted, temp.RNN3_adjusted)\n",
    "aggregates.UnweightedRMSE = aggregates.UnweightedRMSE.apply(sqrt)\n",
    "aggregates.RMSE = aggregates.RMSE.apply(sqrt)\n",
    "\n",
    "print(aggregates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nishray/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "\n",
    "import spacy\n",
    "from spacy.attrs import ORTH\n",
    "#import textacy\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_lg',disable=['ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_obj = pd.Series(train_sa_x_class).apply(lambda essay: nlp(essay.lower()))\n",
    "test_x_obj = pd.Series(test_sa_x_class).apply(lambda essay: nlp(essay.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_height(root):\n",
    "\n",
    "    if not list(root.children):\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 + max(tree_height(x) for x in root.children)\n",
    "    \n",
    "def get_average_heights(paragraph):\n",
    "\n",
    "    if type(paragraph) == str:\n",
    "        doc = nlp(paragraph)\n",
    "    else:\n",
    "        doc = paragraph\n",
    "    roots = [sent.root for sent in doc.sents]\n",
    "    return np.mean([tree_height(root) for root in roots])\n",
    "\n",
    "def get_variance_heights(paragraph):\n",
    "\n",
    "    if type(paragraph) == str:\n",
    "        doc = nlp(paragraph)\n",
    "    else:\n",
    "        doc = paragraph\n",
    "    roots = [sent.root for sent in doc.sents]\n",
    "    return np.std([tree_height(root) for root in roots])\n",
    "\n",
    "def get_tree_heights(paragraph):\n",
    "    if type(paragraph) == str:\n",
    "        doc = nlp(paragraph)\n",
    "    else:\n",
    "        doc = paragraph\n",
    "    roots = [sent.root for sent in doc.sents]\n",
    "    return [tree_height(root) for root in roots]\n",
    "\n",
    "def get_sentences(doc):\n",
    "    sents = list(doc.sents)\n",
    "    return sents\n",
    "\n",
    "def get_sentence_count(sentences):\n",
    "    return float(len(sentences))\n",
    "\n",
    "def get_word_counts(doc):\n",
    "    return doc.count_by(ORTH)\n",
    "\n",
    "def get_connectives(doc):\n",
    "    text = doc.text.lower()\n",
    "    connectives = [\n",
    "    'after',\n",
    "    'earlier',\n",
    "    'before',\n",
    "    'during',\n",
    "    'while',\n",
    "    'later',\n",
    "    'because',\n",
    "    'consequently',\n",
    "    'thus',\n",
    "    'both',\n",
    "    'additionally',\n",
    "    'furthermore',\n",
    "    'moreover',\n",
    "    'actually',\n",
    "    'as a result',\n",
    "    'due to',\n",
    "    'but',\n",
    "    'yet',\n",
    "    'however',\n",
    "    'although',\n",
    "    'nevertheless'\n",
    "    ]\n",
    "    total = 0\n",
    "    for connector in connectives:\n",
    "        total += text.count(connector)\n",
    "    return float((total/len(doc)))\n",
    "\n",
    "def get_pos(doc):\n",
    "    return [token.pos_ for token in doc]\n",
    "\n",
    "\n",
    "def get_posngrams(poslist,n):\n",
    "    posngrams = []\n",
    "    for item in range(len(poslist) - n + 1):\n",
    "        posngrams.append(tuple([poslist[item+i] for i in range(n)]))\n",
    "    return posngrams\n",
    "\n",
    "def get_posgrams_counts(list_grams):\n",
    "    posgrams_counts = defaultdict(int)\n",
    "    for gram in list_grams:\n",
    "        posgrams_counts[gram] += 1\n",
    "    return posgrams_counts\n",
    "\n",
    "def get_TF(list_dicts):\n",
    "    TF_dict = defaultdict(int)\n",
    "    for dictionary in list_dicts:\n",
    "        for gram in dictionary:\n",
    "            TF_dict[gram] += dictionary[gram]\n",
    "    return TF_dict\n",
    "\n",
    "def get_mean_tfTF(posgram_counts,TF):\n",
    "    tfTF_ratios = list()\n",
    "    for key, value in posgram_counts.items():\n",
    "        tfTF_ratios.append(value/TF[key])\n",
    "    return np.mean(tfTF_ratios)\n",
    "\n",
    "def get_posngram_ratio(posngrams):\n",
    "    if len(posngrams) > 0:\n",
    "        return float(len(set(posngrams))/len(posngrams))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_reading_scores(doc):\n",
    "    scores = textacy.TextStats(doc).readability_stats\n",
    "    del scores['smog_index']\n",
    "    return scores\n",
    "\n",
    "def get_word_lengths(doc):\n",
    "    lengths = list()\n",
    "    for word in doc:\n",
    "        if word.is_alpha:\n",
    "            lengths.append(float(len(word)))\n",
    "    return lengths\n",
    "\n",
    "def get_words_of_length(lengths, n, p):\n",
    "    count = 0\n",
    "    for length in lengths:\n",
    "        if length > n and length < p:\n",
    "            count += 1\n",
    "    return float(count)\n",
    "\n",
    "def get_similarity_scores(doc):\n",
    "    sents = [sent for sent in doc.sents]\n",
    "    similarity_scores = list()\n",
    "    for i in range(1,len(sents)):\n",
    "        sent1 = sents[i-1]\n",
    "        sent2 = sents[i]\n",
    "        similarity_scores.append(sent1.similarity(sent2))\n",
    "    return np.mean(similarity_scores)\n",
    "\n",
    "def nth_root(x,n):\n",
    "    return x ** (1/float(n))\n",
    "\n",
    "def get_yules_k(word_counts):\n",
    "    m1 =  sum(word_counts.values())\n",
    "    m2 = sum([freq ** 2 for freq in word_counts.values()])\n",
    "    if m1 == m2:\n",
    "        k = 0 \n",
    "    else:\n",
    "        i = (m1*m1) / (m2-m1)\n",
    "        k = 1/i * 10000\n",
    "    if np.isnan(k):\n",
    "        k=10000\n",
    "    try:\n",
    "        return float(k)\n",
    "    except ZeroDivisionError:\n",
    "        return 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "\n",
    "test_x_obj['essay_set'] = np.asarray(test_sa_y_class.essay_set)\n",
    "train_x_obj['essay_set'] = np.asarray(train_sa_y_class.essay_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_obj = pd.DataFrame(test_x_obj)\n",
    "test_x_obj['essay'] = test_sa_x_class\n",
    "test_x_obj['essay_set'] = np.asarray(test_sa_y_class.essay_set)\n",
    "test_x_obj.columns = ['doc', 'essay']\n",
    "\n",
    "train_x_obj = pd.DataFrame(train_x_obj)\n",
    "train_x_obj['essay'] = train_sa_x_class\n",
    "train_x_obj['essay_set'] = np.asarray(train_sa_y_class.essay_set)\n",
    "train_x_obj.columns = ['doc', 'essay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preengineering\n",
    "train_x_obj['sentences'] = train_x_obj.doc.apply(get_sentences)\n",
    "train_x_obj['word_counts'] = train_x_obj.doc.apply(get_word_counts)\n",
    "train_x_obj['word_lengths'] = train_x_obj.doc.apply(get_word_lengths)\n",
    "train_x_obj['pos'] = train_x_obj.doc.apply(get_pos)\n",
    "\n",
    "train_x_obj['pos_trigrams'] = train_x_obj.pos.apply(lambda pos: get_posngrams(pos, n=3))\n",
    "train_x_obj['pos_fourgrams'] = train_x_obj.pos.apply(lambda pos: get_posngrams(pos, n=4))\n",
    "train_x_obj['pos_trigram_counts'] = train_x_obj.pos_trigrams.apply(get_posgrams_counts)\n",
    "train_x_obj['pos_fourgram_counts'] = train_x_obj.pos_fourgrams.apply(get_posgrams_counts)\n",
    "tri_pos_TF = get_TF(train_x_obj.pos_trigram_counts)\n",
    "four_pos_TF = get_TF(train_x_obj.pos_fourgram_counts)\n",
    "\n",
    "train_x_obj['tree_heights'] = train_x_obj.doc.apply(lambda doc: get_tree_heights(doc))\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "train_x_obj['reading_scores'] = train_x_obj.doc.apply(get_reading_scores)\n",
    "\n",
    "# Lexical Features\n",
    "train_x_obj['words_length_4'] = train_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 4,6))\n",
    "train_x_obj['words_length_6'] = train_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 6,8))\n",
    "train_x_obj['words_length_8'] = train_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 8,10))\n",
    "train_x_obj['words_length_10'] = train_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 10,12))\n",
    "train_x_obj['words_length_12'] = train_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 12,100))\n",
    "train_x_obj['mean_word_length'] = train_x_obj.word_lengths.apply(np.mean)\n",
    "train_x_obj['variance_word_length'] = train_x_obj.word_lengths.apply(np.std)\n",
    "\n",
    "\n",
    "# Length Features\n",
    "train_x_obj['essay_length'] = train_x_obj.doc.apply(len)\n",
    "train_x_obj['num_words'] = train_x_obj.doc.apply(lambda doc: float(len([word for word in doc if word.is_alpha])))\n",
    "train_x_obj['num_sentences'] = train_x_obj.sentences.apply(get_sentence_count)\n",
    "train_x_obj['mean_sentence_length'] = train_x_obj.num_words/train_x_obj.num_sentences\n",
    "train_x_obj['num_characters'] = train_x_obj.essay.apply(len)\n",
    "train_x_obj['fourth_root_num_characters'] = train_x_obj.num_characters.apply(nth_root, n=4)\n",
    "\n",
    "# # Occurrence Features\n",
    "train_x_obj['num_commas'] = train_x_obj.essay.apply(lambda essay: float(essay.count(',')))\n",
    "train_x_obj['num_periods'] = train_x_obj.essay.apply(lambda essay: float(essay.count('.')))\n",
    "train_x_obj['num_exclaim'] = train_x_obj.essay.apply(lambda essay: float(essay.count('!')))\n",
    "train_x_obj['num_question'] = train_x_obj.essay.apply(lambda essay: float(essay.count('?')))\n",
    "train_x_obj['num_semicolon'] = train_x_obj.essay.apply(lambda essay: float(essay.count(';')))\n",
    "train_x_obj['num_colon'] = train_x_obj.essay.apply(lambda essay: float(essay.count(':')))\n",
    "\n",
    "# # Style Features\n",
    "# FIX train_x_obj['vocabulary'] = train_x_obj.word_tokens.apply(lambda word_tokens: set(word.lower() for word in word_tokens if word.isalpha()))\n",
    "train_x_obj['vocab_size'] = train_x_obj.word_counts.apply(len)\n",
    "train_x_obj['type_token_ratio'] = train_x_obj.word_counts.apply(len) / train_x_obj.essay_length\n",
    "train_x_obj['yules_k'] = train_x_obj.word_counts.apply(get_yules_k)\n",
    "\n",
    "# # Syntactical Features\n",
    "# # the number for these lengths comes from Chen and He 2013\n",
    "train_x_obj['sentence_lengths'] = train_x_obj.sentences.apply(lambda sentences: [len(sent) for sent in sentences])\n",
    "train_x_obj['very_short_sentences'] = train_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length <= 10 for length in sentence_lengths])))\n",
    "train_x_obj['short_sentences'] = train_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 10 and length <18 for length in sentence_lengths])))\n",
    "train_x_obj['medium_sentences'] = train_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 18 and length <25 for length in sentence_lengths])))\n",
    "train_x_obj['long_sentences'] = train_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 25 for length in sentence_lengths])))\n",
    "train_x_obj['variance_sentence_length'] = train_x_obj.sentence_lengths.apply(lambda sentence_lengths: np.std(sentence_lengths))\n",
    "\n",
    "train_x_obj['max_height'] = train_x_obj.tree_heights.apply(lambda heights: float(max(heights)))\n",
    "train_x_obj['sum_heights'] = train_x_obj.tree_heights.apply(sum)\n",
    "train_x_obj['mean_heights'] = train_x_obj.tree_heights.apply(np.mean)\n",
    "\n",
    "# train_x_obj['mean_sentence_similarity'] = train_x_obj.doc.apply(get_similarity_scores)\n",
    "\n",
    "# # POS Ngrams\n",
    "train_x_obj['pos_trigram_ratio'] = train_x_obj.pos_trigrams.apply(get_posngram_ratio)\n",
    "train_x_obj['pos_fourgram_ratio'] = train_x_obj.pos_fourgrams.apply(get_posngram_ratio)\n",
    "train_x_obj['mean_trigram_tfTF'] = train_x_obj.pos_trigram_counts.apply(lambda pos_trigram_counts: get_mean_tfTF(pos_trigram_counts, TF=tri_pos_TF))\n",
    "train_x_obj['mean_fourgram_tfTF'] = train_x_obj.pos_fourgram_counts.apply(lambda pos_fourgram_counts: get_mean_tfTF(pos_fourgram_counts, TF=four_pos_TF))\n",
    "\n",
    "# # Cohesion Features\n",
    "train_x_obj['connectives'] = train_x_obj.doc.apply(get_connectives)\n",
    "\n",
    "# Readability Features\n",
    "train_x_obj['flesch_kincaid_grade_level'] = train_x_obj.reading_scores.apply(lambda score_dict:score_dict['flesch_kincaid_grade_level'])\n",
    "train_x_obj['flesch_reading_ease'] = train_x_obj.reading_scores.apply(lambda score_dict:score_dict['flesch_reading_ease'])\n",
    "train_x_obj['gunning_fog_index'] = train_x_obj.reading_scores.apply(lambda score_dict:score_dict['gunning_fog_index'])\n",
    "train_x_obj['coleman_liau_index'] = train_x_obj.reading_scores.apply(lambda score_dict:score_dict['coleman_liau_index'])\n",
    "train_x_obj['automated_readability_index'] = train_x_obj.reading_scores.apply(lambda score_dict:score_dict['automated_readability_index'])\n",
    "train_x_obj['lix'] = train_x_obj.reading_scores.apply(lambda score_dict:score_dict['lix'])\n",
    "train_x_obj['gulpease_index'] = train_x_obj.reading_scores.apply(lambda score_dict:score_dict['gulpease_index'])\n",
    "train_x_obj['wiener_sachtextformel'] = train_x_obj.reading_scores.apply(lambda score_dict:score_dict['wiener_sachtextformel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preengineering\n",
    "test_x_obj['sentences'] = test_x_obj.doc.apply(get_sentences)\n",
    "test_x_obj['word_counts'] = test_x_obj.doc.apply(get_word_counts)\n",
    "test_x_obj['word_lengths'] = test_x_obj.doc.apply(get_word_lengths)\n",
    "test_x_obj['pos'] = test_x_obj.doc.apply(get_pos)\n",
    "\n",
    "test_x_obj['pos_trigrams'] = test_x_obj.pos.apply(lambda pos: get_posngrams(pos, n=3))\n",
    "test_x_obj['pos_fourgrams'] = test_x_obj.pos.apply(lambda pos: get_posngrams(pos, n=4))\n",
    "test_x_obj['pos_trigram_counts'] = test_x_obj.pos_trigrams.apply(get_posgrams_counts)\n",
    "test_x_obj['pos_fourgram_counts'] = test_x_obj.pos_fourgrams.apply(get_posgrams_counts)\n",
    "tri_pos_TF = get_TF(test_x_obj.pos_trigram_counts)\n",
    "four_pos_TF = get_TF(test_x_obj.pos_fourgram_counts)\n",
    "\n",
    "test_x_obj['tree_heights'] = test_x_obj.doc.apply(lambda doc: get_tree_heights(doc))\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "test_x_obj['reading_scores'] = test_x_obj.doc.apply(get_reading_scores)\n",
    "\n",
    "# Lexical Features\n",
    "test_x_obj['words_length_4'] = test_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 4,6))\n",
    "test_x_obj['words_length_6'] = test_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 6,8))\n",
    "test_x_obj['words_length_8'] = test_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 8,10))\n",
    "test_x_obj['words_length_10'] = test_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 10,12))\n",
    "test_x_obj['words_length_12'] = test_x_obj.word_lengths.apply(lambda lengths: get_words_of_length(lengths, 12,100))\n",
    "test_x_obj['mean_word_length'] = test_x_obj.word_lengths.apply(np.mean)\n",
    "test_x_obj['variance_word_length'] = test_x_obj.word_lengths.apply(np.std)\n",
    "\n",
    "\n",
    "# Length Features\n",
    "test_x_obj['essay_length'] = test_x_obj.doc.apply(len)\n",
    "test_x_obj['num_words'] = test_x_obj.doc.apply(lambda doc: float(len([word for word in doc if word.is_alpha])))\n",
    "test_x_obj['num_sentences'] = test_x_obj.sentences.apply(get_sentence_count)\n",
    "test_x_obj['mean_sentence_length'] = test_x_obj.num_words/test_x_obj.num_sentences\n",
    "test_x_obj['num_characters'] = test_x_obj.essay.apply(len)\n",
    "test_x_obj['fourth_root_num_characters'] = test_x_obj.num_characters.apply(nth_root, n=4)\n",
    "\n",
    "# # Occurrence Features\n",
    "test_x_obj['num_commas'] = test_x_obj.essay.apply(lambda essay: float(essay.count(',')))\n",
    "test_x_obj['num_periods'] = test_x_obj.essay.apply(lambda essay: float(essay.count('.')))\n",
    "test_x_obj['num_exclaim'] = test_x_obj.essay.apply(lambda essay: float(essay.count('!')))\n",
    "test_x_obj['num_question'] = test_x_obj.essay.apply(lambda essay: float(essay.count('?')))\n",
    "test_x_obj['num_semicolon'] = test_x_obj.essay.apply(lambda essay: float(essay.count(';')))\n",
    "test_x_obj['num_colon'] = test_x_obj.essay.apply(lambda essay: float(essay.count(':')))\n",
    "\n",
    "# # Style Features\n",
    "# FIX test_x_obj['vocabulary'] = test_x_obj.word_tokens.apply(lambda word_tokens: set(word.lower() for word in word_tokens if word.isalpha()))\n",
    "test_x_obj['vocab_size'] = test_x_obj.word_counts.apply(len)\n",
    "test_x_obj['type_token_ratio'] = test_x_obj.word_counts.apply(len) / test_x_obj.essay_length\n",
    "test_x_obj['yules_k'] = test_x_obj.word_counts.apply(get_yules_k)\n",
    "\n",
    "# # Syntactical Features\n",
    "# # the number for these lengths comes from Chen and He 2013\n",
    "test_x_obj['sentence_lengths'] = test_x_obj.sentences.apply(lambda sentences: [len(sent) for sent in sentences])\n",
    "test_x_obj['very_short_sentences'] = test_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length <= 10 for length in sentence_lengths])))\n",
    "test_x_obj['short_sentences'] = test_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 10 and length <18 for length in sentence_lengths])))\n",
    "test_x_obj['medium_sentences'] = test_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 18 and length <25 for length in sentence_lengths])))\n",
    "test_x_obj['long_sentences'] = test_x_obj.sentence_lengths.apply(lambda sentence_lengths: float(sum([length > 25 for length in sentence_lengths])))\n",
    "test_x_obj['variance_sentence_length'] = test_x_obj.sentence_lengths.apply(lambda sentence_lengths: np.std(sentence_lengths))\n",
    "\n",
    "test_x_obj['max_height'] = test_x_obj.tree_heights.apply(lambda heights: float(max(heights)))\n",
    "test_x_obj['sum_heights'] = test_x_obj.tree_heights.apply(sum)\n",
    "test_x_obj['mean_heights'] = test_x_obj.tree_heights.apply(np.mean)\n",
    "\n",
    "# test_x_obj['mean_sentence_similarity'] = test_x_obj.doc.apply(get_similarity_scores)\n",
    "\n",
    "# # POS Ngrams\n",
    "test_x_obj['pos_trigram_ratio'] = test_x_obj.pos_trigrams.apply(get_posngram_ratio)\n",
    "test_x_obj['pos_fourgram_ratio'] = test_x_obj.pos_fourgrams.apply(get_posngram_ratio)\n",
    "test_x_obj['mean_trigram_tfTF'] = test_x_obj.pos_trigram_counts.apply(lambda pos_trigram_counts: get_mean_tfTF(pos_trigram_counts, TF=tri_pos_TF))\n",
    "test_x_obj['mean_fourgram_tfTF'] = test_x_obj.pos_fourgram_counts.apply(lambda pos_fourgram_counts: get_mean_tfTF(pos_fourgram_counts, TF=four_pos_TF))\n",
    "\n",
    "# # Cohesion Features\n",
    "test_x_obj['connectives'] = test_x_obj.doc.apply(get_connectives)\n",
    "\n",
    "# Readability Features\n",
    "test_x_obj['flesch_kincaid_grade_level'] = test_x_obj.reading_scores.apply(lambda score_dict:score_dict['flesch_kincaid_grade_level'])\n",
    "test_x_obj['flesch_reading_ease'] = test_x_obj.reading_scores.apply(lambda score_dict:score_dict['flesch_reading_ease'])\n",
    "test_x_obj['gunning_fog_index'] = test_x_obj.reading_scores.apply(lambda score_dict:score_dict['gunning_fog_index'])\n",
    "test_x_obj['coleman_liau_index'] = test_x_obj.reading_scores.apply(lambda score_dict:score_dict['coleman_liau_index'])\n",
    "test_x_obj['automated_readability_index'] = test_x_obj.reading_scores.apply(lambda score_dict:score_dict['automated_readability_index'])\n",
    "test_x_obj['lix'] = test_x_obj.reading_scores.apply(lambda score_dict:score_dict['lix'])\n",
    "test_x_obj['gulpease_index'] = test_x_obj.reading_scores.apply(lambda score_dict:score_dict['gulpease_index'])\n",
    "test_x_obj['wiener_sachtextformel'] = test_x_obj.reading_scores.apply(lambda score_dict:score_dict['wiener_sachtextformel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "colList = ['words_length_4', 'words_length_6', 'words_length_8', 'words_length_10',\n",
    "       'words_length_12', 'mean_word_length', 'variance_word_length',\n",
    "       'essay_length', 'num_words', 'num_sentences', 'mean_sentence_length',\n",
    "       'num_characters', 'fourth_root_num_characters', 'num_commas',\n",
    "       'num_periods', 'num_exclaim', 'num_question', 'num_semicolon',\n",
    "       'num_colon', 'vocab_size', 'type_token_ratio', 'yules_k',\n",
    "       'very_short_sentences', 'short_sentences', 'medium_sentences',\n",
    "       'long_sentences', 'variance_sentence_length', 'max_height',\n",
    "       'sum_heights', 'mean_heights', 'pos_trigram_ratio',\n",
    "       'pos_fourgram_ratio', 'mean_trigram_tfTF', 'mean_fourgram_tfTF',\n",
    "       'connectives', 'flesch_kincaid_grade_level', 'flesch_reading_ease',\n",
    "       'gunning_fog_index', 'coleman_liau_index',\n",
    "       'automated_readability_index', 'lix', 'gulpease_index',\n",
    "       'wiener_sachtextformel', 'essay_set']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load RF scores to compare\n",
    "BeauRFScores = pickle.load(open('Beau_scoring_results.pkl', 'rb'))\n",
    "\n",
    "BeauRFScores['RNN'] = rnn_3.predict(sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class))\n",
    "\n",
    "BeauRFScores['RF_unnorm'] = np.round(BeauRFScores.prediction * BeauRFScores['div'])\n",
    "BeauRFScores['RNN_unnorm'] = np.round(BeauRFScores.RNN * BeauRFScores['div'])\n",
    "BeauRFScores['actual_unnorm'] = np.round(BeauRFScores.actual * BeauRFScores['div'])\n",
    "\n",
    "BeauRFScores['abs_error_rf'] = BeauRFScores.prediction - BeauRFScores.actual\n",
    "BeauRFScores['abs_error_rnn'] = BeauRFScores.RNN - BeauRFScores.actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6944040838198994, 0.0)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.stats.pearsonr(BeauRFScores['abs_error_rf'],BeauRFScores['abs_error_rnn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'RNN absolute error')"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = BeauRFScores.plot.scatter(x='abs_error_rf',y='abs_error_rnn', title='RF vs RNN absolute errors')\n",
    "ax.set_xlabel(\"RF absolute error\")\n",
    "ax.set_ylabel(\"RNN absolute error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disagreement</th>\n",
       "      <th>essay</th>\n",
       "      <th>RNN</th>\n",
       "      <th>RF</th>\n",
       "      <th>actual</th>\n",
       "      <th>essay_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6876</th>\n",
       "      <td>0.833719</td>\n",
       "      <td>This conclusion of the story provides a sense ...</td>\n",
       "      <td>0.936856</td>\n",
       "      <td>0.103137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6856</th>\n",
       "      <td>0.784623</td>\n",
       "      <td>I believe that the author  concludes the story...</td>\n",
       "      <td>0.035024</td>\n",
       "      <td>0.819647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6601</th>\n",
       "      <td>0.780497</td>\n",
       "      <td>The author Minfong Ho concluded the story Win...</td>\n",
       "      <td>0.955620</td>\n",
       "      <td>0.175123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>0.703108</td>\n",
       "      <td>I think the author puts that paragraph in the ...</td>\n",
       "      <td>0.151589</td>\n",
       "      <td>0.854698</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6729</th>\n",
       "      <td>0.691801</td>\n",
       "      <td>This story is all about overcoming hardships a...</td>\n",
       "      <td>0.939040</td>\n",
       "      <td>0.247239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      disagreement                                              essay  \\\n",
       "6876      0.833719  This conclusion of the story provides a sense ...   \n",
       "6856      0.784623  I believe that the author  concludes the story...   \n",
       "6601      0.780497  The author Minfong Ho concluded the story Win...   \n",
       "6918      0.703108  I think the author puts that paragraph in the ...   \n",
       "6729      0.691801  This story is all about overcoming hardships a...   \n",
       "\n",
       "           RNN        RF  actual  essay_set  \n",
       "6876  0.936856  0.103137     1.0        4.0  \n",
       "6856  0.035024  0.819647     0.0        4.0  \n",
       "6601  0.955620  0.175123     1.0        4.0  \n",
       "6918  0.151589  0.854698     0.0        4.0  \n",
       "6729  0.939040  0.247239     0.0        4.0  "
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "BeauRFScores.index = test_sa_y_class.index\n",
    "\n",
    "Delta_preds = np.abs(BeauRFScores.RNN - BeauRFScores.prediction)\n",
    "Delta_preds = pd.DataFrame(Delta_preds)\n",
    "Delta_preds['essay'] = test_sa_x_class\n",
    "Delta_preds['RNN'] = BeauRFScores.RNN\n",
    "Delta_preds['RF'] = BeauRFScores.prediction\n",
    "Delta_preds['actual'] = test_sa_y_class.adjusted_domain1_score\n",
    "Delta_preds['essay_set'] = BeauRFScores.essay_set\n",
    "Delta_preds.columns = ['disagreement','essay', 'RNN','RF', 'actual', 'essay_set']\n",
    "\n",
    "#find the five essays most disagreed upon by the two predictors\n",
    "Delta_preds.sort_values('disagreement',ascending=False).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta_preds['RNN_diff'] = np.abs(Delta_preds['RNN'] - Delta_preds['actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essay 6729, was predicted 0.939 by RNN, 0.247 by RF, answer was 0.0, belonged to 4.0 \n",
      "This story is all about overcoming hardships and disappointments as well as accepting and adapting to the things life throws at people, so it is appropriate that the story is ended with a goal and a determined attitude. Throughout the atire reading, Saeng sought comfort in the things most familiar to her, such as flower and the taste of bitter melon. She was hurting and disappointed, and those things were the only things that gave her peace. It was those things that save her the courage she needed to retake her drivers test in the spring, so it seems fitting that she end the story with the mention of the hibiscus. The end paragraph also shows that Saeng is adapting well to her new country. She says, when they come back... in the spring, when the snow melts and the geese return... At first the sounds of geese were alien to her, but now she has accepted their honking as a normal sound. This implies that she is learning to accept her new country, which is the perfect way to end the story.\n",
      "\n",
      "essay 6526, was predicted 0.854 by RNN, 0.945 by RF, answer was 0.0, belonged to 4.0 \n",
      "In the short story, Winter Hibiscus, by Minfong Ho, the author includes a paragraph at the end, stating that the main character, Saeng, will take her driving test again. Ho, includes this paragraph to show that Saeng is determined to adapt. Saeng is new to the United States, her and her mother had moved here from Vietnam. Throughout the story she is reminising about her home life in Vietnam. About her grandmother, and the flowers there. Where was me...hibiscus hedge?.. Her gentle grandmother? When at the flower shop, Saeng finds an hibiscus, flower they also had in Vietnam. She sees how it has adapted to survive in the winter here. As she is planting the flower in her backyard, she makes a promise to herself to pass the driving test next spring. She too, wants to become adapted to the @LOCATION3 and beable to live here without missing home too much. This vow she made is included in the last paragraph. The author included it to show Saeng too, wants to adapt to her surrounding and pass the driving test,\n",
      "\n",
      "essay 3804, was predicted 0.844 by RNN, 0.84 by RF, answer was 0.0, belonged to 3.0 \n",
      "The cyclist in this essay was a very brave man. He had many obsticles in his journey but he still made it. Nature was most definatly not on his side that day. He had to ride his bike up and down hills several times which wears you out very quickly. Flat road was replaced by short rolling hills, After he got threw the hills, he came to a sigh  ROUGH ROAD AHEAD: DO NOT EXCEED POSTED SPEED LIMIT.  Now he has to go through bumply roads. At some point, tumbleweeds crossed my path and a ridiculously large snake- it really did look like a diamond back-blocked the majority of the pavement in front of me. Now the poor guy had to deal with snakes bigger than him! Thankfully he got by the snake, alive. Over one long cripping hill, a building came into view. The man thinks he found someplace to get water, but its an abonded building . ...,by the looks of it  and been a welchs grape Juice Factory and botting plant..\n",
      "\n",
      "essay 4098, was predicted 0.297 by RNN, 0.391 by RF, answer was 1.0, belonged to 3.0 \n",
      "Many things caneffect a cyclist and how they ride, in this essay many things effected him such as the setting of the story. One of the problems of the setting was there were no stores or even people, another and most important thing was the lack of water around him, so many things can effect a cyclist while They are riding inculding there suriondings.\n",
      "\n",
      "essay 4842, was predicted 0.973 by RNN, 0.958 by RF, answer was 0.333, belonged to 3.0 \n",
      "How the features of the setting affected the cyclist in the story, Do Not Exceed Posted Limit, were very dangerous to his life. The cyclist was traveling to the Yosemite National Park on a hot @DATE1. The cyclist traveled miles after miles on his bike only to past by deserted towns, This place might have been thriving little spot...a ghost town. The terrain was threatening because after twenty two miles or so, the cyclist had to overcome, short rolling hills, which would use more energy and make his thirst for water even stronger. The cyclist was out of water and the hot day was not helping. After a grueling journey, the cyclist used all the strength he had to go to a local bait shop and drank from the sink after the trechorous environment he was exposed to. The features of the setting in the story, Do Not Exceed Posted Speed Limit were life threatening and almost killed the cyclist, due to lack of water and the terrain he overcome.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in Delta_preds.sort_values('RNN_diff',ascending=False).head(5).iterrows():\n",
    "    print(\"essay {0}, was predicted {1} by RNN, {2} by RF, answer was {3}, belonged to {4} \".format(row[0],\n",
    "                                                                                   np.around(row[1]['RNN'],3),\n",
    "                                                                                   np.around(row[1]['RF'],3),\n",
    "                                                                                  np.around(row[1]['actual'], 3),\n",
    "                                                                                            row[1]['essay_set']))\n",
    "    print(row[1]['essay'])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essay 6876, was predicted 0.937 by RNN, 0.103 by RF, answer was 1.0, belonged to 4.0 \n",
      "This conclusion of the story provides a sense of hope for the future. The girl is upset that she failed the test but still has hope for what will happen next. In the story the narrator says, She lifted her head. This quote is showing that she is not going to quit and give up, but that she will try until she succeeds. The conclusion of the story leaves readers to wonder if she will try harder and pass the test for the second time. When she says, when the snow melts and the geese return, this shows us that she is like the geese because their both going back for a hand fought journey. Also, when she says, and this hibiscus budding she is saying when everything comes to life and is at its highest peak she will take the test because she will have the most confidence. The author concludes the story to show that there is hope for the future and that she is not going to give up. Although she is upset about failing the test she will certainly be ready to pass it in the spring.\n",
      "\n",
      "essay 6856, was predicted 0.035 by RNN, 0.82 by RF, answer was 0.0, belonged to 4.0 \n",
      "I believe that the author  concludes the story with this paragraph because it's what the story is all about it just exploins what happened in the end as if what happeed through the story was the growth and living of a flower.\n",
      "\n",
      "essay 6601, was predicted 0.956 by RNN, 0.175 by RF, answer was 1.0, belonged to 4.0 \n",
      "The author Minfong Ho concluded the story Winter Hibiscus, with that certain paragraph to show that Saeng would not give up, that she was determined to pass the driving test and also in a way get used to her new home. It is kind of like a motivation, when the snows melt and the geese return and this hibiscus is budding, then I will take that test again. The motivation is the flower, bringing back good people memories that made her happy and believed in her. Minfong also concluded the story the way he/she did to say, by next year Saeng will start fresh. To become happy with her new life and not dwell on her past. Saeng says, .. in the spring... its the beginning of a new year to start over, and rejoice. This was a really good way to end the story, because it sets a positive vibe. Its letting the readers know that Saeng is a strong person, who can remember the past but move on to a new day and life. And I think the author is trying to point out that everyone should have the strength to do the same.  \n",
      "\n",
      "essay 6918, was predicted 0.152 by RNN, 0.855 by RF, answer was 0.0, belonged to 4.0 \n",
      "I think the author puts that paragraph in the story because maybe because its a good luck thing or its a tradition so its very important that its in the story. Also it could be trying to tell us something like she only take test in the spring when the hibiscus starts budding.\n",
      "\n",
      "essay 6729, was predicted 0.939 by RNN, 0.247 by RF, answer was 0.0, belonged to 4.0 \n",
      "This story is all about overcoming hardships and disappointments as well as accepting and adapting to the things life throws at people, so it is appropriate that the story is ended with a goal and a determined attitude. Throughout the atire reading, Saeng sought comfort in the things most familiar to her, such as flower and the taste of bitter melon. She was hurting and disappointed, and those things were the only things that gave her peace. It was those things that save her the courage she needed to retake her drivers test in the spring, so it seems fitting that she end the story with the mention of the hibiscus. The end paragraph also shows that Saeng is adapting well to her new country. She says, when they come back... in the spring, when the snow melts and the geese return... At first the sounds of geese were alien to her, but now she has accepted their honking as a normal sound. This implies that she is learning to accept her new country, which is the perfect way to end the story.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in Delta_preds.sort_values('disagreement',ascending=False).head(5).iterrows():\n",
    "    print(\"essay {0}, was predicted {1} by RNN, {2} by RF, answer was {3}, belonged to {4} \".format(row[0],\n",
    "                                                                                   np.around(row[1]['RNN'],3),\n",
    "                                                                                   np.around(row[1]['RF'],3),\n",
    "                                                                                  np.around(row[1]['actual'], 3),\n",
    "                                                                                            row[1]['essay_set']))\n",
    "    print(row[1]['essay'])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8143151173677936,\n",
       " 0.6977489848749655,\n",
       " 0.6107347902945732,\n",
       " 0.696475303031962,\n",
       " 0.7894096920671944,\n",
       " 0.6397463345890132,\n",
       " 0.7241604040254694,\n",
       " 0.6731841907101861]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = []\n",
    "for essaySetValue in np.unique(BeauRFScores.essay_set):\n",
    "    temp_deepLin = BeauRFScores[BeauRFScores.essay_set == essaySetValue]\n",
    "    vals.append(cohen_kappa_score(temp_deepLin.actual_unnorm, temp_deepLin.RF_unnorm.astype('category'), weights='quadratic'))\n",
    "    \n",
    "vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embed_input (InputLayer)        (None, 588)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embed (Embedding)               (None, 588, 300)     10930500    embed_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "LSTM1 (LSTM)                    (None, 588, 20)      25680       embed[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "LSTM3 (LSTM)                    (None, 588, 20)      3280        LSTM1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ManualFeatureConnected_input (I (None, 44)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "LSTM2 (LSTM)                    (None, 20)           3280        LSTM3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ManualFeatureConnected (Dense)  (None, 25)           1125        ManualFeatureConnected_input[0][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 45)           0           LSTM2[0][0]                      \n",
      "                                                                 ManualFeatureConnected[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "out_layer (Dense)               (None, 1)            46          concatenate_13[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 10,963,911\n",
      "Trainable params: 33,411\n",
      "Non-trainable params: 10,930,500\n",
      "__________________________________________________________________________________________________\n",
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 238s 27ms/step - loss: 349.3594 - acc: 0.0099 - val_loss: 32.4913 - val_acc: 0.0122\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 219s 25ms/step - loss: 20.5417 - acc: 0.0182 - val_loss: 13.9245 - val_acc: 0.0180\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 218s 25ms/step - loss: 10.1626 - acc: 0.0243 - val_loss: 9.1447 - val_acc: 0.0276\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 216s 24ms/step - loss: 6.4286 - acc: 0.0295 - val_loss: 7.3773 - val_acc: 0.0308\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 217s 25ms/step - loss: 4.7551 - acc: 0.0332 - val_loss: 7.0951 - val_acc: 0.0385\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 217s 25ms/step - loss: 3.8615 - acc: 0.0359 - val_loss: 11.7116 - val_acc: 0.0276\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 216s 24ms/step - loss: 3.3379 - acc: 0.0410 - val_loss: 5.4719 - val_acc: 0.0507\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 215s 24ms/step - loss: 2.9935 - acc: 0.0400 - val_loss: 5.5410 - val_acc: 0.0501\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 220s 25ms/step - loss: 2.7485 - acc: 0.0422 - val_loss: 5.1170 - val_acc: 0.0501\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 219s 25ms/step - loss: 2.6173 - acc: 0.0381 - val_loss: 4.9738 - val_acc: 0.0546\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 222s 25ms/step - loss: 2.4494 - acc: 0.0427 - val_loss: 5.1026 - val_acc: 0.0546\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 220s 25ms/step - loss: 2.3545 - acc: 0.0401 - val_loss: 7.1762 - val_acc: 0.0186\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 220s 25ms/step - loss: 2.2564 - acc: 0.0418 - val_loss: 4.6924 - val_acc: 0.0430\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 218s 25ms/step - loss: 2.1312 - acc: 0.0452 - val_loss: 4.5130 - val_acc: 0.0533\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 222s 25ms/step - loss: 2.0394 - acc: 0.0439 - val_loss: 5.0393 - val_acc: 0.0514\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 222s 25ms/step - loss: 1.9463 - acc: 0.0419 - val_loss: 4.6253 - val_acc: 0.0411\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 222s 25ms/step - loss: 1.8799 - acc: 0.0452 - val_loss: 4.1605 - val_acc: 0.0462\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 223s 25ms/step - loss: 1.8317 - acc: 0.0467 - val_loss: 3.9806 - val_acc: 0.0559\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 224s 25ms/step - loss: 1.7465 - acc: 0.0458 - val_loss: 3.9277 - val_acc: 0.0475\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 224s 25ms/step - loss: 1.6948 - acc: 0.0465 - val_loss: 3.9494 - val_acc: 0.0584\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 218s 25ms/step - loss: 1.6693 - acc: 0.0485 - val_loss: 3.6750 - val_acc: 0.0539\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 215s 24ms/step - loss: 1.5987 - acc: 0.0481 - val_loss: 3.5920 - val_acc: 0.0552\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 216s 25ms/step - loss: 1.5219 - acc: 0.0483 - val_loss: 3.4973 - val_acc: 0.0546\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 226s 26ms/step - loss: 1.4767 - acc: 0.0522 - val_loss: 4.0092 - val_acc: 0.0584\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 223s 25ms/step - loss: 1.4117 - acc: 0.0529 - val_loss: 3.4333 - val_acc: 0.0495\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 217s 25ms/step - loss: 1.3948 - acc: 0.0533 - val_loss: 3.2795 - val_acc: 0.0565\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 217s 25ms/step - loss: 1.3327 - acc: 0.0543 - val_loss: 3.1850 - val_acc: 0.0623\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 215s 24ms/step - loss: 1.2735 - acc: 0.0578 - val_loss: 3.3945 - val_acc: 0.0636\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 218s 25ms/step - loss: 1.2710 - acc: 0.0560 - val_loss: 2.9445 - val_acc: 0.0584\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 215s 24ms/step - loss: 1.2148 - acc: 0.0593 - val_loss: 2.9118 - val_acc: 0.0552\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 220s 25ms/step - loss: 1.1792 - acc: 0.0564 - val_loss: 3.0006 - val_acc: 0.0655\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 220s 25ms/step - loss: 1.1455 - acc: 0.0595 - val_loss: 2.7662 - val_acc: 0.0552\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 222s 25ms/step - loss: 1.1350 - acc: 0.0586 - val_loss: 2.8067 - val_acc: 0.0546\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 218s 25ms/step - loss: 1.1391 - acc: 0.0597 - val_loss: 2.5994 - val_acc: 0.0636\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 216s 25ms/step - loss: 1.0781 - acc: 0.0591 - val_loss: 2.5575 - val_acc: 0.0610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd15146cc88>"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "RNN_comb_best = Sequential()\n",
    "RNN_comb_best.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],\n",
    "                       input_length=max_len_class,trainable=False,name='embed'))\n",
    "RNN_comb_best.add(LSTM(20, return_sequences=True,name='LSTM1'))\n",
    "RNN_comb_best.add(LSTM(20, return_sequences=True,name='LSTM3'))\n",
    "RNN_comb_best.add(LSTM(20,name='LSTM2'))\n",
    "\n",
    "Manual_Features_comb = Sequential()\n",
    "Manual_Features_comb.add(Dense(25,input_shape=(len(colList),),name='ManualFeatureConnected'))\n",
    "\n",
    "mergedOut = Concatenate()([RNN_comb_best.output, Manual_Features_comb.output])\n",
    "mergedOut = Dense(1, name='out_layer')(mergedOut)\n",
    "\n",
    "rnn_combined_final = Model([RNN_comb_best.input, Manual_Features_comb.input], mergedOut)\n",
    "rnn_combined_final.compile(optimizer='adagrad',loss='mse', metrics=['accuracy'])\n",
    "rnn_combined_final.summary()\n",
    "\n",
    "\n",
    "rnn_combined_final.fit([sequences_matrix_class, np.array(train_x_obj[colList].fillna(0))],train_sa_y_class.adjusted_domain1_score, batch_size = 100, epochs=35, validation_split=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embed_input (InputLayer)        (None, 588)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embed (Embedding)               (None, 588, 300)     10930500    embed_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "deep1 (Dense)                   (None, 588, 100)     30100       embed[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "deep2 (Dense)                   (None, 588, 50)      5050        deep1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ManualFeatureConnected_input (I (None, 44)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 29400)        0           deep2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "ManualFeatureConnected (Dense)  (None, 25)           1125        ManualFeatureConnected_input[0][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 29425)        0           flatten_26[0][0]                 \n",
      "                                                                 ManualFeatureConnected[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 29425)        0           concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "out_layer (Dense)               (None, 51)           1500726     dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,467,501\n",
      "Trainable params: 1,537,001\n",
      "Non-trainable params: 10,930,500\n",
      "__________________________________________________________________________________________________\n",
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 24s 3ms/step - loss: 3.4936 - acc: 0.0611 - val_loss: 3.3106 - val_acc: 0.0694\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 14s 2ms/step - loss: 3.0212 - acc: 0.0891 - val_loss: 2.9772 - val_acc: 0.0835\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 13s 2ms/step - loss: 2.6285 - acc: 0.1120 - val_loss: 2.7056 - val_acc: 0.1741\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 2.2842 - acc: 0.2117 - val_loss: 2.4802 - val_acc: 0.2261\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 13s 2ms/step - loss: 1.9047 - acc: 0.3090 - val_loss: 2.5422 - val_acc: 0.2852\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 1.6567 - acc: 0.4498 - val_loss: 2.4570 - val_acc: 0.4001\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 1.4076 - acc: 0.5294 - val_loss: 2.6545 - val_acc: 0.4245\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 12s 1ms/step - loss: 1.1629 - acc: 0.6468 - val_loss: 2.4838 - val_acc: 0.4528\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 0.9839 - acc: 0.6998 - val_loss: 2.8352 - val_acc: 0.4586\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 14s 2ms/step - loss: 0.8398 - acc: 0.7594 - val_loss: 2.9207 - val_acc: 0.4669\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 14s 2ms/step - loss: 0.7274 - acc: 0.7987 - val_loss: 2.8078 - val_acc: 0.4637\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 0.5354 - acc: 0.8204 - val_loss: 2.9271 - val_acc: 0.4753\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 14s 2ms/step - loss: 0.4229 - acc: 0.8631 - val_loss: 3.2460 - val_acc: 0.4714\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 0.3587 - acc: 0.8888 - val_loss: 3.5576 - val_acc: 0.4740\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 0.3244 - acc: 0.9017 - val_loss: 3.7580 - val_acc: 0.4689\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 0.2866 - acc: 0.9196 - val_loss: 3.5888 - val_acc: 0.4663\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 0.2470 - acc: 0.9391 - val_loss: 3.8960 - val_acc: 0.4663\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 0.2231 - acc: 0.9470 - val_loss: 4.2078 - val_acc: 0.4599\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 12s 1ms/step - loss: 0.1995 - acc: 0.9551 - val_loss: 4.3343 - val_acc: 0.4592\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 0.1887 - acc: 0.9618 - val_loss: 4.4001 - val_acc: 0.4509\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 0.1705 - acc: 0.9676 - val_loss: 4.6841 - val_acc: 0.4624\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 0.1593 - acc: 0.9711 - val_loss: 4.6085 - val_acc: 0.4451\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 14s 2ms/step - loss: 0.1446 - acc: 0.9772 - val_loss: 4.6976 - val_acc: 0.4541\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 0.1437 - acc: 0.9763 - val_loss: 4.8097 - val_acc: 0.4644\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 13s 2ms/step - loss: 0.1344 - acc: 0.9789 - val_loss: 4.8253 - val_acc: 0.4451\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 0.1323 - acc: 0.9803 - val_loss: 4.7882 - val_acc: 0.4566\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 13s 2ms/step - loss: 0.1289 - acc: 0.9806 - val_loss: 4.7412 - val_acc: 0.4496\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 12s 1ms/step - loss: 0.1275 - acc: 0.9816 - val_loss: 4.8120 - val_acc: 0.4489\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 14s 2ms/step - loss: 0.1201 - acc: 0.9829 - val_loss: 4.8806 - val_acc: 0.4573\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 14s 2ms/step - loss: 0.1196 - acc: 0.9838 - val_loss: 4.9379 - val_acc: 0.4522\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 15s 2ms/step - loss: 0.1151 - acc: 0.9864 - val_loss: 4.9913 - val_acc: 0.4618\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 14s 2ms/step - loss: 0.1111 - acc: 0.9865 - val_loss: 4.9575 - val_acc: 0.4656\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 13s 2ms/step - loss: 0.1189 - acc: 0.9847 - val_loss: 4.8852 - val_acc: 0.4522\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 0.1157 - acc: 0.9846 - val_loss: 5.1102 - val_acc: 0.4483\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 13s 1ms/step - loss: 0.1176 - acc: 0.9829 - val_loss: 5.1529 - val_acc: 0.4599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce84b2ab70>"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "\n",
    "FF_comb = Sequential()\n",
    "FF_comb.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],\n",
    "                       input_length=max_len_class,trainable=False,name='embed'))\n",
    "FF_comb.add(Dense(100,name='deep1'))\n",
    "FF_comb.add(Dense(50,name='deep2'))\n",
    "FF_comb.add(Flatten())\n",
    "\n",
    "Manual_Features_comb_ff = Sequential()\n",
    "Manual_Features_comb_ff.add(Dense(25,input_shape=(len(colList),),name='ManualFeatureConnected'))\n",
    "\n",
    "mergedOut_ff = Concatenate()([FF_comb.output, Manual_Features_comb_ff.output])\n",
    "mergedOut_ff = Dropout(0.2)(mergedOut_ff)\n",
    "mergedOut_ff = Dense(len(pd.get_dummies(train_sa_y_class.adjusted_domain1_score).columns),activation='sigmoid',name='out_layer')(mergedOut_ff)\n",
    "\n",
    "\n",
    "ff_combined_final = Model([FF_comb.input, Manual_Features_comb_ff.input], mergedOut_ff)\n",
    "ff_combined_final.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "ff_combined_final.summary()\n",
    "\n",
    "#final_model.summary()\n",
    "#sahil_model_comb.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "ff_combined_final.fit([sequences_matrix_class, np.array(train_x_obj[colList].fillna(0))],np.asarray(pd.get_dummies(train_sa_y_class.adjusted_domain1_score)), batch_size = 150, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embed_input (InputLayer)        (None, 588)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embed (Embedding)               (None, 588, 300)     10930500    embed_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 584, 64)      96064       embed[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 292, 64)      0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 288, 64)      20544       max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 144, 64)      0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "ManualFeatureConnected_input (I (None, 44)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 9216)         0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "ManualFeatureConnected (Dense)  (None, 25)           1125        ManualFeatureConnected_input[0][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 9241)         0           flatten_25[0][0]                 \n",
      "                                                                 ManualFeatureConnected[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "out_layer (Dense)               (None, 1)            9242        concatenate_15[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 11,057,475\n",
      "Trainable params: 126,975\n",
      "Non-trainable params: 10,930,500\n",
      "__________________________________________________________________________________________________\n",
      "Train on 8823 samples, validate on 1557 samples\n",
      "Epoch 1/35\n",
      "8823/8823 [==============================] - 37s 4ms/step - loss: 19.9531 - acc: 0.0543 - val_loss: 0.2739 - val_acc: 0.0880\n",
      "Epoch 2/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.2110 - acc: 0.0850 - val_loss: 0.2720 - val_acc: 0.1047\n",
      "Epoch 3/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.1698 - acc: 0.0969 - val_loss: 0.1873 - val_acc: 0.1162\n",
      "Epoch 4/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.1427 - acc: 0.1093 - val_loss: 0.1603 - val_acc: 0.1227\n",
      "Epoch 5/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.1180 - acc: 0.1172 - val_loss: 0.1460 - val_acc: 0.1259\n",
      "Epoch 6/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.1039 - acc: 0.1203 - val_loss: 0.1354 - val_acc: 0.1291\n",
      "Epoch 7/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0925 - acc: 0.1229 - val_loss: 0.1652 - val_acc: 0.1201\n",
      "Epoch 8/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0877 - acc: 0.1252 - val_loss: 0.1254 - val_acc: 0.1387\n",
      "Epoch 9/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0726 - acc: 0.1275 - val_loss: 0.1152 - val_acc: 0.1349\n",
      "Epoch 10/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0681 - acc: 0.1288 - val_loss: 0.1065 - val_acc: 0.1407\n",
      "Epoch 11/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0594 - acc: 0.1301 - val_loss: 0.1010 - val_acc: 0.1413\n",
      "Epoch 12/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0515 - acc: 0.1323 - val_loss: 0.1088 - val_acc: 0.1413\n",
      "Epoch 13/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0546 - acc: 0.1329 - val_loss: 0.0934 - val_acc: 0.1426\n",
      "Epoch 14/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0438 - acc: 0.1343 - val_loss: 0.0945 - val_acc: 0.1374\n",
      "Epoch 15/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0418 - acc: 0.1348 - val_loss: 0.0839 - val_acc: 0.1426\n",
      "Epoch 16/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0374 - acc: 0.1356 - val_loss: 0.0838 - val_acc: 0.1452\n",
      "Epoch 17/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0397 - acc: 0.1353 - val_loss: 0.0932 - val_acc: 0.1362\n",
      "Epoch 18/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0343 - acc: 0.1365 - val_loss: 0.0758 - val_acc: 0.1452\n",
      "Epoch 19/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0304 - acc: 0.1369 - val_loss: 0.0744 - val_acc: 0.1452\n",
      "Epoch 20/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0279 - acc: 0.1376 - val_loss: 0.0702 - val_acc: 0.1452\n",
      "Epoch 21/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0302 - acc: 0.1373 - val_loss: 0.0708 - val_acc: 0.1426\n",
      "Epoch 22/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0266 - acc: 0.1380 - val_loss: 0.0666 - val_acc: 0.1445\n",
      "Epoch 23/35\n",
      "8823/8823 [==============================] - 28s 3ms/step - loss: 0.0238 - acc: 0.1379 - val_loss: 0.0655 - val_acc: 0.1458\n",
      "Epoch 24/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0246 - acc: 0.1383 - val_loss: 0.0691 - val_acc: 0.1426\n",
      "Epoch 25/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0218 - acc: 0.1385 - val_loss: 0.0701 - val_acc: 0.1464\n",
      "Epoch 26/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0215 - acc: 0.1383 - val_loss: 0.0722 - val_acc: 0.1400\n",
      "Epoch 27/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0219 - acc: 0.1390 - val_loss: 0.0626 - val_acc: 0.1445\n",
      "Epoch 28/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0196 - acc: 0.1392 - val_loss: 0.0617 - val_acc: 0.1452\n",
      "Epoch 29/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0191 - acc: 0.1395 - val_loss: 0.1045 - val_acc: 0.1439\n",
      "Epoch 30/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0270 - acc: 0.1394 - val_loss: 0.0604 - val_acc: 0.1464\n",
      "Epoch 31/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0187 - acc: 0.1397 - val_loss: 0.0674 - val_acc: 0.1458\n",
      "Epoch 32/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0207 - acc: 0.1396 - val_loss: 0.0785 - val_acc: 0.1381\n",
      "Epoch 33/35\n",
      "8823/8823 [==============================] - 27s 3ms/step - loss: 0.0154 - acc: 0.1400 - val_loss: 0.0567 - val_acc: 0.1458\n",
      "Epoch 34/35\n",
      "8823/8823 [==============================] - 28s 3ms/step - loss: 0.0212 - acc: 0.1396 - val_loss: 0.0932 - val_acc: 0.1439\n",
      "Epoch 35/35\n",
      "8823/8823 [==============================] - 28s 3ms/step - loss: 0.0263 - acc: 0.1394 - val_loss: 0.1181 - val_acc: 0.1278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fce86093048>"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "\n",
    "CNN_comb = Sequential()\n",
    "CNN_comb.add(Embedding(len(tok_class.word_index)+1,300, weights=[embedding_matrix],\n",
    "                       input_length=max_len_class,trainable=False,name='embed'))\n",
    "CNN_comb.add(Conv1D(64, 5, activation = 'relu'))\n",
    "CNN_comb.add(MaxPooling1D(2))\n",
    "CNN_comb.add(Conv1D(64, 5, activation = 'relu'))\n",
    "CNN_comb.add(MaxPooling1D(2))\n",
    "CNN_comb.add(Flatten())\n",
    "\n",
    "Manual_Features_comb_cnn = Sequential()\n",
    "Manual_Features_comb_cnn.add(Dense(25,input_shape=(len(colList),),name='ManualFeatureConnected'))\n",
    "\n",
    "mergedOut_cnn = Concatenate()([CNN_comb.output, Manual_Features_comb_cnn.output])\n",
    "mergedOut_cnn = Dense(1, name='out_layer')(mergedOut_cnn)\n",
    "\n",
    "cnn_combined_final = Model([CNN_comb.input, Manual_Features_comb_cnn.input], mergedOut_cnn)\n",
    "cnn_combined_final.compile(optimizer='adam',loss='mse', metrics=['accuracy'])\n",
    "cnn_combined_final.summary()\n",
    "\n",
    "#final_model.summary()\n",
    "#sahil_model_comb.compile(optimizer='adam',loss='mse',metrics=['accuracy'])\n",
    "cnn_combined_final.fit([sequences_matrix_class, np.array(train_x_obj[colList].fillna(0))],train_sa_y_class.adjusted_domain1_score, batch_size = 100, epochs=35, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_comb_preds = rnn_combined_final.predict([sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class), np.asarray(test_x_obj[colList].fillna(0)).reshape(sequences_test_matrix_class.shape[0],len(colList))])\n",
    "cnn_comb_preds = cnn_combined_final.predict([sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class), np.asarray(test_x_obj[colList].fillna(0)).reshape(sequences_test_matrix_class.shape[0],len(colList))])\n",
    "ff_comb_preds = ff_combined_final.predict([sequences_test_matrix_class.reshape(sequences_test_matrix_class.shape[0],max_len_class), np.asarray(test_x_obj[colList].fillna(0)).reshape(sequences_test_matrix_class.shape[0],len(colList))])\n",
    "\n",
    "ff_comb_preds = np.argmax(ff_comb_preds,axis=1)\n",
    "\n",
    "comb_pred_frame = pd.DataFrame({'RNN':rnn_comb_preds.flatten(),\n",
    "                           'CNN':cnn_comb_preds.flatten(),\n",
    "                           'FF':ff_comb_preds.flatten(),\n",
    "                           'actual':np.asarray(test_sa_y_class.adjusted_domain1_score),\n",
    "                            'dataset': np.asarray(test_sa_y_class.essay_set)})\n",
    "comb_pred_frame = comb_pred_frame.merge(DivSeries, on='dataset')\n",
    "\n",
    "comb_pred_frame.FF = comb_pred_frame.FF.apply(RetrieveNormalize)\n",
    "\n",
    "for colName in ['CNN', 'RNN', 'FF', 'actual']:\n",
    "    comb_pred_frame[colName] = comb_pred_frame[colName] * comb_pred_frame['div']\n",
    "    comb_pred_frame[colName] = comb_pred_frame[colName].apply(round)\n",
    "\n",
    "comb_pred_frame.actual = comb_pred_frame.actual.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN    0.281917\n",
       "RNN    0.051349\n",
       "FF     0.555492\n",
       "dtype: float64"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QuadKappaCalculation = pd.DataFrame(columns = ['CNN', 'RNN', 'FF'],index = np.unique(comb_pred_frame.dataset))\n",
    "for essaySetValue in np.unique(comb_pred_frame.dataset):\n",
    "    temp_comb_ES = comb_pred_frame[comb_pred_frame.dataset == essaySetValue]\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'FF'] = cohen_kappa_score(temp_comb_ES.actual, temp_comb_ES.FF.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'RNN'] = cohen_kappa_score(temp_comb_ES.actual, temp_comb_ES.RNN.apply(round),weights='quadratic')\n",
    "    QuadKappaCalculation.loc[essaySetValue, 'CNN'] = cohen_kappa_score(temp_comb_ES.actual, temp_comb_ES.CNN.apply(round),weights='quadratic')\n",
    "QuadKappaCalculation.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FF combined RMSE, Cohen, Quad Cohen, accuracy: 2.4478379504659085, 0.4099417112628072, 0.9641473609548521, 0.4842064714946071\n",
      "CNN combined RMSE, Cohen, Quad Cohen, accuracy: 7.362033533636288, 0.212759991018314, 0.5446092514531857, 0.31432973805855163\n",
      "RNN combined RMSE, Cohen, Quad Cohen, accuracy: 24.57694436143025, 0.02990794079049064, 0.2996733395574569, 0.08936825885978428\n"
     ]
    }
   ],
   "source": [
    "print(\"FF combined RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(comb_pred_frame.actual, comb_pred_frame.FF.apply(round))),\n",
    "                                       cohen_kappa_score(comb_pred_frame.actual, comb_pred_frame.FF.apply(round)),\n",
    "                                        cohen_kappa_score(comb_pred_frame.actual, comb_pred_frame.FF.apply(round),weights='quadratic'),\n",
    "                                                 accuracy_score(comb_pred_frame.actual, comb_pred_frame.FF.apply(round))))\n",
    "\n",
    "\n",
    "print(\"CNN combined RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(comb_pred_frame.actual, comb_pred_frame.CNN.apply(round))),\n",
    "                                       cohen_kappa_score(comb_pred_frame.actual, comb_pred_frame.CNN.apply(round)),\n",
    "                                        cohen_kappa_score(comb_pred_frame.actual, comb_pred_frame.CNN.apply(round),weights='quadratic'),\n",
    "                                                 accuracy_score(comb_pred_frame.actual, comb_pred_frame.CNN.apply(round))))\n",
    "\n",
    "print(\"RNN combined RMSE, Cohen, Quad Cohen, accuracy: {0}, {1}, {2}, {3}\".format(sqrt(mean_squared_error(comb_pred_frame.actual, comb_pred_frame.RNN.apply(round))),\n",
    "                                       cohen_kappa_score(comb_pred_frame.actual, comb_pred_frame.RNN.apply(round)),\n",
    "                                        cohen_kappa_score(comb_pred_frame.actual, comb_pred_frame.RNN.apply(round),weights='quadratic'),             \n",
    "                                      accuracy_score(comb_pred_frame.actual, comb_pred_frame.RNN.apply(round))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
