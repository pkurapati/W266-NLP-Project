{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "import json, os, re, shutil, sys, time\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "from collections import defaultdict\n",
    "import xmltodict\n",
    "import untangle\n",
    "import xml.etree.ElementTree as ET\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NumPy, Pandas and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "\n",
    "from numpy.random import seed\n",
    "from pandas import read_csv, DataFrame\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Activation, Dropout, Conv1D, MaxPooling1D, Bidirectional, Flatten, TimeDistributed\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AEG Long Essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score      ...        \\\n",
       "0             NaN             NaN            NaN      ...         \n",
       "1             NaN             NaN            NaN      ...         \n",
       "2             NaN             NaN            NaN      ...         \n",
       "3             NaN             NaN            NaN      ...         \n",
       "4             NaN             NaN            NaN      ...         \n",
       "\n",
       "   rater2_trait3  rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait2  rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "0            NaN            NaN            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN            NaN  \n",
       "3            NaN            NaN            NaN            NaN            NaN  \n",
       "4            NaN            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aeg_long = pd.read_csv(\"../data-DNC/AEG/training_set_rel3.tsv\",sep='\\t',encoding = \"latin1\")\n",
    "#aeg_long.columns = aeg_long.iloc[0]\n",
    "#aeg_long.reindex(aeg_long.index.drop(0))\n",
    "aeg_long.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_score(essay):\n",
    "    score = 0\n",
    "    score = float(essay[3])\n",
    "    essay_set = essay[1]\n",
    "    if essay_set == 1:\n",
    "        div = 12\n",
    "    elif essay_set == 2:\n",
    "        div = 5\n",
    "    elif essay_set == 3:\n",
    "        div = 3\n",
    "    elif essay_set == 4:\n",
    "        div = 3\n",
    "    elif essay_set == 5:\n",
    "        div = 4\n",
    "    elif essay_set == 6:\n",
    "        div = 4\n",
    "    elif essay_set == 7:\n",
    "        div = 25\n",
    "    elif essay_set == 8:\n",
    "        div = 50\n",
    "    return score/div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am good.\n",
      "I need to be better.\n",
      "Are you good?\n",
      "You are awesome!\n",
      "Let's stop here.\n"
     ]
    }
   ],
   "source": [
    "sen = \"I am good. I need to be better. Are you good? You are awesome! Let's stop here.\"\n",
    "tk = nlp(sen)\n",
    "for s in tk.sents:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aeg_long_mod = aeg_long[['essay_id','essay_set','essay','domain1_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration : 0\n",
      "Duration:  0:00:00.001225\n",
      "At iteration : 1000\n",
      "Duration:  0:02:26.681574\n",
      "At iteration : 2000\n",
      "Duration:  0:05:59.845798\n",
      "At iteration : 3000\n",
      "Duration:  0:11:11.224126\n",
      "At iteration : 4000\n",
      "Duration:  0:15:46.331055\n",
      "At iteration : 5000\n",
      "Duration:  0:17:50.410508\n",
      "At iteration : 6000\n",
      "Duration:  0:19:45.485454\n",
      "At iteration : 7000\n",
      "Duration:  0:21:39.620808\n",
      "At iteration : 8000\n",
      "Duration:  0:24:15.281349\n",
      "At iteration : 9000\n",
      "Duration:  0:27:08.179534\n",
      "At iteration : 10000\n",
      "Duration:  0:30:36.951509\n",
      "At iteration : 11000\n",
      "Duration:  0:35:13.829451\n",
      "At iteration : 12000\n",
      "Duration:  0:42:56.885765\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start=datetime.now()\n",
    "aeg_long_sentence = pd.DataFrame(columns=['essay_id','essay_set','sentence','domain1_score'])\n",
    "for i in range(len(aeg_long_mod)):\n",
    "    if i%1000 == 0:\n",
    "        print(\"At iteration :\",i)\n",
    "        print(\"Duration: \",datetime.now()-start)\n",
    "    sentence = nlp(aeg_long_mod.essay[i])\n",
    "    for s in sentence.sents:\n",
    "        aeg_long_sentence = aeg_long_sentence.append({'essay_id' : aeg_long_mod.essay_id[i],\n",
    "                                                      'essay_set' : aeg_long_mod.essay_set[i],'sentence' : s, \n",
    "                                                      'domain1_score' : aeg_long_mod.domain1_score[i]},\n",
    "                                                     ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>sentence</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Dear, local, newspaper, ,, I, think, effects,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Thing, about, !)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Do, nt, you, think, so, ?)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(How, would, you, feel, if, your, teenager, is...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Do, you, ever, time, to, chat, with, your, fr...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Well, now, -, there, 's, a, new, way, to, cha...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Just, think, now, while, your, setting, up, m...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(How, did, you, learn, about, other, countrys,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Well, I, have, by, computer, /, internet)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(,, it, 's, a, new, way, to, learn, about, wha...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(You, might, think, your, child, spends, a, lo...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Believe, it, or, not, the, computer, is, much...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(If, your, child, is, home, on, your, computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(You, might, not, know, where, your, child, is...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Rather, than, your, child, on, the, computer,...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Now, I, hope, you, have, reached, a, point, t...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Thank, you, for, listening, .)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Dear, @CAPS1, @CAPS2, ,, I, believe, that, us...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Using, computers, can, help, us, find, coordi...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Also, computers, will, benefit, us, by, helpi...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Now, lets, go, into, the, wonder, world, of, ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Using, a, computer, will, help, us, in, life,...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Many, people, have, myspace, ,, facebooks, ,,...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Many, people, believe, computers, are, bad, b...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(I, am, very, fortunate, for, having, a, compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Computers, help, us, with, finding, our, loca...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(If, we, did, n't, go, on, the, internet, a, l...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Would, you, rather, use, a, computer, or, be,...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(When, your, supposed, to, be, vacationing, in...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Million, of, information, is, found, on, the,...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id essay_set                                           sentence  \\\n",
       "0         1         1  (Dear, local, newspaper, ,, I, think, effects,...   \n",
       "1         1         1                                  (Thing, about, !)   \n",
       "2         1         1                        (Do, nt, you, think, so, ?)   \n",
       "3         1         1  (How, would, you, feel, if, your, teenager, is...   \n",
       "4         1         1  (Do, you, ever, time, to, chat, with, your, fr...   \n",
       "5         1         1  (Well, now, -, there, 's, a, new, way, to, cha...   \n",
       "6         1         1  (Just, think, now, while, your, setting, up, m...   \n",
       "7         1         1  (How, did, you, learn, about, other, countrys,...   \n",
       "8         1         1         (Well, I, have, by, computer, /, internet)   \n",
       "9         1         1  (,, it, 's, a, new, way, to, learn, about, wha...   \n",
       "10        1         1  (You, might, think, your, child, spends, a, lo...   \n",
       "11        1         1  (Believe, it, or, not, the, computer, is, much...   \n",
       "12        1         1  (If, your, child, is, home, on, your, computer...   \n",
       "13        1         1  (You, might, not, know, where, your, child, is...   \n",
       "14        1         1  (Rather, than, your, child, on, the, computer,...   \n",
       "15        1         1  (Now, I, hope, you, have, reached, a, point, t...   \n",
       "16        1         1                    (Thank, you, for, listening, .)   \n",
       "17        2         1  (Dear, @CAPS1, @CAPS2, ,, I, believe, that, us...   \n",
       "18        2         1  (Using, computers, can, help, us, find, coordi...   \n",
       "19        2         1  (Also, computers, will, benefit, us, by, helpi...   \n",
       "20        2         1  (Now, lets, go, into, the, wonder, world, of, ...   \n",
       "21        2         1  (Using, a, computer, will, help, us, in, life,...   \n",
       "22        2         1  (Many, people, have, myspace, ,, facebooks, ,,...   \n",
       "23        2         1  (Many, people, believe, computers, are, bad, b...   \n",
       "24        2         1  (I, am, very, fortunate, for, having, a, compu...   \n",
       "25        2         1  (Computers, help, us, with, finding, our, loca...   \n",
       "26        2         1  (If, we, did, n't, go, on, the, internet, a, l...   \n",
       "27        2         1  (Would, you, rather, use, a, computer, or, be,...   \n",
       "28        2         1  (When, your, supposed, to, be, vacationing, in...   \n",
       "29        2         1  (Million, of, information, is, found, on, the,...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              8  \n",
       "2              8  \n",
       "3              8  \n",
       "4              8  \n",
       "5              8  \n",
       "6              8  \n",
       "7              8  \n",
       "8              8  \n",
       "9              8  \n",
       "10             8  \n",
       "11             8  \n",
       "12             8  \n",
       "13             8  \n",
       "14             8  \n",
       "15             8  \n",
       "16             8  \n",
       "17             9  \n",
       "18             9  \n",
       "19             9  \n",
       "20             9  \n",
       "21             9  \n",
       "22             9  \n",
       "23             9  \n",
       "24             9  \n",
       "25             9  \n",
       "26             9  \n",
       "27             9  \n",
       "28             9  \n",
       "29             9  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aeg_long_sentence.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeg_long_sentence['Norm_Score'] = aeg_long_sentence.apply(get_score,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spacy_to_text(essay):\n",
    "    return essay[2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aeg_long_sentence['Mod_Sentence'] = aeg_long_sentence.apply(spacy_to_text,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>sentence</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>Norm_Score</th>\n",
       "      <th>Mod_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Dear, local, newspaper, ,, I, think, effects,...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Thing, about, !)</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Thing about!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Do, nt, you, think, so, ?)</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Dont you think so?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(How, would, you, feel, if, your, teenager, is...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>How would you feel if your teenager is always ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Do, you, ever, time, to, chat, with, your, fr...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Do you ever time to chat with your friends or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Well, now, -, there, 's, a, new, way, to, cha...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Well now - there's a new way to chat the compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Just, think, now, while, your, setting, up, m...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Just think now while your setting up meeting w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(How, did, you, learn, about, other, countrys,...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>How did you learn about other countrys/states ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Well, I, have, by, computer, /, internet)</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Well I have by computer/internet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(,, it, 's, a, new, way, to, learn, about, wha...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>, it's a new way to learn about what going on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(You, might, think, your, child, spends, a, lo...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>You might think your child spends a lot of tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Believe, it, or, not, the, computer, is, much...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Believe it or not the computer is much interes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(If, your, child, is, home, on, your, computer...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>If your child is home on your computer or at a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(You, might, not, know, where, your, child, is...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>You might not know where your child is, @CAPS2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Rather, than, your, child, on, the, computer,...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Rather than your child on the computer learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Now, I, hope, you, have, reached, a, point, t...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Now I hope you have reached a point to underst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>(Thank, you, for, listening, .)</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Thank you for listening.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Dear, @CAPS1, @CAPS2, ,, I, believe, that, us...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Using, computers, can, help, us, find, coordi...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Using computers can help us find coordibates, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Also, computers, will, benefit, us, by, helpi...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Also computers will benefit us by helping with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Now, lets, go, into, the, wonder, world, of, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Now lets go into the wonder world of technology.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Using, a, computer, will, help, us, in, life,...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Using a computer will help us in life by talki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Many, people, have, myspace, ,, facebooks, ,,...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Many people have myspace, facebooks, aim, thes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Many, people, believe, computers, are, bad, b...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Many people believe computers are bad but how ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(I, am, very, fortunate, for, having, a, compu...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>I am very fortunate for having a computer that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Computers, help, us, with, finding, our, loca...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Computers help us with finding our locations, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(If, we, did, n't, go, on, the, internet, a, l...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>If we didn't go on the internet a lot we would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Would, you, rather, use, a, computer, or, be,...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Would you rather use a computer or be in @LOCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(When, your, supposed, to, be, vacationing, in...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>When your supposed to be vacationing in @LOCAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>(Million, of, information, is, found, on, the,...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Million of information is found on the internet.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id essay_set                                           sentence  \\\n",
       "0         1         1  (Dear, local, newspaper, ,, I, think, effects,...   \n",
       "1         1         1                                  (Thing, about, !)   \n",
       "2         1         1                        (Do, nt, you, think, so, ?)   \n",
       "3         1         1  (How, would, you, feel, if, your, teenager, is...   \n",
       "4         1         1  (Do, you, ever, time, to, chat, with, your, fr...   \n",
       "5         1         1  (Well, now, -, there, 's, a, new, way, to, cha...   \n",
       "6         1         1  (Just, think, now, while, your, setting, up, m...   \n",
       "7         1         1  (How, did, you, learn, about, other, countrys,...   \n",
       "8         1         1         (Well, I, have, by, computer, /, internet)   \n",
       "9         1         1  (,, it, 's, a, new, way, to, learn, about, wha...   \n",
       "10        1         1  (You, might, think, your, child, spends, a, lo...   \n",
       "11        1         1  (Believe, it, or, not, the, computer, is, much...   \n",
       "12        1         1  (If, your, child, is, home, on, your, computer...   \n",
       "13        1         1  (You, might, not, know, where, your, child, is...   \n",
       "14        1         1  (Rather, than, your, child, on, the, computer,...   \n",
       "15        1         1  (Now, I, hope, you, have, reached, a, point, t...   \n",
       "16        1         1                    (Thank, you, for, listening, .)   \n",
       "17        2         1  (Dear, @CAPS1, @CAPS2, ,, I, believe, that, us...   \n",
       "18        2         1  (Using, computers, can, help, us, find, coordi...   \n",
       "19        2         1  (Also, computers, will, benefit, us, by, helpi...   \n",
       "20        2         1  (Now, lets, go, into, the, wonder, world, of, ...   \n",
       "21        2         1  (Using, a, computer, will, help, us, in, life,...   \n",
       "22        2         1  (Many, people, have, myspace, ,, facebooks, ,,...   \n",
       "23        2         1  (Many, people, believe, computers, are, bad, b...   \n",
       "24        2         1  (I, am, very, fortunate, for, having, a, compu...   \n",
       "25        2         1  (Computers, help, us, with, finding, our, loca...   \n",
       "26        2         1  (If, we, did, n't, go, on, the, internet, a, l...   \n",
       "27        2         1  (Would, you, rather, use, a, computer, or, be,...   \n",
       "28        2         1  (When, your, supposed, to, be, vacationing, in...   \n",
       "29        2         1  (Million, of, information, is, found, on, the,...   \n",
       "\n",
       "   domain1_score  Norm_Score  \\\n",
       "0              8    0.666667   \n",
       "1              8    0.666667   \n",
       "2              8    0.666667   \n",
       "3              8    0.666667   \n",
       "4              8    0.666667   \n",
       "5              8    0.666667   \n",
       "6              8    0.666667   \n",
       "7              8    0.666667   \n",
       "8              8    0.666667   \n",
       "9              8    0.666667   \n",
       "10             8    0.666667   \n",
       "11             8    0.666667   \n",
       "12             8    0.666667   \n",
       "13             8    0.666667   \n",
       "14             8    0.666667   \n",
       "15             8    0.666667   \n",
       "16             8    0.666667   \n",
       "17             9    0.750000   \n",
       "18             9    0.750000   \n",
       "19             9    0.750000   \n",
       "20             9    0.750000   \n",
       "21             9    0.750000   \n",
       "22             9    0.750000   \n",
       "23             9    0.750000   \n",
       "24             9    0.750000   \n",
       "25             9    0.750000   \n",
       "26             9    0.750000   \n",
       "27             9    0.750000   \n",
       "28             9    0.750000   \n",
       "29             9    0.750000   \n",
       "\n",
       "                                         Mod_Sentence  \n",
       "0   Dear local newspaper, I think effects computer...  \n",
       "1                                        Thing about!  \n",
       "2                                  Dont you think so?  \n",
       "3   How would you feel if your teenager is always ...  \n",
       "4   Do you ever time to chat with your friends or ...  \n",
       "5   Well now - there's a new way to chat the compu...  \n",
       "6   Just think now while your setting up meeting w...  \n",
       "7   How did you learn about other countrys/states ...  \n",
       "8                    Well I have by computer/internet  \n",
       "9   , it's a new way to learn about what going on ...  \n",
       "10  You might think your child spends a lot of tim...  \n",
       "11  Believe it or not the computer is much interes...  \n",
       "12  If your child is home on your computer or at a...  \n",
       "13  You might not know where your child is, @CAPS2...  \n",
       "14  Rather than your child on the computer learnin...  \n",
       "15  Now I hope you have reached a point to underst...  \n",
       "16                           Thank you for listening.  \n",
       "17  Dear @CAPS1 @CAPS2, I believe that using compu...  \n",
       "18  Using computers can help us find coordibates, ...  \n",
       "19  Also computers will benefit us by helping with...  \n",
       "20   Now lets go into the wonder world of technology.  \n",
       "21  Using a computer will help us in life by talki...  \n",
       "22  Many people have myspace, facebooks, aim, thes...  \n",
       "23  Many people believe computers are bad but how ...  \n",
       "24  I am very fortunate for having a computer that...  \n",
       "25  Computers help us with finding our locations, ...  \n",
       "26  If we didn't go on the internet a lot we would...  \n",
       "27  Would you rather use a computer or be in @LOCA...  \n",
       "28  When your supposed to be vacationing in @LOCAT...  \n",
       "29   Million of information is found on the internet.  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aeg_long_sentence.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_sentence,test_x_sentence,train_y_sentence,test_y_sentence = train_test_split(np.asarray(aeg_long_sentence.Mod_Sentence), np.asarray(aeg_long_sentence.Norm_Score),test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x_sentence,test_x_sentence,train_y_orig_score,test_y_orig_score,train_y_sentence,test_y_sentence = train_test_split(np.asarray(aeg_long_sentence.Mod_Sentence),np.asarray(aeg_long_sentence.domain1_score), np.asarray(aeg_long_sentence.Norm_Score),test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147120,)\n",
      "(36781,)\n",
      "[46 3 4 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_y_orig_score.shape)\n",
    "print(test_y_orig_score.shape)\n",
    "print(train_y_orig_score[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(train_x_sentence)\n",
    "train_seq = tokenizer.texts_to_sequences(train_x_sentence)\n",
    "train_data = pad_sequences(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len_class = train_data.shape[1]\n",
    "max_words_class = vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36781, 175)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_x_sentence)\n",
    "test_data = pad_sequences(test_seq, maxlen=max_len_class)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183901, 6)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aeg_long_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('/Users/kurapati/W266/data/glove.42B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSE(actual, predict):\n",
    "    diff = actual - predict\n",
    "    diff = sum(diff**2) / len(actual)\n",
    "    return np.sqrt(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cohen Kappa score as defined by the kaggle challenge/wikipedia\n",
    "def CohenKappa(actual, predict):\n",
    "    CohenDF = pd.DataFrame([actual.astype(np.double).round(), np.around(predict.astype(np.double))]).transpose()\n",
    "    count = len(CohenDF)\n",
    "    CohenDF.columns = ['actual','predict']\n",
    "    correct = len(CohenDF[CohenDF.actual==CohenDF.predict])\n",
    "    acc = correct / count\n",
    "    pe = 0\n",
    "    for value in CohenDF.actual.unique():\n",
    "        pe += len(CohenDF[CohenDF.actual == value]) * len(CohenDF[CohenDF.predict == value])\n",
    "    pe = pe / np.square(count)\n",
    "    return(1 - (1-acc)/(1-pe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skll.metrics as metric\n",
    "\n",
    "# Calculates the average quadratic kappa for the entire essay set\n",
    "def get_average_kappa(targets, predictions):\n",
    "        num = len(targets)\n",
    "        total_kappa = 0\n",
    "\n",
    "        for i in range(0, num):\n",
    "                total_kappa += metric.kappa([targets], [predictions], 'quadratic')\n",
    "\n",
    "        avg_kappa  = float(total_kappa) / float(num)\n",
    "        return avg_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_all = TfidfVectorizer()\n",
    "tfidf_all.fit(train_x_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = tfidf_all.transform(train_x_sentence)\n",
    "test_transform = tfidf_all.transform(test_x_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "lr = LinearRegression(fit_intercept=True)\n",
    "rf = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(train_transform.toarray(), train_y_sentence)\n",
    "lr.fit(train_transform.toarray(), train_y_sentence)\n",
    "rf.fit(train_transform.toarray(), train_y_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147120, 34007)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_transform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_class = train_data.shape[1]\n",
    "max_words_class = vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom metrics\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "#model definitions\n",
    "def CNN():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class,300,input_length=max_len_class))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(tf.keras.layers.LSTM(100))\n",
    "    model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model_conv.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "def CNN_glove_1():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(tf.keras.layers.LSTM(100))\n",
    "    model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    #sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    #model_conv.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "def stack_lstm():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32, return_sequences=True))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32, return_sequences=True))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    #sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    #model_conv.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_conv\n",
    "def CNN_glove():\n",
    "    conv_glove = Sequential()\n",
    "    conv_glove.add(Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    conv_glove.add(Conv1D(64, 5, activation = 'relu'))\n",
    "    conv_glove.add(MaxPooling1D(4))\n",
    "    conv_glove.add(LSTM(100))\n",
    "    conv_glove.add(Dense(100))\n",
    "    conv_glove.add(Dense(1, kernel_initializer='normal'))\n",
    "    sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "    conv_glove.compile(loss = 'mse', optimizer = sgd, metrics = ['accuracy'])\n",
    "    return conv_glove\n",
    "\n",
    "def CNN_no_dense():\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    #model_conv.add(tf.keras.layers.Embedding(max_words_class,300,input_length=max_len_class))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(tf.keras.layers.LSTM(100))\n",
    "    #model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model_conv.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "#bidirectional RNN is screwed up atm\n",
    "def Bidirectional_RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len_class])\n",
    "    layer = Embedding(max_words,200,input_length=max_len_class)(inputs)\n",
    "    layer = Bidirectional(LSTM(10))(layer)\n",
    "    layer = Dense(32,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model\n",
    "\n",
    "def feedforward_NN():\n",
    "    model_ff = Sequential()\n",
    "    model_ff.add(tf.keras.layers.Embedding(max_words_class,200,input_length=max_len_class))\n",
    "    model_ff.add(tf.keras.layers.Flatten())\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "    model_ff.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_ff\n",
    "\n",
    "def RNN():\n",
    "    model_rnn = Sequential()\n",
    "    model_rnn.add(tf.keras.layers.Embedding(max_words_class,200,input_length=max_len_class))\n",
    "    model_rnn.add(tf.keras.layers.LSTM(10))\n",
    "    model_rnn.add(tf.keras.layers.Dense(1,name='out_layer'))\n",
    "    model_rnn.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 175, 200)          10000000  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 35000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                1750050   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 11,750,101\n",
      "Trainable params: 11,750,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feedforward = feedforward_NN()\n",
    "feedforward.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 175, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 175, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 171, 64)           96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 42, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,172,265\n",
      "Trainable params: 172,265\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNN_glove_model = CNN_glove_1()\n",
    "CNN_glove_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=CNN_glove_1, epochs=10, batch_size=50)\n",
    "#kfold = KFold(n_splits=5, random_state=43)\n",
    "#results = np.sqrt(-1*cross_val_score(estimator, train_data, train_y_sentence,scoring= \"neg_mean_squared_error\", cv=kfold))\n",
    "#print(\"Training RMSE mean and std from CV: {} {}\".format(results.mean(),results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "147120/147120 [==============================] - 309s 2ms/step - loss: 0.0594 - acc: 0.0978\n",
      "Epoch 2/10\n",
      "147120/147120 [==============================] - 303s 2ms/step - loss: 0.0301 - acc: 0.0991\n",
      "Epoch 3/10\n",
      "147120/147120 [==============================] - 282s 2ms/step - loss: 0.0302 - acc: 0.0993\n",
      "Epoch 4/10\n",
      "147120/147120 [==============================] - 287s 2ms/step - loss: 0.0296 - acc: 0.0994\n",
      "Epoch 5/10\n",
      "147120/147120 [==============================] - 313s 2ms/step - loss: 0.0287 - acc: 0.0999\n",
      "Epoch 6/10\n",
      "147120/147120 [==============================] - 319s 2ms/step - loss: 0.0280 - acc: 0.1003\n",
      "Epoch 7/10\n",
      "147120/147120 [==============================] - 304s 2ms/step - loss: 0.0275 - acc: 0.1005\n",
      "Epoch 8/10\n",
      "147120/147120 [==============================] - 319s 2ms/step - loss: 0.0268 - acc: 0.1008\n",
      "Epoch 9/10\n",
      "147120/147120 [==============================] - 321s 2ms/step - loss: 0.0262 - acc: 0.1010\n",
      "Epoch 10/10\n",
      "147120/147120 [==============================] - 317s 2ms/step - loss: 0.0256 - acc: 0.1014\n",
      "RMSE:  0.1708167228943508\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(train_data, train_y_sentence)\n",
    "prediction_cnn_glove=estimator.predict(test_data)\n",
    "rmse_val = RMSE(test_y_sentence,prediction_cnn_glove)\n",
    "print(\"RMSE: \",rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36781,)\n",
      "(36781,)\n"
     ]
    }
   ],
   "source": [
    "print(prediction_cnn_glove.shape)\n",
    "print(test_y_orig_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Orig_Score</th>\n",
       "      <th>Norm_Score</th>\n",
       "      <th>Pred_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.728527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.757053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.711505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.882032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.773958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.815720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43.0</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.747872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.672615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Orig_Score  Norm_Score  Pred_Score\n",
       "0         5.0    1.000000    0.769634\n",
       "1         0.0    0.000000    0.474710\n",
       "2         8.0    0.666667    0.728527\n",
       "3        13.0    0.520000    0.757053\n",
       "4        11.0    0.916667    0.711505\n",
       "5         0.0    0.000000    0.882032\n",
       "6        21.0    0.840000    0.773958\n",
       "7         8.0    0.666667    0.815720\n",
       "8        43.0    0.860000    0.747872\n",
       "9         6.0    0.500000    0.672615"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_mult_factor(x):\n",
    "    if x[1] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.around(x[0]/x[1])\n",
    "cnn_df = pd.DataFrame([test_y_orig_score.astype(np.double), test_y_sentence.astype(np.double),\n",
    "                      prediction_cnn_glove.astype(np.double)]).transpose()\n",
    "cnn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "cnn_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Orig_Score</th>\n",
       "      <th>Norm_Score</th>\n",
       "      <th>Pred_Score</th>\n",
       "      <th>Mult_Factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769634</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474710</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.728527</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.757053</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.711505</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.882032</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.773958</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.815720</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43.0</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.747872</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.672615</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Orig_Score  Norm_Score  Pred_Score  Mult_Factor\n",
       "0         5.0    1.000000    0.769634          5.0\n",
       "1         0.0    0.000000    0.474710          0.0\n",
       "2         8.0    0.666667    0.728527         12.0\n",
       "3        13.0    0.520000    0.757053         25.0\n",
       "4        11.0    0.916667    0.711505         12.0\n",
       "5         0.0    0.000000    0.882032          0.0\n",
       "6        21.0    0.840000    0.773958         25.0\n",
       "7         8.0    0.666667    0.815720         12.0\n",
       "8        43.0    0.860000    0.747872         50.0\n",
       "9         6.0    0.500000    0.672615         12.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_df['Mult_Factor'] = cnn_df.apply(find_mult_factor,axis=1)\n",
    "cnn_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Orig_Score</th>\n",
       "      <th>Norm_Score</th>\n",
       "      <th>Pred_Score</th>\n",
       "      <th>Mult_Factor</th>\n",
       "      <th>Denorm_Pred_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.769634</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.474710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.728527</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.757053</td>\n",
       "      <td>25.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.711505</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.882032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.773958</td>\n",
       "      <td>25.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.815720</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43.0</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.747872</td>\n",
       "      <td>50.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.672615</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Orig_Score  Norm_Score  Pred_Score  Mult_Factor  Denorm_Pred_Score\n",
       "0         5.0    1.000000    0.769634          5.0                4.0\n",
       "1         0.0    0.000000    0.474710          0.0                0.0\n",
       "2         8.0    0.666667    0.728527         12.0                9.0\n",
       "3        13.0    0.520000    0.757053         25.0               19.0\n",
       "4        11.0    0.916667    0.711505         12.0                9.0\n",
       "5         0.0    0.000000    0.882032          0.0                0.0\n",
       "6        21.0    0.840000    0.773958         25.0               19.0\n",
       "7         8.0    0.666667    0.815720         12.0               10.0\n",
       "8        43.0    0.860000    0.747872         50.0               37.0\n",
       "9         6.0    0.500000    0.672615         12.0                8.0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def denormalize(x):\n",
    "    return np.around(x[2] * x[3])\n",
    "cnn_df['Denorm_Pred_Score'] = cnn_df.apply(denormalize,axis=1)\n",
    "cnn_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  2.916820824744241\n"
     ]
    }
   ],
   "source": [
    "orig_score = cnn_df.Orig_Score.as_matrix(columns=None)\n",
    "pred_score = cnn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "rmse_cnn = RMSE(orig_score,pred_score)\n",
    "print(\"RMSE: \",rmse_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen Kappa:  0.25738861667132784\n"
     ]
    }
   ],
   "source": [
    "c_kappa = CohenKappa(orig_score,pred_score)\n",
    "print(\"Cohen Kappa: \",c_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.3219868954079552\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.16944170874087938\n"
     ]
    }
   ],
   "source": [
    "# Old.. Ignore\n",
    "prediction=estimator.predict(test_data)\n",
    "rmse_val = RMSE(test_y_sentence,prediction)\n",
    "print(\"RMSE: \",rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147120/147120 [==============================] - 409s 3ms/step - loss: 0.0301 - acc: 0.0991\n",
      "Epoch 2/20\n",
      "147120/147120 [==============================] - 393s 3ms/step - loss: 0.0281 - acc: 0.0999\n",
      "Epoch 3/20\n",
      "147120/147120 [==============================] - 383s 3ms/step - loss: 0.0272 - acc: 0.1004\n",
      "Epoch 4/20\n",
      "147120/147120 [==============================] - 379s 3ms/step - loss: 0.0264 - acc: 0.1009\n",
      "Epoch 5/20\n",
      "102000/147120 [===================>..........] - ETA: 1:57 - loss: 0.0256 - acc: 0.1012"
     ]
    }
   ],
   "source": [
    "estimator_lstm = KerasRegressor(build_fn=stack_lstm, epochs=20, batch_size=50)\n",
    "estimator_lstm.fit(train_data, train_y_sentence)\n",
    "prediction_lstm=estimator_lstm.predict(test_data)\n",
    "rmse_val_lstm = RMSE(test_y_sentence,prediction_lstm)\n",
    "print(\"RMSE: \",rmse_val_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CNN_Glove</th>\n",
       "      <th>Stacked LSTM</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.692931</td>\n",
       "      <td>0.723046</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.702375</td>\n",
       "      <td>0.678391</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.798613</td>\n",
       "      <td>0.734653</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.703292</td>\n",
       "      <td>0.677914</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.671944</td>\n",
       "      <td>0.669563</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.747734</td>\n",
       "      <td>0.820812</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.677340</td>\n",
       "      <td>0.650709</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.616736</td>\n",
       "      <td>0.506052</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.732711</td>\n",
       "      <td>0.706890</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.828496</td>\n",
       "      <td>0.792971</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CNN_Glove  Stacked LSTM  Actual\n",
       "0   0.692931      0.723046    0.60\n",
       "1   0.702375      0.678391    0.60\n",
       "2   0.798613      0.734653    0.78\n",
       "3   0.703292      0.677914    0.80\n",
       "4   0.671944      0.669563    0.52\n",
       "5   0.747734      0.820812    0.50\n",
       "6   0.677340      0.650709    0.75\n",
       "7   0.616736      0.506052    0.75\n",
       "8   0.732711      0.706890    0.75\n",
       "9   0.828496      0.792971    1.00"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_df = pd.DataFrame([prediction.astype(np.double), prediction_lstm.astype(np.double), test_y_sentence.astype(np.double)]).transpose()\n",
    "cnn_df.columns = ['CNN_Glove','Stacked LSTM','Actual']\n",
    "cnn_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen Kappa:  0.15593670766702683\n"
     ]
    }
   ],
   "source": [
    "c_kappa = CohenKappa(test_y_sentence,cnn_glove_predict)\n",
    "print(\"Cohen Kappa: \",c_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen Kappa LSTM:  0.14598100325900654\n"
     ]
    }
   ],
   "source": [
    "c_kappa = CohenKappa(test_y_sentence,prediction_lstm)\n",
    "print(\"Cohen Kappa LSTM: \",c_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 125052 samples, validate on 22068 samples\n",
      "Epoch 1/10\n",
      "125052/125052 [==============================] - 260s 2ms/step - loss: 0.0292 - acc: 0.0982 - val_loss: 0.0283 - val_acc: 0.1032\n",
      "Epoch 2/10\n",
      "125052/125052 [==============================] - 273s 2ms/step - loss: 0.0275 - acc: 0.0989 - val_loss: 0.0282 - val_acc: 0.1037\n",
      "Epoch 3/10\n",
      "125052/125052 [==============================] - 262s 2ms/step - loss: 0.0265 - acc: 0.0994 - val_loss: 0.0279 - val_acc: 0.1035\n",
      "Epoch 4/10\n",
      "125052/125052 [==============================] - 269s 2ms/step - loss: 0.0256 - acc: 0.1003 - val_loss: 0.0284 - val_acc: 0.1029\n",
      "Epoch 5/10\n",
      "125052/125052 [==============================] - 278s 2ms/step - loss: 0.0247 - acc: 0.1006 - val_loss: 0.0288 - val_acc: 0.1034\n",
      "Epoch 6/10\n",
      "125052/125052 [==============================] - 275s 2ms/step - loss: 0.0239 - acc: 0.1012 - val_loss: 0.0284 - val_acc: 0.1034\n",
      "Epoch 7/10\n",
      "125052/125052 [==============================] - 260s 2ms/step - loss: 0.0231 - acc: 0.1016 - val_loss: 0.0296 - val_acc: 0.1029\n",
      "Epoch 8/10\n",
      "  8700/125052 [=>............................] - ETA: 4:37 - loss: 0.0222 - acc: 0.1002"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-973aaf28ffea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCNN_glove_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2915\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "CNN_glove_model.fit(train_data,train_y_sentence, batch_size = 50, epochs=10, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36781, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_glove_predict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36781,)\n",
      "(36781,)\n"
     ]
    }
   ],
   "source": [
    "cnn_glove_predict = np.reshape(cnn_glove_predict,[cnn_glove_predict.shape[0],])\n",
    "print(cnn_glove_predict.shape)\n",
    "print(test_y_sentence.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36781,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(cnn_glove_predict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36781"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CohenDF = pd.DataFrame([cnn_glove_predict.astype(np.double).round(), np.around(test_y_sentence.astype(np.double))]).transpose()\n",
    "CohenDF.columns = ['actual','predict']\n",
    "len(CohenDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8770289007911694\n"
     ]
    }
   ],
   "source": [
    "count = len(CohenDF)\n",
    "CohenDF.columns = ['actual','predict']\n",
    "correct = len(CohenDF[CohenDF.actual==CohenDF.predict])\n",
    "acc = correct / count\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8770289007911694\n"
     ]
    }
   ],
   "source": [
    "# If not rounded\n",
    "CohenDF = pd.DataFrame([cnn_glove_predict.astype(np.double).round(), np.around(test_y_sentence.astype(np.double))]).transpose()\n",
    "CohenDF.columns = ['actual','predict']\n",
    "count = len(CohenDF)\n",
    "CohenDF.columns = ['actual','predict']\n",
    "correct = len(CohenDF[CohenDF.actual==CohenDF.predict])\n",
    "acc = correct / count\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([ 1138, 35643]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(CohenDF.actual,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actual  predict\n",
       "0      1.0      1.0\n",
       "1      1.0      1.0\n",
       "2      1.0      1.0\n",
       "3      1.0      1.0\n",
       "4      1.0      1.0\n",
       "5      1.0      0.0\n",
       "6      1.0      1.0\n",
       "7      1.0      1.0\n",
       "8      1.0      1.0\n",
       "9      1.0      1.0\n",
       "10     1.0      1.0\n",
       "11     1.0      1.0\n",
       "12     1.0      1.0\n",
       "13     1.0      1.0\n",
       "14     1.0      1.0\n",
       "15     1.0      1.0\n",
       "16     1.0      1.0\n",
       "17     1.0      1.0\n",
       "18     1.0      1.0\n",
       "19     1.0      1.0\n",
       "20     1.0      0.0\n",
       "21     0.0      1.0\n",
       "22     1.0      1.0\n",
       "23     1.0      1.0\n",
       "24     1.0      1.0\n",
       "25     1.0      1.0\n",
       "26     1.0      0.0\n",
       "27     1.0      1.0\n",
       "28     1.0      1.0\n",
       "29     1.0      1.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CohenDF.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.17559793893398817\n"
     ]
    }
   ],
   "source": [
    "rmse_val = RMSE(test_y_sentence,cnn_glove_predict)\n",
    "print(\"RMSE: \",rmse_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen Kappa:  0.15593670766702683\n"
     ]
    }
   ],
   "source": [
    "c_kappa = CohenKappa(test_y_sentence,cnn_glove_predict)\n",
    "print(\"Cohen Kappa: \",c_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "continuous is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-34e1b0d2cc0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcohen_kappa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcohen_kappa_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnn_glove_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cohen Kappa: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcohen_kappa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mcohen_kappa_score\u001b[0;34m(y1, y2, labels, weights, sample_weight)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \"\"\"\n\u001b[1;32m    350\u001b[0m     confusion = confusion_matrix(y1, y2, labels=labels,\n\u001b[0;32m--> 351\u001b[0;31m                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    352\u001b[0m     \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0msum0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \"\"\"\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# No metrics support \"multiclass-multioutput\" format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: continuous is not supported"
     ]
    }
   ],
   "source": [
    "cohen_kappa = cohen_kappa_score(test_y_sentence,cnn_glove_predict)\n",
    "print(\"Cohen Kappa: \",cohen_kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AESW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = ET.parse('../data-DNC/AESW/aesw2016_v1.2_train.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_attrib = list()\n",
    "sent_text = list()\n",
    "for sent in root.iter('sentence'):\n",
    "    sent_attrib.append(sent.attrib['sid'])\n",
    "    sent_text.append(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_dict = dict()\n",
    "for sent in root.iter('sentence'):\n",
    "    sid = sent.attrib['sid']\n",
    "    sent_dict[sid]=sent.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1189412"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AESW Data for grammatical errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>To facilitate an easier notation throughout t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>To facilitate an easier notation throughout t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Therefore MATH defines a special order of tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>This is important since only MATH is the real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Note that in all contour time-integrals we es...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1                                                  2\n",
       "0  1  1.0   To facilitate an easier notation throughout t...\n",
       "1 -1  1.0   To facilitate an easier notation throughout t...\n",
       "2  0  1.1   Therefore MATH defines a special order of tim...\n",
       "3  0  1.2   This is important since only MATH is the real...\n",
       "4  0  1.3   Note that in all contour time-integrals we es..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aesw_train_labels = pd.read_csv(\"../data-DNC/AESW/aesw2016_v1.2_train.tok\",sep='\\t',encoding = \"latin1\",header=None)\n",
    "aesw_train_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1196903"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aesw_train_labels.columns = ['Key_ID','Sent_ID','Text']\n",
    "aesw_train_labels = aesw_train_labels[aesw_train_labels.Key_ID!=1]\n",
    "len(aesw_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = aesw_train_labels.Key_ID == -1\n",
    "column_name = 'Key_ID'\n",
    "aesw_train_labels.loc[mask, column_name] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key_ID</th>\n",
       "      <th>Sent_ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>To facilitate an easier notation throughout t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Therefore MATH defines a special order of tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>This is important since only MATH is the real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Note that in all contour time-integrals we es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Theorem REF proves the equivalence of ensembl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Key_ID  Sent_ID                                               Text\n",
       "1       1      1.0   To facilitate an easier notation throughout t...\n",
       "2       0      1.1   Therefore MATH defines a special order of tim...\n",
       "3       0      1.2   This is important since only MATH is the real...\n",
       "4       0      1.3   Note that in all contour time-integrals we es...\n",
       "5       0      2.0   Theorem REF proves the equivalence of ensembl..."
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aesw_train_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = aesw_train_labels.Text.as_matrix(columns=None)\n",
    "y_train = aesw_train_labels.Key_ID.as_matrix(columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The team of robots in Chapter REF and CITE cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>The Hamiltonian -LRB- Lyapunov function -RRB-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>The Hamiltonian -LRB- Lyapunov function -RRB-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Therefore , for MATH robots , the time deriva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Therefore , for MATH robots the time derivati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1                                                  2\n",
       "0  0  1.0   The team of robots in Chapter REF and CITE cr...\n",
       "1  1  1.1   The Hamiltonian -LRB- Lyapunov function -RRB-...\n",
       "2 -1  1.1   The Hamiltonian -LRB- Lyapunov function -RRB-...\n",
       "3  1  1.2   Therefore , for MATH robots , the time deriva...\n",
       "4 -1  1.2   Therefore , for MATH robots the time derivati..."
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aesw_dev_labels = pd.read_csv(\"../data-DNC/AESW/aesw2016_v1.2_dev.tok\",sep='\\t',encoding = \"latin1\",header=None)\n",
    "aesw_dev_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148409"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aesw_dev_labels.columns = ['Key_ID','Sent_ID','Text']\n",
    "aesw_dev_labels = aesw_dev_labels[aesw_dev_labels.Key_ID!=1]\n",
    "len(aesw_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = aesw_dev_labels.Key_ID == -1\n",
    "column_name = 'Key_ID'\n",
    "aesw_dev_labels.loc[mask, column_name] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_dev = aesw_dev_labels.Text.as_matrix(columns=None)\n",
    "y_dev = aesw_dev_labels.Key_ID.as_matrix(columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "train_data = pad_sequences(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = train_data.shape[1]\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148409, 271)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_seq = tokenizer.texts_to_sequences(x_dev)\n",
    "dev_data = pad_sequences(dev_seq, maxlen=max_seq_len)\n",
    "dev_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b2d85bdd8>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed(2017)\n",
    "conv = Sequential()\n",
    "conv.add(Embedding(vocabulary_size, 100, input_length=max_seq_len))\n",
    "conv.add(Conv1D(64, 5, activation = 'relu'))\n",
    "conv.add(MaxPooling1D(2))\n",
    "conv.add(Flatten())\n",
    "conv.add(Dense(1, activation = 'sigmoid'))\n",
    "sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "conv.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "conv.fit(train_data, y_train, batch_size = 500, epochs = 8, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.607146466858479\n"
     ]
    }
   ],
   "source": [
    "pred = conv.predict(dev_data)\n",
    "pred_labels = (pred > 0.5).astype(np.int)\n",
    "pred_labels = np.reshape(pred_labels,[dev_data.shape[0],])\n",
    "\n",
    "#print(y_dev[0:5])\n",
    "#print(pred_labels[0:5])\n",
    "accuracy = accuracy_score(y_dev,pred_labels)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148409,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(y_dev))\n",
    "print(type(y_dev))\n",
    "print(type(pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('/Users/kurapati/W266/data/glove.42B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Glove word embeddings instead of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa0d81c50>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed(2018)\n",
    "conv_glove = Sequential()\n",
    "conv_glove.add(Embedding(vocabulary_size, 300, input_length=50, weights=[embedding_matrix], trainable=False))\n",
    "conv_glove.add(Conv1D(64, 5, activation = 'relu'))\n",
    "conv_glove.add(MaxPooling1D(4))\n",
    "conv_glove.add(Flatten())\n",
    "conv_glove.add(Dense(1, activation = 'sigmoid'))\n",
    "sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "conv_glove.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "conv_glove.fit(data, y_train, batch_size = 500, epochs = 8, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6500212251278561\n"
     ]
    }
   ],
   "source": [
    "pred = conv_glove.predict(dev_data)\n",
    "pred_labels = (pred > 0.5).astype(np.int)\n",
    "pred_labels = np.reshape(pred_labels,[148409,])\n",
    "\n",
    "#print(y_dev[0:5])\n",
    "#print(pred_labels[0:5])\n",
    "accuracy = accuracy_score(y_dev,pred_labels)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa029c550>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With CNN and LSTM\n",
    "seed(2018)\n",
    "conv_glove = Sequential()\n",
    "conv_glove.add(Embedding(vocabulary_size, 300, input_length=50, weights=[embedding_matrix], trainable=False))\n",
    "conv_glove.add(Conv1D(64, 5, activation = 'relu'))\n",
    "conv_glove.add(MaxPooling1D(4))\n",
    "conv_glove.add(LSTM(100))\n",
    "conv_glove.add(Dense(1, activation = 'sigmoid'))\n",
    "sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "conv_glove.compile(loss = 'binary_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
    "conv_glove.fit(data, y_train, batch_size = 500, epochs = 8, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_glove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d7bcc207bb0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_glove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m148409\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(y_dev[0:5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_glove' is not defined"
     ]
    }
   ],
   "source": [
    "pred = conv_glove.predict(dev_data)\n",
    "pred_labels = (pred > 0.5).astype(np.int)\n",
    "pred_labels = np.reshape(pred_labels,[148409,])\n",
    "\n",
    "#print(y_dev[0:5])\n",
    "#print(pred_labels[0:5])\n",
    "accuracy = accuracy_score(y_dev,pred_labels)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1:  0.4894053721753443\n"
     ]
    }
   ],
   "source": [
    "cnn_glove_f1 = f1_score(y_dev,pred_labels)\n",
    "print(\"F1: \",cnn_glove_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.47\n"
     ]
    }
   ],
   "source": [
    "average_precision = average_precision_score(y_dev,pred_labels)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([90106, 58303]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_dev,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 50, 300)           15000000  \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 46, 64)            96064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 11, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 15,162,165\n",
      "Trainable params: 162,165\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ted_main = pd.read_csv(\"../data-DNC/TED/ted_main.csv\")\n",
    "ted_transcript = pd.read_csv(\"../data-DNC/TED/transcripts.csv\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>duration</th>\n",
       "      <th>film_date</th>\n",
       "      <th>languages</th>\n",
       "      <th>num_speaker</th>\n",
       "      <th>published_date</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2550.000000</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>2.550000e+03</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>2550.000000</td>\n",
       "      <td>2.550000e+03</td>\n",
       "      <td>2.550000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>191.562353</td>\n",
       "      <td>826.510196</td>\n",
       "      <td>1.321928e+09</td>\n",
       "      <td>27.326275</td>\n",
       "      <td>1.028235</td>\n",
       "      <td>1.343525e+09</td>\n",
       "      <td>1.698297e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>282.315223</td>\n",
       "      <td>374.009138</td>\n",
       "      <td>1.197391e+08</td>\n",
       "      <td>9.563452</td>\n",
       "      <td>0.207705</td>\n",
       "      <td>9.464009e+07</td>\n",
       "      <td>2.498479e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>7.464960e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.151367e+09</td>\n",
       "      <td>5.044300e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>1.257466e+09</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.268463e+09</td>\n",
       "      <td>7.557928e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>118.000000</td>\n",
       "      <td>848.000000</td>\n",
       "      <td>1.333238e+09</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.340935e+09</td>\n",
       "      <td>1.124524e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>221.750000</td>\n",
       "      <td>1046.750000</td>\n",
       "      <td>1.412964e+09</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.423432e+09</td>\n",
       "      <td>1.700760e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6404.000000</td>\n",
       "      <td>5256.000000</td>\n",
       "      <td>1.503792e+09</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.506092e+09</td>\n",
       "      <td>4.722711e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          comments     duration     film_date    languages  num_speaker  \\\n",
       "count  2550.000000  2550.000000  2.550000e+03  2550.000000  2550.000000   \n",
       "mean    191.562353   826.510196  1.321928e+09    27.326275     1.028235   \n",
       "std     282.315223   374.009138  1.197391e+08     9.563452     0.207705   \n",
       "min       2.000000   135.000000  7.464960e+07     0.000000     1.000000   \n",
       "25%      63.000000   577.000000  1.257466e+09    23.000000     1.000000   \n",
       "50%     118.000000   848.000000  1.333238e+09    28.000000     1.000000   \n",
       "75%     221.750000  1046.750000  1.412964e+09    33.000000     1.000000   \n",
       "max    6404.000000  5256.000000  1.503792e+09    72.000000     5.000000   \n",
       "\n",
       "       published_date         views  \n",
       "count    2.550000e+03  2.550000e+03  \n",
       "mean     1.343525e+09  1.698297e+06  \n",
       "std      9.464009e+07  2.498479e+06  \n",
       "min      1.151367e+09  5.044300e+04  \n",
       "25%      1.268463e+09  7.557928e+05  \n",
       "50%      1.340935e+09  1.124524e+06  \n",
       "75%      1.423432e+09  1.700760e+06  \n",
       "max      1.506092e+09  4.722711e+07  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted_main.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2467</td>\n",
       "      <td>2467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2464</td>\n",
       "      <td>2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>I'm going to tell you a little bit about my TE...</td>\n",
       "      <td>https://www.ted.com/talks/brene_brown_listenin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               transcript  \\\n",
       "count                                                2467   \n",
       "unique                                               2464   \n",
       "top     I'm going to tell you a little bit about my TE...   \n",
       "freq                                                    2   \n",
       "\n",
       "                                                      url  \n",
       "count                                                2467  \n",
       "unique                                               2464  \n",
       "top     https://www.ted.com/talks/brene_brown_listenin...  \n",
       "freq                                                    2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted_transcript.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>description</th>\n",
       "      <th>duration</th>\n",
       "      <th>event</th>\n",
       "      <th>film_date</th>\n",
       "      <th>languages</th>\n",
       "      <th>main_speaker</th>\n",
       "      <th>name</th>\n",
       "      <th>num_speaker</th>\n",
       "      <th>published_date</th>\n",
       "      <th>ratings</th>\n",
       "      <th>related_talks</th>\n",
       "      <th>speaker_occupation</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4553</td>\n",
       "      <td>Sir Ken Robinson makes an entertaining and pro...</td>\n",
       "      <td>1164</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>60</td>\n",
       "      <td>Ken Robinson</td>\n",
       "      <td>Ken Robinson: Do schools kill creativity?</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 19645}, {...</td>\n",
       "      <td>[{'id': 865, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Author/educator</td>\n",
       "      <td>['children', 'creativity', 'culture', 'dance',...</td>\n",
       "      <td>Do schools kill creativity?</td>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "      <td>47227110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>265</td>\n",
       "      <td>With the same humor and humanity he exuded in ...</td>\n",
       "      <td>977</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140825600</td>\n",
       "      <td>43</td>\n",
       "      <td>Al Gore</td>\n",
       "      <td>Al Gore: Averting the climate crisis</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 544}, {'i...</td>\n",
       "      <td>[{'id': 243, 'hero': 'https://pe.tedcdn.com/im...</td>\n",
       "      <td>Climate advocate</td>\n",
       "      <td>['alternative energy', 'cars', 'climate change...</td>\n",
       "      <td>Averting the climate crisis</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "      <td>3200520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124</td>\n",
       "      <td>New York Times columnist David Pogue takes aim...</td>\n",
       "      <td>1286</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140739200</td>\n",
       "      <td>26</td>\n",
       "      <td>David Pogue</td>\n",
       "      <td>David Pogue: Simplicity sells</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 7, 'name': 'Funny', 'count': 964}, {'i...</td>\n",
       "      <td>[{'id': 1725, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Technology columnist</td>\n",
       "      <td>['computers', 'entertainment', 'interface desi...</td>\n",
       "      <td>Simplicity sells</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "      <td>1636292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>In an emotionally charged talk, MacArthur-winn...</td>\n",
       "      <td>1116</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140912000</td>\n",
       "      <td>35</td>\n",
       "      <td>Majora Carter</td>\n",
       "      <td>Majora Carter: Greening the ghetto</td>\n",
       "      <td>1</td>\n",
       "      <td>1151367060</td>\n",
       "      <td>[{'id': 3, 'name': 'Courageous', 'count': 760}...</td>\n",
       "      <td>[{'id': 1041, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Activist for environmental justice</td>\n",
       "      <td>['MacArthur grant', 'activism', 'business', 'c...</td>\n",
       "      <td>Greening the ghetto</td>\n",
       "      <td>https://www.ted.com/talks/majora_carter_s_tale...</td>\n",
       "      <td>1697550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>593</td>\n",
       "      <td>You've never seen data presented like this. Wi...</td>\n",
       "      <td>1190</td>\n",
       "      <td>TED2006</td>\n",
       "      <td>1140566400</td>\n",
       "      <td>48</td>\n",
       "      <td>Hans Rosling</td>\n",
       "      <td>Hans Rosling: The best stats you've ever seen</td>\n",
       "      <td>1</td>\n",
       "      <td>1151440680</td>\n",
       "      <td>[{'id': 9, 'name': 'Ingenious', 'count': 3202}...</td>\n",
       "      <td>[{'id': 2056, 'hero': 'https://pe.tedcdn.com/i...</td>\n",
       "      <td>Global health expert; data visionary</td>\n",
       "      <td>['Africa', 'Asia', 'Google', 'demo', 'economic...</td>\n",
       "      <td>The best stats you've ever seen</td>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_shows_t...</td>\n",
       "      <td>12005869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comments                                        description  duration  \\\n",
       "0      4553  Sir Ken Robinson makes an entertaining and pro...      1164   \n",
       "1       265  With the same humor and humanity he exuded in ...       977   \n",
       "2       124  New York Times columnist David Pogue takes aim...      1286   \n",
       "3       200  In an emotionally charged talk, MacArthur-winn...      1116   \n",
       "4       593  You've never seen data presented like this. Wi...      1190   \n",
       "\n",
       "     event   film_date  languages   main_speaker  \\\n",
       "0  TED2006  1140825600         60   Ken Robinson   \n",
       "1  TED2006  1140825600         43        Al Gore   \n",
       "2  TED2006  1140739200         26    David Pogue   \n",
       "3  TED2006  1140912000         35  Majora Carter   \n",
       "4  TED2006  1140566400         48   Hans Rosling   \n",
       "\n",
       "                                            name  num_speaker  published_date  \\\n",
       "0      Ken Robinson: Do schools kill creativity?            1      1151367060   \n",
       "1           Al Gore: Averting the climate crisis            1      1151367060   \n",
       "2                  David Pogue: Simplicity sells            1      1151367060   \n",
       "3             Majora Carter: Greening the ghetto            1      1151367060   \n",
       "4  Hans Rosling: The best stats you've ever seen            1      1151440680   \n",
       "\n",
       "                                             ratings  \\\n",
       "0  [{'id': 7, 'name': 'Funny', 'count': 19645}, {...   \n",
       "1  [{'id': 7, 'name': 'Funny', 'count': 544}, {'i...   \n",
       "2  [{'id': 7, 'name': 'Funny', 'count': 964}, {'i...   \n",
       "3  [{'id': 3, 'name': 'Courageous', 'count': 760}...   \n",
       "4  [{'id': 9, 'name': 'Ingenious', 'count': 3202}...   \n",
       "\n",
       "                                       related_talks  \\\n",
       "0  [{'id': 865, 'hero': 'https://pe.tedcdn.com/im...   \n",
       "1  [{'id': 243, 'hero': 'https://pe.tedcdn.com/im...   \n",
       "2  [{'id': 1725, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "3  [{'id': 1041, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "4  [{'id': 2056, 'hero': 'https://pe.tedcdn.com/i...   \n",
       "\n",
       "                     speaker_occupation  \\\n",
       "0                       Author/educator   \n",
       "1                      Climate advocate   \n",
       "2                  Technology columnist   \n",
       "3    Activist for environmental justice   \n",
       "4  Global health expert; data visionary   \n",
       "\n",
       "                                                tags  \\\n",
       "0  ['children', 'creativity', 'culture', 'dance',...   \n",
       "1  ['alternative energy', 'cars', 'climate change...   \n",
       "2  ['computers', 'entertainment', 'interface desi...   \n",
       "3  ['MacArthur grant', 'activism', 'business', 'c...   \n",
       "4  ['Africa', 'Asia', 'Google', 'demo', 'economic...   \n",
       "\n",
       "                             title  \\\n",
       "0      Do schools kill creativity?   \n",
       "1      Averting the climate crisis   \n",
       "2                 Simplicity sells   \n",
       "3              Greening the ghetto   \n",
       "4  The best stats you've ever seen   \n",
       "\n",
       "                                                 url     views  \n",
       "0  https://www.ted.com/talks/ken_robinson_says_sc...  47227110  \n",
       "1  https://www.ted.com/talks/al_gore_on_averting_...   3200520  \n",
       "2  https://www.ted.com/talks/david_pogue_says_sim...   1636292  \n",
       "3  https://www.ted.com/talks/majora_carter_s_tale...   1697550  \n",
       "4  https://www.ted.com/talks/hans_rosling_shows_t...  12005869  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted_main.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good morning. How are you?(Laughter)It's been ...</td>\n",
       "      <td>https://www.ted.com/talks/ken_robinson_says_sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you so much, Chris. And it's truly a gre...</td>\n",
       "      <td>https://www.ted.com/talks/al_gore_on_averting_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Music: \"The Sound of Silence,\" Simon &amp; Garfun...</td>\n",
       "      <td>https://www.ted.com/talks/david_pogue_says_sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you're here today â€” and I'm very happy that...</td>\n",
       "      <td>https://www.ted.com/talks/majora_carter_s_tale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>About 10 years ago, I took on the task to teac...</td>\n",
       "      <td>https://www.ted.com/talks/hans_rosling_shows_t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  Good morning. How are you?(Laughter)It's been ...   \n",
       "1  Thank you so much, Chris. And it's truly a gre...   \n",
       "2  (Music: \"The Sound of Silence,\" Simon & Garfun...   \n",
       "3  If you're here today â€” and I'm very happy that...   \n",
       "4  About 10 years ago, I took on the task to teac...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/ken_robinson_says_sc...  \n",
       "1  https://www.ted.com/talks/al_gore_on_averting_...  \n",
       "2  https://www.ted.com/talks/david_pogue_says_sim...  \n",
       "3  https://www.ted.com/talks/majora_carter_s_tale...  \n",
       "4  https://www.ted.com/talks/hans_rosling_shows_t...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted_transcript.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2550\n",
      "2467\n"
     ]
    }
   ],
   "source": [
    "print(len(ted_main))\n",
    "print(len(ted_transcript))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not all TED MAIN has a transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2464"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted_match = ted_main[ted_main.url.isin(ted_transcript.url)]\n",
    "ted_notmatch = ted_main[~ted_main.url.isin(ted_transcript.url)]\n",
    "len(ted_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = aeg_long.essay[0]\n",
    "#sentence = ted_transcript.transcript[0]\n",
    "s0 = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t0 = Text(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear local newspaper , I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people , helps us learn about the globe ( astronomy ) and keeps us out of troble ! Thing about ! Dont you think so ? How would you feel if your teenager is always on the phone with friends ! Do you ever time to chat with your friends or buisness partner about things . Well now - there 's a new way to chat the computer , theirs plenty of sites on the internet to do so : @ ORGANIZATION1 , @ ORGANIZATION2 , @ CAPS1 , facebook , myspace ect . Just think now while your setting up meeting with your boss on the computer , your teenager is having fun on the phone not rushing to get off cause you want to use it . How did you learn about other countrys/states outside of yours ? Well I have by computer/internet , it 's a new way to learn about what going on in our time ! You might think your child spends a lot of time on the computer , but ask them so question about the economy , sea floor spreading or even about the @ DATE1 's you 'll be surprise at how much he/she knows . Believe it or not the computer is much interesting then in class all day reading out of books . If your child is home on your computer or at a local library , it 's better than being out with friends being fresh , or being perpressured to doing something they know isnt right . You might not know where your child is , @ CAPS2 forbidde in a hospital bed because of a drive-by . Rather than your child on the computer learning , chatting or just playing games , safe and sound in your home or community place . Now I hope you have reached a point to understand and agree with me , because computers can have great effects on you or child because it gives us time to chat with friends/new people , helps us learn about the globe and believe or not keeps us out of troble . Thank you for listening .\n"
     ]
    }
   ],
   "source": [
    "sentence_mod = ' '.join(s0)\n",
    "print(sentence_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4689119170984456"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "lexical_diversity(t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_similar(word):\n",
    "    queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return by_similarity[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#word_syn_list = [wordnet_lemmatizer.lemmatize(w.lower_) for w in most_similar(nlp.vocab[u'Dear'])]\n",
    "word_syn_list = [w.lower_ for w in most_similar(nlp.vocab[u'Safe'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'safe', 'safety', 'secure'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(word_syn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.lexeme.Lexeme'>\n"
     ]
    }
   ],
   "source": [
    "w_sample = nlp.vocab[u'Safe']\n",
    "print(type(w_sample))\n",
    "for w in w_sample.vocab:\n",
    "    print(w.prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = nlp(sentence_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_spacy = pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type'])\n",
    "nltk_pos_tagged = nltk.pos_tag(sentence.split())\n",
    "df_nltk = pd.DataFrame(nltk_pos_tagged, columns=['Word', 'POS tag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS tag</th>\n",
       "      <th>Tag type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>local</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>newspaper</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>think</td>\n",
       "      <td>VBP</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>effects</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>computers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>people</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>are</td>\n",
       "      <td>VBP</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>great</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>learning</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>skills</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/</td>\n",
       "      <td>SYM</td>\n",
       "      <td>SYM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>affects</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>because</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>give</td>\n",
       "      <td>VBP</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>us</td>\n",
       "      <td>PRP</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>time</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>chat</td>\n",
       "      <td>VB</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>with</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>friends</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>/</td>\n",
       "      <td>SYM</td>\n",
       "      <td>SYM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>new</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>people</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>time</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>chat</td>\n",
       "      <td>VB</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>with</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>friends</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>/</td>\n",
       "      <td>SYM</td>\n",
       "      <td>SYM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>new</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>people</td>\n",
       "      <td>NNS</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>helps</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>us</td>\n",
       "      <td>PRP</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>learn</td>\n",
       "      <td>VB</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>about</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>globe</td>\n",
       "      <td>NN</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>believe</td>\n",
       "      <td>VBP</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>or</td>\n",
       "      <td>CC</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>not</td>\n",
       "      <td>RB</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>keeps</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>us</td>\n",
       "      <td>PRP</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>out</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>troble</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>Thank</td>\n",
       "      <td>VBP</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>you</td>\n",
       "      <td>PRP</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>for</td>\n",
       "      <td>IN</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>listening</td>\n",
       "      <td>VBG</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>402 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word POS tag Tag type\n",
       "0         Dear      JJ      ADJ\n",
       "1        local      JJ      ADJ\n",
       "2    newspaper      NN     NOUN\n",
       "3            ,       ,    PUNCT\n",
       "4            I     PRP     PRON\n",
       "5        think     VBP     VERB\n",
       "6      effects     NNS     NOUN\n",
       "7    computers     NNS     NOUN\n",
       "8         have     VBP     VERB\n",
       "9           on      IN      ADP\n",
       "10      people     NNS     NOUN\n",
       "11         are     VBP     VERB\n",
       "12       great      JJ      ADJ\n",
       "13    learning      NN     NOUN\n",
       "14      skills     NNS     NOUN\n",
       "15           /     SYM      SYM\n",
       "16     affects     NNS     NOUN\n",
       "17     because      IN      ADP\n",
       "18        they     PRP     PRON\n",
       "19        give     VBP     VERB\n",
       "20          us     PRP     PRON\n",
       "21        time      NN     NOUN\n",
       "22          to      TO     PART\n",
       "23        chat      VB     VERB\n",
       "24        with      IN      ADP\n",
       "25     friends     NNS     NOUN\n",
       "26           /     SYM      SYM\n",
       "27         new      JJ      ADJ\n",
       "28      people     NNS     NOUN\n",
       "29           ,       ,    PUNCT\n",
       "..         ...     ...      ...\n",
       "372       time      NN     NOUN\n",
       "373         to      TO     PART\n",
       "374       chat      VB     VERB\n",
       "375       with      IN      ADP\n",
       "376    friends     NNS     NOUN\n",
       "377          /     SYM      SYM\n",
       "378        new      JJ      ADJ\n",
       "379     people     NNS     NOUN\n",
       "380          ,       ,    PUNCT\n",
       "381      helps     VBZ     VERB\n",
       "382         us     PRP     PRON\n",
       "383      learn      VB     VERB\n",
       "384      about      IN      ADP\n",
       "385        the      DT      DET\n",
       "386      globe      NN     NOUN\n",
       "387        and      CC    CCONJ\n",
       "388    believe     VBP     VERB\n",
       "389         or      CC    CCONJ\n",
       "390        not      RB      ADV\n",
       "391      keeps     VBZ     VERB\n",
       "392         us     PRP     PRON\n",
       "393        out      IN      ADP\n",
       "394         of      IN      ADP\n",
       "395     troble      JJ      ADJ\n",
       "396          .       .    PUNCT\n",
       "397      Thank     VBP     VERB\n",
       "398        you     PRP     PRON\n",
       "399        for      IN      ADP\n",
       "400  listening     VBG     VERB\n",
       "401          .       .    PUNCT\n",
       "\n",
       "[402 rows x 3 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>local</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>newspaper,</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>think</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>effects</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>computers</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>people</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>are</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>great</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>learning</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>skills/affects</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>because</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>give</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>us</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>time</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>chat</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>with</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>friends/new</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>people,</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>helps</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>us</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>learn</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>about</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>globe(astronomy)</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>child</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>because</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>it</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>gives</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>us</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>time</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>chat</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>with</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>friends/new</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>people,</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>helps</td>\n",
       "      <td>VBZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>us</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>learn</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>about</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>globe</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>believe</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>or</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>not</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>keeps</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>us</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>out</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>troble.</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>Thank</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>you</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>for</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>listening.</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>338 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Word POS tag\n",
       "0                Dear     NNP\n",
       "1               local      JJ\n",
       "2          newspaper,      NN\n",
       "3                   I     PRP\n",
       "4               think     VBP\n",
       "5             effects     NNS\n",
       "6           computers     NNS\n",
       "7                have     VBP\n",
       "8                  on      IN\n",
       "9              people     NNS\n",
       "10                are     VBP\n",
       "11              great      JJ\n",
       "12           learning      JJ\n",
       "13     skills/affects     NNS\n",
       "14            because      IN\n",
       "15               they     PRP\n",
       "16               give     VBP\n",
       "17                 us     PRP\n",
       "18               time      NN\n",
       "19                 to      TO\n",
       "20               chat      VB\n",
       "21               with      IN\n",
       "22        friends/new      JJ\n",
       "23            people,      NN\n",
       "24              helps     VBZ\n",
       "25                 us     PRP\n",
       "26              learn      VB\n",
       "27              about      IN\n",
       "28                the      DT\n",
       "29   globe(astronomy)      NN\n",
       "..                ...     ...\n",
       "308             child      VB\n",
       "309           because      IN\n",
       "310                it     PRP\n",
       "311             gives     VBZ\n",
       "312                us     PRP\n",
       "313              time      NN\n",
       "314                to      TO\n",
       "315              chat      VB\n",
       "316              with      IN\n",
       "317       friends/new      JJ\n",
       "318           people,      NN\n",
       "319             helps     VBZ\n",
       "320                us     PRP\n",
       "321             learn      VB\n",
       "322             about      IN\n",
       "323               the      DT\n",
       "324             globe      NN\n",
       "325               and      CC\n",
       "326           believe     VBP\n",
       "327                or      CC\n",
       "328               not      RB\n",
       "329             keeps      VB\n",
       "330                us     PRP\n",
       "331               out      IN\n",
       "332                of      IN\n",
       "333           troble.      NN\n",
       "334             Thank     NNP\n",
       "335               you     PRP\n",
       "336               for      IN\n",
       "337        listening.      NN\n",
       "\n",
       "[338 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORGANIZATION2 GPE\n",
      "DATE1 ORG\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x1a42ac9a60>, {'Dear': {'costly', 'beloved', 'dear'}, 'local': {'local', 'local_anesthetic'}, 'newspaper': {'newspaper'}, 'I': {'I', 'iodine', 'one'}, 'think': {'think'}, 'effects': {'effects', 'effect', 'consequence', 'impression'}, 'computers': {'computer', 'calculator'}, 'people': {'citizenry', 'people', 'multitude'}, 'great': {'bang-up', 'big', 'great', 'capital'}, 'learning': {'eruditeness', 'learning'}, 'give': {'give'}, 'us': {'U', 'United_States', 'uracil', 'uranium'}, 'time': {'clock_time', 'prison_term', 'time', 'meter', 'fourth_dimension'}, 'chat': {'New_World_chat', 'Old_World_chat', 'chat'}, 'helps': {'avail', 'assistant', 'aid'}, 'globe': {'Earth', 'globe', 'ball'}, 'astronomy': {'astronomy'}, 'keeps': {'support', 'keep', 'hold'}, 'Thing': {'matter', 'thing'}, 'feel': {'feel', 'spirit', 'tactile_property'}, 'teenager': {'adolescent'}, 'phone': {'telephone', 'phone', 'earphone'}, 'friends': {'ally', 'friend', 'Friend', 'acquaintance', 'supporter'}, 'Do': {'bash', 'Doctor_of_Osteopathy', 'do'}, 'partner': {'spouse', 'collaborator', 'partner'}, 'things': {'things', 'matter', 'thing'}, 'Well': {'well'}, 'way': {'way', 'manner', 'means', 'room', 'direction'}, 'plenty': {'plenty', 'batch'}, 'sites': {'site', 'web_site'}, 'internet': {'internet'}, 'ect': {'electroconvulsive_therapy'}, 'setting': {'mount', 'context', 'place_setting', 'mise_en_scene', 'setting'}, 'meeting': {'merging', 'meeting', 'confluence'}, 'boss': {'knob', 'party_boss', 'boss', 'foreman', 'Bos'}, 'computer': {'computer', 'calculator'}, 'fun': {'fun', 'playfulness'}, 'rushing': {'haste', 'rush'}, 'get': {'get'}, 'cause': {'campaign', 'cause', 'causal_agent', 'lawsuit'}, 'want': {'privation', 'lack', 'need', 'wish'}, 'use': {'function', 'consumption', 'use', 'habit', 'manipulation'}, 'outside': {'outside'}, 'going': {'going', 'departure', 'passing'}, 'might': {'might'}, 'child': {'child'}, 'lot': {'lot', 'set', 'bunch', 'batch', 'fortune', 'draw', 'Lot'}, 'question': {'motion', 'doubt', 'question'}, 'economy': {'economy'}, 'sea': {'ocean', 'sea'}, 'floor': {'floor'}, 'spreading': {'spread', 'dissemination'}, 'even': {'evening'}, 'surprise': {'surprise'}, 'much': {'much'}, 'knows': {'know'}, 'class': {'class', 'course'}, 'day': {'day', 'sidereal_day', 'Day'}, 'reading': {'recitation', 'interpretation', 'Reading', 'reading'}, 'books': {'script', 'Koran', 'ledger', 'book', 'Bible', 'record'}, 'home': {'dwelling', 'home_plate', 'home', 'base', 'family'}, 'better': {'better', 'bettor'}, 'know': {'know'}, 'right': {'right', 'veracious', 'good', 'proper', 'correct'}, 'hospital': {'hospital'}, 'bed': {'layer', 'bed', 'seam'}, 'playing': {'playing', 'acting'}, 'games': {'game', 'plot'}, 'safe': {'condom', 'safe'}, 'sound': {'audio', 'phone', 'strait', 'sound'}, 'community': {'residential_district', 'community'}, 'place': {'plaza', 'stead', 'home', 'position', 'seat', 'topographic_point', 'place', 'space'}, 'Now': {'now'}, 'hope': {'Hope', 'hope', 'promise'}, 'point': {'distributor_point', 'compass_point', 'decimal_point', 'point', 'detail', 'period', 'degree', 'item'}, 'gives': {'give'}, 'listening': {'listening'}})\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "word_syns = defaultdict(lambda: set())\n",
    "for word_index,word in enumerate(s0):\n",
    "    if word not in stop_words:\n",
    "        word_pos = doc[word_index].pos_\n",
    "        if word_pos == \"ADJ\":\n",
    "            syns = wordnet.synsets(word,wordnet.ADJ)\n",
    "        elif word_pos == \"NOUN\" or \"PROPN\":\n",
    "            syns = wordnet.synsets(word,wordnet.NOUN)\n",
    "        elif word_pos == \"ADV\":\n",
    "            syns = wordnet.synsets(word,wordnet.ADV)\n",
    "        else:\n",
    "            continue\n",
    "            #if word not in stop_words:\n",
    "            #or word_pos == \"NOUN\" or word_pos == \"VERB\"\n",
    "        for i in range(len(syns)):\n",
    "            word_syns[word].add(syns[i].lemmas()[0].name())\n",
    "\n",
    "print(word_syns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x1a42d03e18>, {'Dear': {'thank', 'dear'}, 'local': {'local', 'businesses', 'locally', 'regional'}, 'newspaper': {'tabloid', 'newspapers', 'magazine', 'newspaper'}, 'effects': {'effects', 'adverse', 'affect', 'effect'}, 'computers': {'laptops', 'computers', 'desktops', 'computer'}, 'people': {'individuals', 'those', 'people', 'others'}, 'great': {'fantastic', 'good', 'great', 'wonderful'}, 'learning': {'learn', 'teach', 'teaching', 'learning'}, 'skills/affects': {'educated', 'wade', 'eradicate', 'equator'}, 'chat': {'chatroom', 'chatting', 'chat', 'chats'}, ',': {'and', ',', ';'}, 'us': {'our', 'us', 'we', 'them'}, 'learn': {'understand', 'teach', 'learn', 'learning'}, '!': {':)', '!', ':-)', ';)'}, 'How': {'what', 'how'}, 'feel': {'felt', 'feels', 'feel', 'feeling'}, 'phone': {'phone', 'cellphone', 'phones', 'telephone'}, 'friends': {'buddies', 'pals', 'friend', 'friends'}, 'time': {'day', 'time', 'once', 'when'}, 'things': {'things', 'everything', 'something', 'thing'}, 'Well': {'but', 'well'}, 'new': {'newly', 'newest', 'new', 'latest'}, 'computer': {'software', 'computer', 'laptop', 'computers'}, 'sites': {'site', 'web', 'sites', 'websites'}, 'internet': {'online', 'web', 'websites', 'internet'}, '@': {'@', 'pm', 'at'}, 'facebook': {'fb', 'google', 'twitter', 'facebook'}, 'ect': {'whatnot', 'etc', 'anyways', 'ect'}, 'Just': {'just', 'actually'}, 'setting': {'settings', 'setting', 'set', 'changing'}, 'meeting': {'meeting', 'conference', 'committee', 'meetings'}, 'rushing': {'rushes', 'touchdowns', 'rushing', 'rushed'}, 'cause': {'causing', 'cause', 'caused', 'causes'}, '?': {'....', '?', '!', 'anyway'}, 'think': {'guess', 'thought', 'think', 'know'}, 'lot': {'lot', 'some', 'lots', 'much'}, 'question': {'questions', 'answer', 'problem', 'question'}, 'economy': {'economy', 'economies', 'recession', 'economic'}, \"'s\": {'s', 'own', 'his', \"'s\"}, 'surprise': {'surprising', 'surprised', 'surprises', 'surprise'}, 'much': {'so', 'even', 'too', 'much'}, 'he/she': {'him/her', 'she/he', 's/he', 'he/she'}, 'reading': {'books', 'reading', 'read', 'writing'}, '.': {'....', '.', '...', 'so'}, 'If': {'if', 'unless'}, 'library': {'libraries', 'library', 'bookstore', 'librarian'}, 'fresh': {'tasty', 'freshly', 'delicious', 'fresh'}, 'perpressured': {'educated', 'wade', 'eradicate', 'equator'}, 'know': {'what', 'think', 'tell', 'know'}, 'right': {'way', 'right', 'you', 'just'}, 'CAPS2': {'sonny', 'peasant', 'vivi', 'concentrate'}, 'hospital': {'medical', 'hospitals', 'clinic', 'hospital'}, 'playing': {'plays', 'play', 'playing', 'played'}, 'sound': {'sounded', 'sounding', 'sounds', 'sound'}, 'home': {'home', 'living', 'house', 'homes'}, 'Now': {'now', 'already'}, 'hope': {'hope', 'glad', 'hopefully', 'hoping'}, 'understand': {'explain', 'understand', 'realize', 'know'}, 'agree': {'think', 'disagree', 'believe', 'agree'}, 'friends/new': {'educated', 'wade', 'eradicate', 'equator'}, 'believe': {'think', 'say', 'why', 'believe'}, 'troble': {'forsee', 'shotting', 'probs', 'shitton'}})\n"
     ]
    }
   ],
   "source": [
    "# Use spacy instead of wordnet\n",
    "word_syns_spacy = defaultdict(lambda: set())\n",
    "for word_index,word in enumerate(s0):\n",
    "    if word not in stop_words:\n",
    "        word_pos = doc[word_index].pos_\n",
    "        if word_pos == \"ADJ\" or word_pos == \"NOUN\" or word_pos == \"PROPN\" or word_pos == \"ADV\":\n",
    "            word_syn_list = [w.lower_ for w in most_similar(nlp.vocab[word])]\n",
    "            for syn_w in word_syn_list:\n",
    "                word_syns_spacy[word].add(syn_w)\n",
    "\n",
    "print(word_syns_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('safe.n.01'), Synset('safe.n.02'), Synset('condom.n.01')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('safe',wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
