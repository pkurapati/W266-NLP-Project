{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "import json, os, re, shutil, sys, time\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "from collections import defaultdict\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NumPy, Pandas and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from numpy.random import seed\n",
    "from pandas import read_csv, DataFrame\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Activation, Dropout, Conv1D, MaxPooling1D, Bidirectional, Flatten, TimeDistributed\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AEG Long Essay Sentence Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12976, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data\n",
    "aeg_long = pd.read_csv(\"../data-DNC/AEG/training_set_rel3.tsv\",sep='\\t',encoding = \"latin1\")\n",
    "aeg_long.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test. We need to do this first to ensure that when we split to \n",
    "# sentence level, we have sentences of a given essay in either training or test but not on both.\n",
    "\n",
    "tr_essay,ts_essay,tr_domain_score,ts_domain_score,tr_essay_id,ts_essay_id,tr_essay_set,ts_essay_set=train_test_split(\n",
    "    np.asarray(aeg_long.essay),np.asarray(aeg_long.domain1_score),\n",
    "    np.asarray(aeg_long.essay_id),np.asarray(aeg_long.essay_set),test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10380,)\n",
      "(2596,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shapes of training and test datasets\n",
    "print(tr_essay.shape)\n",
    "print(ts_essay.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test data frames with relevant fields\n",
    "\n",
    "x_train_df = pd.DataFrame([tr_essay_id,tr_essay_set,tr_essay,tr_domain_score.astype(np.double)]).transpose()\n",
    "x_train_df.columns = ['essay_id','essay_set','essay','domain1_score']\n",
    "x_test_df = pd.DataFrame([ts_essay_id,ts_essay_set,ts_essay,ts_domain_score.astype(np.double)]).transpose()\n",
    "x_test_df.columns = ['essay_id','essay_set','essay','domain1_score']\n",
    "#print(x_train_df.head(5))\n",
    "#print(x_test_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split essay into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will use spacy to split the essay into sentences. \n",
    "# Load spacy large english module\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split essay into sentences\n",
    "def create_sentences_df(df):\n",
    "    \"\"\" Function to split essay data into individual sentences. Returns a dataframe\"\"\"\n",
    "    start=datetime.now()\n",
    "    aeg_long_sentence = pd.DataFrame(columns=['essay_id','essay_set','sentence','domain1_score'])\n",
    "    for i in range(len(df)):\n",
    "        if i%1000 == 0:\n",
    "            print(\"At iteration :\",i)\n",
    "            print(\"Duration: \",datetime.now()-start)\n",
    "        sentence = nlp(df.essay[i])\n",
    "        for s in sentence.sents:\n",
    "            aeg_long_sentence = aeg_long_sentence.append({'essay_id' : df.essay_id[i],\n",
    "                                                          'essay_set' : df.essay_set[i],'sentence' : s.text, \n",
    "                                                          'domain1_score' : df.domain1_score[i]},\n",
    "                                                         ignore_index=True)\n",
    "    return aeg_long_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We also need to convert the spacy format text into regular text\n",
    "def spacy_to_text(essay):\n",
    "    return essay[2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration : 0\n",
      "Duration:  0:00:00.001094\n",
      "At iteration : 1000\n",
      "Duration:  0:01:13.910230\n",
      "At iteration : 2000\n",
      "Duration:  0:02:41.471731\n",
      "At iteration : 3000\n",
      "Duration:  0:04:57.690647\n",
      "At iteration : 4000\n",
      "Duration:  0:07:51.580886\n",
      "At iteration : 5000\n",
      "Duration:  0:11:39.480142\n",
      "At iteration : 6000\n",
      "Duration:  0:16:11.116163\n",
      "At iteration : 7000\n",
      "Duration:  0:21:46.361166\n",
      "At iteration : 8000\n",
      "Duration:  0:27:57.529110\n",
      "At iteration : 9000\n",
      "Duration:  0:34:42.670452\n",
      "At iteration : 10000\n",
      "Duration:  0:42:12.653912\n"
     ]
    }
   ],
   "source": [
    "# Split train data into sentences\n",
    "x_train_sentence_df = create_sentences_df(x_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration : 0\n",
      "Duration:  0:00:00.001098\n",
      "At iteration : 1000\n",
      "Duration:  0:01:15.384433\n",
      "At iteration : 2000\n",
      "Duration:  0:02:53.557499\n"
     ]
    }
   ],
   "source": [
    "# Split test data into sentences\n",
    "x_test_sentence_df = create_sentences_df(x_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each essay set has a different scoring range. We need to normalize the scores to a standard scale for training.\n",
    "def normalize_score(essay):\n",
    "    \"\"\" Normalizes the domain score based on percentage\"\"\"\n",
    "    score = 0\n",
    "    score = float(essay[3])\n",
    "    essay_set = essay[1]\n",
    "    if essay_set == 1:\n",
    "        div = 12\n",
    "    elif essay_set == 2:\n",
    "        div = 5\n",
    "    elif essay_set == 3:\n",
    "        div = 3\n",
    "    elif essay_set == 4:\n",
    "        div = 3\n",
    "    elif essay_set == 5:\n",
    "        div = 4\n",
    "    elif essay_set == 6:\n",
    "        div = 4\n",
    "    elif essay_set == 7:\n",
    "        div = 25\n",
    "    elif essay_set == 8:\n",
    "        div = 50\n",
    "    return score/div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_sentence_df['Norm_Score'] = x_train_sentence_df.apply(normalize_score,axis=1)\n",
    "x_test_sentence_df['Norm_Score'] = x_test_sentence_df.apply(normalize_score,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>sentence</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>Norm_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4075</td>\n",
       "      <td>2</td>\n",
       "      <td>Did your child ever bring home a book?</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4075</td>\n",
       "      <td>2</td>\n",
       "      <td>A piece of music, movie or magazine?</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4075</td>\n",
       "      <td>2</td>\n",
       "      <td>Did you ever stop to think that the piece of i...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4075</td>\n",
       "      <td>2</td>\n",
       "      <td>There are points in time where there isn't an ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4075</td>\n",
       "      <td>2</td>\n",
       "      <td>The driving question is do you want to remove ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id essay_set                                           sentence  \\\n",
       "0     4075         2             Did your child ever bring home a book?   \n",
       "1     4075         2               A piece of music, movie or magazine?   \n",
       "2     4075         2  Did you ever stop to think that the piece of i...   \n",
       "3     4075         2  There are points in time where there isn't an ...   \n",
       "4     4075         2  The driving question is do you want to remove ...   \n",
       "\n",
       "   domain1_score  Norm_Score  \n",
       "0            3.0         0.6  \n",
       "1            3.0         0.6  \n",
       "2            3.0         0.6  \n",
       "3            3.0         0.6  \n",
       "4            3.0         0.6  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_sentence_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store this dataset in pickle format so that we don't have to redo the above steps.\n",
    "x_train_sentence_df.to_pickle(\"./x_train_sentence_df.pkl\")\n",
    "x_test_sentence_df.to_pickle(\"./x_test_sentence_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from pickle. \n",
    "x_train_sentence_df = pd.read_pickle(\"/home/pkurapati/x_train_sentence_df.pkl\")\n",
    "x_test_sentence_df = pd.read_pickle(\"/home/pkurapati/x_test_sentence_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147647,)\n",
      "(147647,)\n",
      "(36254,)\n",
      "(36254,)\n"
     ]
    }
   ],
   "source": [
    "# Create train and test text and labels\n",
    "\n",
    "x_train = x_train_sentence_df['sentence'].values\n",
    "y_train = x_train_sentence_df['Norm_Score'].values\n",
    "x_test = x_test_sentence_df['sentence'].values\n",
    "y_test = x_test_sentence_df['Norm_Score'].values\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "train_data = pad_sequences(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_class = train_data.shape[1]\n",
    "max_words_class = vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36254, 175)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "test_data = pad_sequences(test_seq, maxlen=max_len_class)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('/home/pkurapati/W266-NLP-Project/data-DNC/glove.42B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(actual, predict):\n",
    "    diff = actual - predict\n",
    "    diff = sum(diff**2) / len(actual)\n",
    "    return np.sqrt(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cohen Kappa score as defined by the kaggle challenge/wikipedia\n",
    "def CohenKappa(actual, predict):\n",
    "    CohenDF = pd.DataFrame([actual.astype(np.double).round(), np.around(predict.astype(np.double))]).transpose()\n",
    "    count = len(CohenDF)\n",
    "    CohenDF.columns = ['actual','predict']\n",
    "    correct = len(CohenDF[CohenDF.actual==CohenDF.predict])\n",
    "    acc = correct / count\n",
    "    pe = 0\n",
    "    for value in CohenDF.actual.unique():\n",
    "        pe += len(CohenDF[CohenDF.actual == value]) * len(CohenDF[CohenDF.predict == value])\n",
    "    pe = pe / np.square(count)\n",
    "    print(\"Count: \",count)\n",
    "    print(\"pe:\" ,pe)\n",
    "    return(1 - (1-acc)/(1-pe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model definitions\n",
    "\n",
    "def FF_NN():\n",
    "    \"\"\" Simple feed forward NN\"\"\"\n",
    "    model_ff = Sequential()\n",
    "    model_ff.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_ff.add(tf.keras.layers.Flatten())\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_ff.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "    model_ff.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_ff\n",
    "\n",
    "def GRU():\n",
    "    \"\"\" Gated Recurrent Unit\"\"\"\n",
    "    model_gru = Sequential()\n",
    "    model_gru.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_gru.add(tf.keras.layers.GRU(32,activation='tanh'))\n",
    "    model_gru.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_gru.add(tf.keras.layers.Dense(1,name='out_layer'))\n",
    "    model_gru.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_rnn\n",
    "\n",
    "def CNN_FF():\n",
    "    \"\"\" CNN with Feed Forward NN \"\"\"\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "def CNN_lstm():\n",
    "    \"\"\" CNN with single layer LSTM & Feed Forward NN\"\"\"\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(tf.keras.layers.LSTM(100))\n",
    "    model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "def stack_lstm():\n",
    "    \"\"\" Three layered stacked LSTM.\"\"\"\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32,return_sequences=True))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32, return_sequences=True))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.2))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "def stateful_stacked_lstm():\n",
    "    # In stateful, total samples needs to be divisible by batch size\n",
    "    # we have 147026 samples, so selecting 6683 (6683*22=147026)\n",
    "    # The test sample need to be a multiple of 6683 as well\n",
    "    batch_size=2\n",
    "    model_conv = Sequential()\n",
    "    # In stateful, we have to pass batch_input_shape to the first layer\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], \n",
    "                                             trainable=False,batch_input_shape=(batch_size,max_len_class)))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32,stateful=True,return_sequences=True))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    #sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    #model_conv.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_conv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get the multiplication factor. We can get it back from the essay set, but\n",
    "# it is better to do it from the score, because there are scores with value 0, and its MF should be 0\n",
    "\n",
    "def find_mult_factor(x):\n",
    "    \"\"\" Function to find the multiplication factor for denormalizing\"\"\"\n",
    "    if x[1] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.around(x[0]/x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(x):\n",
    "    \"\"\" Function to Denormalize the score\"\"\"\n",
    "    return np.around(x[2] * x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_min_mean_score(df):\n",
    "    \"\"\" Function to find the max, min and rounded mean of sentence scores\"\"\"\n",
    "    new_df = pd.DataFrame(columns=['essay_id','essay_set','Orig_Score','Max_Score','Min_Score','Mean_Score'])\n",
    "    essay_ids = np.unique(df.essay_id)\n",
    "    for e_id in essay_ids:\n",
    "        df_temp = df[df.essay_id == e_id]\n",
    "        max_score = np.max(df_temp.pred_score)\n",
    "        min_score = np.min(df_temp.pred_score)\n",
    "        # we need to round the mean so that kappa score doesnt complain\n",
    "        mean_score = np.around(np.mean(df_temp.pred_score))\n",
    "        new_df = new_df.append({'essay_id':e_id,'essay_set':int(np.unique(df_temp.essay_set)),\n",
    "                                'Orig_Score':int(np.unique(df_temp.orig_score)),'Max_Score':max_score,\n",
    "                                'Min_Score':min_score,'Mean_Score':mean_score},ignore_index=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the per essay set scores\n",
    "\n",
    "def essay_set_metrics(df):\n",
    "    \"\"\" Calculate per essay set metrics\"\"\"\n",
    "    set_df = pd.DataFrame(columns=['essay_set','RMSE','Kappa','Accuracy'])\n",
    "    e_sets = np.unique(df.essay_set)\n",
    "    for e_s in e_sets:\n",
    "        df_s = df[df.essay_set == e_s]\n",
    "        original_score = df_s.Orig_Score.values.astype(int)\n",
    "        predicted_score = df_s.Mean_Score.values.astype(int)\n",
    "        rmse = RMSE(original_score,predicted_score)\n",
    "        kappa = cohen_kappa_score(original_score,predicted_score)\n",
    "        accuracy = accuracy_score(original_score,predicted_score)\n",
    "        set_df = set_df.append({'essay_set':e_s,'RMSE':rmse,'Kappa':kappa,'Accuracy':accuracy},\n",
    "                              ignore_index=True)\n",
    "    return set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_count(df):\n",
    "    \"\"\" Returns the number of sentences in an essay \"\"\"\n",
    "    essay_count = df.groupby('essay_id').count()\n",
    "    essay_count = essay_count.drop(['sentence','domain1_score','Norm_Score'],axis=1)\n",
    "    essay_count.columns = ['Number_of_Sentences']\n",
    "    return essay_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147647/147647 [==============================] - 40s 269us/step - loss: 0.0383 - acc: 0.0980\n",
      "Epoch 2/20\n",
      "147647/147647 [==============================] - 26s 176us/step - loss: 0.0288 - acc: 0.1003\n",
      "Epoch 3/20\n",
      "147647/147647 [==============================] - 26s 178us/step - loss: 0.0280 - acc: 0.1007\n",
      "Epoch 4/20\n",
      "147647/147647 [==============================] - 26s 178us/step - loss: 0.0273 - acc: 0.1012\n",
      "Epoch 5/20\n",
      "147647/147647 [==============================] - 26s 177us/step - loss: 0.0265 - acc: 0.1014\n",
      "Epoch 6/20\n",
      "147647/147647 [==============================] - 27s 180us/step - loss: 0.0258 - acc: 0.1018\n",
      "Epoch 7/20\n",
      "147647/147647 [==============================] - 26s 178us/step - loss: 0.0254 - acc: 0.1021\n",
      "Epoch 8/20\n",
      "147647/147647 [==============================] - 27s 181us/step - loss: 0.0246 - acc: 0.1026\n",
      "Epoch 9/20\n",
      "147647/147647 [==============================] - 27s 180us/step - loss: 0.0241 - acc: 0.1029\n",
      "Epoch 10/20\n",
      "147647/147647 [==============================] - 27s 180us/step - loss: 0.0235 - acc: 0.1031\n",
      "Epoch 11/20\n",
      "147647/147647 [==============================] - 26s 174us/step - loss: 0.0229 - acc: 0.1034\n",
      "Epoch 12/20\n",
      "147647/147647 [==============================] - 26s 178us/step - loss: 0.0225 - acc: 0.1036\n",
      "Epoch 13/20\n",
      "147647/147647 [==============================] - 26s 178us/step - loss: 0.0221 - acc: 0.1038\n",
      "Epoch 14/20\n",
      "147647/147647 [==============================] - 26s 179us/step - loss: 0.0217 - acc: 0.1039\n",
      "Epoch 15/20\n",
      "147647/147647 [==============================] - 27s 180us/step - loss: 0.0214 - acc: 0.1041\n",
      "Epoch 16/20\n",
      "147647/147647 [==============================] - 26s 177us/step - loss: 0.0212 - acc: 0.1041\n",
      "Epoch 17/20\n",
      "147647/147647 [==============================] - 26s 177us/step - loss: 0.0209 - acc: 0.1043\n",
      "Epoch 18/20\n",
      "147647/147647 [==============================] - 26s 179us/step - loss: 0.0205 - acc: 0.1045\n",
      "Epoch 19/20\n",
      "147647/147647 [==============================] - 26s 178us/step - loss: 0.0202 - acc: 0.1045\n",
      "Epoch 20/20\n",
      "147647/147647 [==============================] - 27s 183us/step - loss: 0.0201 - acc: 0.1047\n",
      "RMSE before denormalizing:  0.1737379978477365\n"
     ]
    }
   ],
   "source": [
    "# Train & Predict\n",
    "estimator = KerasRegressor(build_fn=CNN_lstm, epochs=20, batch_size=500)\n",
    "estimator.fit(train_data, y_train)\n",
    "prediction_cnn_glove=estimator.predict(test_data)\n",
    "# Metric before denormalizing. This is not very useful as the scale per essay set is different\n",
    "rmse_val = RMSE(y_test,prediction_cnn_glove)\n",
    "print(\"RMSE before denormalizing: \",rmse_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  2.6965874406799935\n",
      "Cohen Kappa:  0.26002881898405206\n",
      "Accuracy:  0.32608815578970596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:12: FutureWarning:\n",
      "\n",
      "Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: FutureWarning:\n",
      "\n",
      "Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Denormalize\n",
    "\n",
    "# Construct pandas data frame of scores of each sentences\n",
    "cnn_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_cnn_glove.astype(np.double)]).transpose()\n",
    "cnn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "# Find Multiplication factor using the function we defined before\n",
    "cnn_df['Mult_Factor'] = cnn_df.apply(find_mult_factor,axis=1)\n",
    "cnn_df['Denorm_Pred_Score'] = cnn_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores of each sentences\n",
    "orig_score = cnn_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = cnn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "rmse_cnn = RMSE(orig_score,pred_score)\n",
    "\n",
    "# Provide the metrics at sentence level\n",
    "print(\"RMSE: \",rmse_cnn)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score)\n",
    "print(\"Cohen Kappa: \",cohen_kappa)\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### CNN_LSTM Results : MAX Score of all Sentences ###\n",
      "RMSE:  3.1392318454285677\n",
      "Cohen Kappa:  0.17031510015035434\n",
      "Accuracy:  0.262326656394453\n",
      "### CNN_LSTM Results : Min Score of all Sentences ###\n",
      "RMSE:  2.8175107307741944\n",
      "Cohen Kappa:  0.2586769081038869\n",
      "Accuracy:  0.35824345146379044\n",
      "### CNN_LSTM Results : Mean Score of all Sentences ###\n",
      "RMSE:  2.136970048019511\n",
      "Cohen Kappa:  0.2707089966704237\n",
      "Accuracy:  0.3647919876733436\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.454219</td>\n",
       "      <td>0.042462</td>\n",
       "      <td>0.240437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.869577</td>\n",
       "      <td>0.072927</td>\n",
       "      <td>0.476712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.769659</td>\n",
       "      <td>0.081901</td>\n",
       "      <td>0.434018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.643650</td>\n",
       "      <td>0.408171</td>\n",
       "      <td>0.594286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.007968</td>\n",
       "      <td>0.067819</td>\n",
       "      <td>0.338667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.884122</td>\n",
       "      <td>0.072215</td>\n",
       "      <td>0.474394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.695649</td>\n",
       "      <td>-0.003495</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4.949394</td>\n",
       "      <td>0.007603</td>\n",
       "      <td>0.048951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_set      RMSE     Kappa  Accuracy\n",
       "0        1.0  1.454219  0.042462  0.240437\n",
       "1        2.0  0.869577  0.072927  0.476712\n",
       "2        3.0  0.769659  0.081901  0.434018\n",
       "3        4.0  0.643650  0.408171  0.594286\n",
       "4        5.0  1.007968  0.067819  0.338667\n",
       "5        6.0  0.884122  0.072215  0.474394\n",
       "6        7.0  4.695649 -0.003495  0.066667\n",
       "7        8.0  4.949394  0.007603  0.048951"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,cnn_df.Orig_Score.values,\n",
    "                          cnn_df.Denorm_Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','pred_score']\n",
    "cnn_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "cnn_test_df.essay_id = cnn_test_df.essay_id.astype(int)\n",
    "cnn_test_df.essay_set = cnn_test_df.essay_set.astype(int)\n",
    "cnn_test_df.Orig_Score = cnn_test_df.Orig_Score.astype(int)\n",
    "cnn_test_df.Max_Score = cnn_test_df.Max_Score.astype(int)\n",
    "cnn_test_df.Min_Score = cnn_test_df.Min_Score.astype(int)\n",
    "cnn_test_df.Mean_Score = cnn_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = cnn_test_df.Orig_Score.values\n",
    "max_pred_score = cnn_test_df.Max_Score.values\n",
    "min_pred_score = cnn_test_df.Min_Score.values\n",
    "mean_pred_score = cnn_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_cnn = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_cnn = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_cnn = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_cnn = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_cnn = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_cnn = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Rounded Mean score of all sentences are taken\n",
    "rmse_mean_cnn = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_cnn = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "accuracy_mean_cnn = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### CNN_LSTM Results : MAX Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_cnn)\n",
    "print(\"Accuracy: \",accuracy_max_cnn)\n",
    "\n",
    "print(\"### CNN_LSTM Results : Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_cnn)\n",
    "print(\"Accuracy: \",accuracy_min_cnn)\n",
    "\n",
    "print(\"### CNN_LSTM Results : Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_cnn)\n",
    "print(\"Accuracy: \",accuracy_mean_cnn)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(cnn_test_df)\n",
    "essay_set_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147647/147647 [==============================] - 315s 2ms/step - loss: 0.0462 - acc: 0.0960\n",
      "Epoch 2/20\n",
      "147647/147647 [==============================] - 309s 2ms/step - loss: 0.0307 - acc: 0.0999\n",
      "Epoch 3/20\n",
      " 61500/147647 [===========>..................] - ETA: 2:59 - loss: 0.0287 - acc: 0.1024"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-48df3db2e356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mestimator_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstack_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mestimator_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Predict for test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator_lstm = KerasRegressor(build_fn=stack_lstm, epochs=20, batch_size=500)\n",
    "estimator_lstm.fit(train_data, y_train)\n",
    "\n",
    "# Predict for test data\n",
    "prediction_lstm=estimator_lstm.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_lstm)\n",
    "# Find the overall RMSE value. This is not very relevant considering different scale for each set.\n",
    "print(\"RMSE: \",rmse_val)\n",
    "\n",
    "# Construct pandas data frame of scores of each sentences\n",
    "lstm_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_lstm.astype(np.double)]).transpose()\n",
    "lstm_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "# Find Multiplication factor using the function we defined before\n",
    "lstm_df['Mult_Factor'] = lstm_df.apply(find_mult_factor,axis=1)\n",
    "# Find the denormalized predicted score\n",
    "lstm_df['Denorm_Pred_Score'] = lstm_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores of each sentences\n",
    "orig_score = lstm_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = lstm_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "\n",
    "# Provide the metrics at sentence level\n",
    "rmse_lstm = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score)\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "print(\"RMSE Sentence Level: \",rmse_lstm)\n",
    "print(\"Kappa Sentence Level: \",cohen_kappa)\n",
    "print(\"Accuracy Sentence Level: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,lstm_df.Orig_Score.values,\n",
    "                          lstm_df.Denorm_Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','pred_score']\n",
    "lstm_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "lstm_test_df.essay_id = lstm_test_df.essay_id.astype(int)\n",
    "lstm_test_df.essay_set = lstm_test_df.essay_set.astype(int)\n",
    "lstm_test_df.Orig_Score = lstm_test_df.Orig_Score.astype(int)\n",
    "lstm_test_df.Max_Score = lstm_test_df.Max_Score.astype(int)\n",
    "lstm_test_df.Min_Score = lstm_test_df.Min_Score.astype(int)\n",
    "lstm_test_df.Mean_Score = lstm_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = lstm_test_df.Orig_Score.values\n",
    "max_pred_score = lstm_test_df.Max_Score.values\n",
    "min_pred_score = lstm_test_df.Min_Score.values\n",
    "mean_pred_score = lstm_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_lstm = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_lstm = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_lstm = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_lstm = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_lstm = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_lstm = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_lstm = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_lstm = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "accuracy_mean_lstm = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### Stacked LSTM Results : MAX Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_lstm)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_lstm)\n",
    "print(\"Accuracy: \",accuracy_max_lstm)\n",
    "\n",
    "print(\"### Stacked LSTM Results : Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_lstm)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_lstm)\n",
    "print(\"Accuracy: \",accuracy_min_lstm)\n",
    "\n",
    "print(\"### Stacked LSTM Results : Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_lstm)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_lstm)\n",
    "print(\"Accuracy: \",accuracy_mean_lstm)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(lstm_test_df)\n",
    "essay_set_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147647/147647 [==============================] - 229s 2ms/step - loss: 0.1327 - acc: 0.0923\n",
      "Epoch 2/20\n",
      "147647/147647 [==============================] - 239s 2ms/step - loss: 0.0310 - acc: 0.0999\n",
      "Epoch 3/20\n",
      "147647/147647 [==============================] - 238s 2ms/step - loss: 0.0299 - acc: 0.1001\n",
      "Epoch 4/20\n",
      "147647/147647 [==============================] - 240s 2ms/step - loss: 0.0293 - acc: 0.1003\n",
      "Epoch 5/20\n",
      "147647/147647 [==============================] - 229s 2ms/step - loss: 0.0287 - acc: 0.1004\n",
      "Epoch 6/20\n",
      "147647/147647 [==============================] - 228s 2ms/step - loss: 0.0284 - acc: 0.1006\n",
      "Epoch 7/20\n",
      "147647/147647 [==============================] - 228s 2ms/step - loss: 0.0279 - acc: 0.1007\n",
      "Epoch 8/20\n",
      "147647/147647 [==============================] - 227s 2ms/step - loss: 0.0276 - acc: 0.1010\n",
      "Epoch 9/20\n",
      "147647/147647 [==============================] - 227s 2ms/step - loss: 0.0274 - acc: 0.1010\n",
      "Epoch 10/20\n",
      "147647/147647 [==============================] - 232s 2ms/step - loss: 0.0271 - acc: 0.1013\n",
      "Epoch 11/20\n",
      "147647/147647 [==============================] - 228s 2ms/step - loss: 0.0268 - acc: 0.1013\n",
      "Epoch 12/20\n",
      "147647/147647 [==============================] - 229s 2ms/step - loss: 0.0264 - acc: 0.1016\n",
      "Epoch 13/20\n",
      "147647/147647 [==============================] - 227s 2ms/step - loss: 0.0262 - acc: 0.1017\n",
      "Epoch 14/20\n",
      "147647/147647 [==============================] - 232s 2ms/step - loss: 0.0259 - acc: 0.1019\n",
      "Epoch 15/20\n",
      "147647/147647 [==============================] - 237s 2ms/step - loss: 0.0256 - acc: 0.1021\n",
      "Epoch 16/20\n",
      "147647/147647 [==============================] - 239s 2ms/step - loss: 0.0253 - acc: 0.1021\n",
      "Epoch 17/20\n",
      "147647/147647 [==============================] - 237s 2ms/step - loss: 0.0249 - acc: 0.1025\n",
      "Epoch 18/20\n",
      "147647/147647 [==============================] - 232s 2ms/step - loss: 0.0249 - acc: 0.1025\n",
      "Epoch 19/20\n",
      "147647/147647 [==============================] - 231s 2ms/step - loss: 0.0245 - acc: 0.1027\n",
      "Epoch 20/20\n",
      "147647/147647 [==============================] - 232s 2ms/step - loss: 0.0243 - acc: 0.1029\n",
      "RMSE:  0.16804032743322922\n",
      "RMSE Sentence Level:  2.6571219006354285\n",
      "Kappa Sentence Level:  0.2640794006154953\n",
      "Accuracy Sentence Level:  0.33077729354002317\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator = KerasRegressor(build_fn=CNN_glove_1, epochs=20, batch_size=500)\n",
    "estimator.fit(train_data, y_train)\n",
    "\n",
    "# Predict for test data\n",
    "prediction_rnn=estimator.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_rnn)\n",
    "# Find the overall RMSE value. This is not very relevant considering different scale for each set.\n",
    "print(\"RMSE: \",rmse_val)\n",
    "\n",
    "# Construct pandas data frame of scores of each sentences\n",
    "rnn_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_rnn.astype(np.double)]).transpose()\n",
    "rnn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "# Find Multiplication factor using the function we defined before\n",
    "rnn_df['Mult_Factor'] = rnn_df.apply(find_mult_factor,axis=1)\n",
    "# Find the denormalized predicted score\n",
    "rnn_df['Denorm_Pred_Score'] = rnn_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores of each sentences\n",
    "orig_score = rnn_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = rnn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "\n",
    "# Provide the metrics at sentence level\n",
    "rmse_rnn = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score)\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "print(\"RMSE Sentence Level: \",rmse_rnn)\n",
    "print(\"Kappa Sentence Level: \",cohen_kappa)\n",
    "print(\"Accuracy Sentence Level: \",accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### RNN GRU Results : MAX Score of all Sentences ###\n",
      "RMSE:  2.766462143045881\n",
      "Cohen Kappa:  0.23434395207267844\n",
      "Accuracy:  0.32704160246533126\n",
      "### RNN GRU Results : Min Score of all Sentences ###\n",
      "RMSE:  2.7515229009165276\n",
      "Cohen Kappa:  0.24823282567822424\n",
      "Accuracy:  0.3501540832049307\n",
      "### RNN GRU Results : Mean Score of all Sentences ###\n",
      "RMSE:  2.05284574550881\n",
      "Cohen Kappa:  0.29721906803674936\n",
      "Accuracy:  0.3898305084745763\n",
      "Count:  366\n",
      "pe: 0.2415494640031055\n",
      "Count:  365\n",
      "pe: 0.43092512666541566\n",
      "Count:  341\n",
      "pe: 0.3934262691239325\n",
      "Count:  350\n",
      "pe: 0.3190938775510204\n",
      "Count:  375\n",
      "pe: 0.29968355555555554\n",
      "Count:  371\n",
      "pe: 0.43327932810717734\n",
      "Count:  285\n",
      "pe: 0.07799322868574945\n",
      "Count:  143\n",
      "pe: 0.05027140691476356\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.406464</td>\n",
       "      <td>0.059774</td>\n",
       "      <td>0.286885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.797256</td>\n",
       "      <td>0.167117</td>\n",
       "      <td>0.526027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.765840</td>\n",
       "      <td>0.047581</td>\n",
       "      <td>0.422287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.614120</td>\n",
       "      <td>0.458704</td>\n",
       "      <td>0.631429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.930949</td>\n",
       "      <td>0.116590</td>\n",
       "      <td>0.381333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.849925</td>\n",
       "      <td>0.110597</td>\n",
       "      <td>0.495957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.526724</td>\n",
       "      <td>-0.012285</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4.744522</td>\n",
       "      <td>-0.023480</td>\n",
       "      <td>0.027972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_set      RMSE     Kappa  Accuracy\n",
       "0        1.0  1.406464  0.059774  0.286885\n",
       "1        2.0  0.797256  0.167117  0.526027\n",
       "2        3.0  0.765840  0.047581  0.422287\n",
       "3        4.0  0.614120  0.458704  0.631429\n",
       "4        5.0  0.930949  0.116590  0.381333\n",
       "5        6.0  0.849925  0.110597  0.495957\n",
       "6        7.0  4.526724 -0.012285  0.066667\n",
       "7        8.0  4.744522 -0.023480  0.027972"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,rnn_df.Orig_Score.values,\n",
    "                          rnn_df.Denorm_Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','pred_score']\n",
    "rnn_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "rnn_test_df.essay_id = rnn_test_df.essay_id.astype(int)\n",
    "rnn_test_df.essay_set = rnn_test_df.essay_set.astype(int)\n",
    "rnn_test_df.Orig_Score = rnn_test_df.Orig_Score.astype(int)\n",
    "rnn_test_df.Max_Score = rnn_test_df.Max_Score.astype(int)\n",
    "rnn_test_df.Min_Score = rnn_test_df.Min_Score.astype(int)\n",
    "rnn_test_df.Mean_Score = rnn_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = rnn_test_df.Orig_Score.values\n",
    "max_pred_score = rnn_test_df.Max_Score.values\n",
    "min_pred_score = rnn_test_df.Min_Score.values\n",
    "mean_pred_score = rnn_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_rnn = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_rnn = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_rnn = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_rnn = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_rnn = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_rnn = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_rnn = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_rnn = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "accuracy_mean_rnn = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### RNN GRU Results : MAX Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_rnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_rnn)\n",
    "print(\"Accuracy: \",accuracy_max_rnn)\n",
    "\n",
    "print(\"### RNN GRU Results : Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_rnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_rnn)\n",
    "print(\"Accuracy: \",accuracy_min_rnn)\n",
    "\n",
    "print(\"### RNN GRU Results : Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_rnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_rnn)\n",
    "print(\"Accuracy: \",accuracy_mean_rnn)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(rnn_test_df)\n",
    "essay_set_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed Forward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.17686244661930914\n",
      "RMSE Sentence Level:  2.872749442331444\n",
      "Kappa Sentence Level:  0.25184272299747656\n",
      "Accuracy Sentence Level:  0.31877861753185854\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator_nn = KerasRegressor(build_fn=FF_NN, epochs=20, batch_size=500)\n",
    "estimator_nn.fit(train_data, y_train)\n",
    "\n",
    "# Predict for test data\n",
    "prediction_nn=estimator_nn.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_nn)\n",
    "# Find the overall RMSE value. This is not very relevant considering different scale for each set.\n",
    "print(\"RMSE: \",rmse_val)\n",
    "\n",
    "# Construct pandas data frame of scores of each sentences\n",
    "nn_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_nn.astype(np.double)]).transpose()\n",
    "nn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "# Find Multiplication factor using the function we defined before\n",
    "nn_df['Mult_Factor'] = nn_df.apply(find_mult_factor,axis=1)\n",
    "# Find the denormalized predicted score\n",
    "nn_df['Denorm_Pred_Score'] = nn_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores of each sentences\n",
    "orig_score = nn_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = nn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "\n",
    "# Provide the metrics at sentence level\n",
    "rmse_nn = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score)\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "print(\"RMSE Sentence Level: \",rmse_nn)\n",
    "print(\"Kappa Sentence Level: \",cohen_kappa)\n",
    "print(\"Accuracy Sentence Level: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Simple FF NN Results : MAX Score of all Sentences ###\n",
      "RMSE:  3.105241852828997\n",
      "Cohen Kappa:  0.20269573315671097\n",
      "Accuracy:  0.29622496147919875\n",
      "### Simple FF NN Results : Min Score of all Sentences ###\n",
      "RMSE:  3.3593807669173277\n",
      "Cohen Kappa:  0.185911899786948\n",
      "Accuracy:  0.2935285053929122\n",
      "### Simple FF NN Results : Mean Score of all Sentences ###\n",
      "RMSE:  2.002213720930788\n",
      "Cohen Kappa:  0.2981514485881216\n",
      "Accuracy:  0.39175654853620956\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.393780</td>\n",
       "      <td>0.081675</td>\n",
       "      <td>0.311475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.792084</td>\n",
       "      <td>0.193879</td>\n",
       "      <td>0.539726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.756206</td>\n",
       "      <td>0.074548</td>\n",
       "      <td>0.436950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.639196</td>\n",
       "      <td>0.411744</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.001332</td>\n",
       "      <td>0.067606</td>\n",
       "      <td>0.341333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.849925</td>\n",
       "      <td>0.094471</td>\n",
       "      <td>0.487871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.451690</td>\n",
       "      <td>0.018524</td>\n",
       "      <td>0.101754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>4.454117</td>\n",
       "      <td>0.013742</td>\n",
       "      <td>0.062937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_set      RMSE     Kappa  Accuracy\n",
       "0        1.0  1.393780  0.081675  0.311475\n",
       "1        2.0  0.792084  0.193879  0.539726\n",
       "2        3.0  0.756206  0.074548  0.436950\n",
       "3        4.0  0.639196  0.411744  0.600000\n",
       "4        5.0  1.001332  0.067606  0.341333\n",
       "5        6.0  0.849925  0.094471  0.487871\n",
       "6        7.0  4.451690  0.018524  0.101754\n",
       "7        8.0  4.454117  0.013742  0.062937"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,nn_df.Orig_Score.values,\n",
    "                          nn_df.Denorm_Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','pred_score']\n",
    "nn_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "nn_test_df.essay_id = nn_test_df.essay_id.astype(int)\n",
    "nn_test_df.essay_set = nn_test_df.essay_set.astype(int)\n",
    "nn_test_df.Orig_Score = nn_test_df.Orig_Score.astype(int)\n",
    "nn_test_df.Max_Score = nn_test_df.Max_Score.astype(int)\n",
    "nn_test_df.Min_Score = nn_test_df.Min_Score.astype(int)\n",
    "nn_test_df.Mean_Score = nn_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = nn_test_df.Orig_Score.values\n",
    "max_pred_score = nn_test_df.Max_Score.values\n",
    "min_pred_score = nn_test_df.Min_Score.values\n",
    "mean_pred_score = nn_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_nn = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_nn = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_nn = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_nn = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_nn = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_nn = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_nn = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_nn = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "accuracy_mean_nn = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### Simple FF NN Results : MAX Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_nn)\n",
    "print(\"Accuracy: \",accuracy_max_nn)\n",
    "\n",
    "print(\"### Simple FF NN Results : Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_nn)\n",
    "print(\"Accuracy: \",accuracy_min_nn)\n",
    "\n",
    "print(\"### Simple FF NN Results : Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_nn)\n",
    "print(\"Accuracy: \",accuracy_mean_nn)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(nn_test_df)\n",
    "essay_set_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
