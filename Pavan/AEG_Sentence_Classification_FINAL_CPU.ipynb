{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "import json, os, re, shutil, sys, time\n",
    "import seaborn as sns\n",
    "from importlib import reload\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "from collections import defaultdict\n",
    "import xmltodict\n",
    "import untangle\n",
    "import xml.etree.ElementTree as ET\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Import spacy\n",
    "import spacy\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# NumPy, Pandas and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "\n",
    "from numpy.random import seed\n",
    "from pandas import read_csv, DataFrame\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Activation, Dropout, Conv1D, MaxPooling1D, Bidirectional, Flatten, TimeDistributed\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import keras.backend as K\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AEG Long Essay Sentence Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "aeg_long = pd.read_csv(\"../data-DNC/AEG/training_set_rel3.tsv\",sep='\\t',encoding = \"latin1\")\n",
    "#aeg_long.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test. We need to do this first to ensure that when we split to \n",
    "# sentence level, we have sentences of a given essay in either training or test but not on both.\n",
    "\n",
    "train_comb,test_comb = train_test_split(aeg_long,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10380, 5)\n",
      "(2596, 5)\n",
      "[ 3567  4233  6518 15174 18855]\n",
      "[ 9908  9872   305 12771  6839]\n"
     ]
    }
   ],
   "source": [
    "# Create train and test data frames with relevant fields\n",
    "\n",
    "x_train_df = train_comb.filter(['essay_id','essay_set','essay','domain1_score'], axis=1)\n",
    "x_test_df = test_comb.filter(['essay_id','essay_set','essay','domain1_score'], axis=1)\n",
    "\n",
    "x_train_df = x_train_df.reset_index()\n",
    "x_test_df = x_test_df.reset_index()\n",
    "print(x_train_df.shape)\n",
    "print(x_test_df.shape)\n",
    "print(x_train_df.essay_id[:5].values)\n",
    "print(x_test_df.essay_id[:5].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split essay into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will use spacy to split the essay into sentences. \n",
    "# Load spacy large english module\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to split essay into sentences\n",
    "def create_sentences_df(df):\n",
    "    \"\"\" Function to split essay data into individual sentences. Returns a dataframe\"\"\"\n",
    "    start=datetime.now()\n",
    "    aeg_long_sentence = pd.DataFrame(columns=['essay_id','essay_set','sentence','domain1_score'])\n",
    "    for i in range(len(df)):\n",
    "        if i%1000 == 0:\n",
    "            print(\"At iteration :\",i)\n",
    "            print(\"Duration: \",datetime.now()-start)\n",
    "        sentence = nlp(df.essay[i])\n",
    "        for s in sentence.sents:\n",
    "            aeg_long_sentence = aeg_long_sentence.append({'essay_id' : df.essay_id[i],\n",
    "                                                          'essay_set' : df.essay_set[i],'sentence' : s.text, \n",
    "                                                          'domain1_score' : df.domain1_score[i]},\n",
    "                                                         ignore_index=True)\n",
    "    return aeg_long_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration : 0\n",
      "Duration:  0:00:00.007296\n",
      "At iteration : 1000\n",
      "Duration:  0:01:23.314513\n",
      "At iteration : 2000\n",
      "Duration:  0:03:12.908393\n",
      "At iteration : 3000\n",
      "Duration:  0:05:40.725304\n",
      "At iteration : 4000\n",
      "Duration:  0:09:21.342668\n",
      "At iteration : 5000\n",
      "Duration:  0:13:23.004936\n",
      "At iteration : 6000\n",
      "Duration:  0:18:27.577065\n",
      "At iteration : 7000\n",
      "Duration:  0:23:38.768215\n",
      "At iteration : 8000\n",
      "Duration:  0:29:45.614656\n",
      "At iteration : 9000\n",
      "Duration:  0:36:41.560594\n",
      "At iteration : 10000\n",
      "Duration:  0:44:11.900203\n"
     ]
    }
   ],
   "source": [
    "# Split train data into sentences\n",
    "x_train_sentence_df = create_sentences_df(x_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration : 0\n",
      "Duration:  0:00:00.005520\n",
      "At iteration : 1000\n",
      "Duration:  0:01:16.552397\n",
      "At iteration : 2000\n",
      "Duration:  0:02:52.433178\n"
     ]
    }
   ],
   "source": [
    "# Split test data into sentences\n",
    "x_test_sentence_df = create_sentences_df(x_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each essay set has a different scoring range. We need to normalize the scores to a standard scale for training.\n",
    "def normalize_score(essay):\n",
    "    \"\"\" Normalizes the domain score based on percentage\"\"\"\n",
    "    score = 0\n",
    "    score = float(essay[3])\n",
    "    essay_set = essay[1]\n",
    "    if essay_set == 1:\n",
    "        div = 12\n",
    "    elif essay_set == 2:\n",
    "        div = 5\n",
    "    elif essay_set == 3:\n",
    "        div = 3\n",
    "    elif essay_set == 4:\n",
    "        div = 3\n",
    "    elif essay_set == 5:\n",
    "        div = 4\n",
    "    elif essay_set == 6:\n",
    "        div = 4\n",
    "    elif essay_set == 7:\n",
    "        div = 25\n",
    "    elif essay_set == 8:\n",
    "        div = 50\n",
    "    return score/div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_sentence_df['Norm_Score'] = x_train_sentence_df.apply(normalize_score,axis=1)\n",
    "x_test_sentence_df['Norm_Score'] = x_test_sentence_df.apply(normalize_score,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>sentence</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>Norm_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3567</td>\n",
       "      <td>2</td>\n",
       "      <td>There are many types of reading materials for ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3567</td>\n",
       "      <td>2</td>\n",
       "      <td>You can find things on cars, trucks, sports, a...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3567</td>\n",
       "      <td>2</td>\n",
       "      <td>There are some materials in a library though t...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3567</td>\n",
       "      <td>2</td>\n",
       "      <td>But should those materials be removed from the...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3567</td>\n",
       "      <td>2</td>\n",
       "      <td>Some think that they should and others think t...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id essay_set                                           sentence  \\\n",
       "0     3567         2  There are many types of reading materials for ...   \n",
       "1     3567         2  You can find things on cars, trucks, sports, a...   \n",
       "2     3567         2  There are some materials in a library though t...   \n",
       "3     3567         2  But should those materials be removed from the...   \n",
       "4     3567         2  Some think that they should and others think t...   \n",
       "\n",
       "  domain1_score  Norm_Score  \n",
       "0             4         0.8  \n",
       "1             4         0.8  \n",
       "2             4         0.8  \n",
       "3             4         0.8  \n",
       "4             4         0.8  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_sentence_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store this dataset in pickle format so that we don't have to redo the above steps.\n",
    "x_train_sentence_df.to_pickle(\"./x_train_sentence_df.pkl\")\n",
    "x_test_sentence_df.to_pickle(\"./x_test_sentence_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147123, 5)\n",
      "(36778, 5)\n"
     ]
    }
   ],
   "source": [
    "# Open the pickled version\n",
    "x_train_sentence_df = pd.read_pickle(\"./x_train_sentence_df.pkl\")\n",
    "x_test_sentence_df = pd.read_pickle(\"./x_test_sentence_df.pkl\")\n",
    "print(x_train_sentence_df.shape)\n",
    "print(x_test_sentence_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def append_sentence(sid,sentence):\n",
    "#    sentence = sid_dict[sid] + sentence\n",
    "#    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sid_dict = {1:\"One. \",2:\"Two. \",3:\"Three. \",4:\"Four. \",5:\"Five \",6:\"Six. \",7:\"Seven. \",8:\"Eight. \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train_sentence_df['mod_sentence'] = np.vectorize(append_sentence)(x_train_sentence_df['essay_set'],\n",
    "#                                                                    x_train_sentence_df['sentence'])\n",
    "\n",
    "#x_test_sentence_df['mod_sentence'] = np.vectorize(append_sentence)(x_test_sentence_df['essay_set'],\n",
    "#                                                                    x_test_sentence_df['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_sentence_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147123,)\n",
      "(147123,)\n",
      "(36778,)\n",
      "(36778,)\n"
     ]
    }
   ],
   "source": [
    "# Create train and test text and labels\n",
    "\n",
    "x_train = x_train_sentence_df['sentence'].values\n",
    "y_train = x_train_sentence_df['Norm_Score'].values\n",
    "x_test = x_test_sentence_df['sentence'].values\n",
    "y_test = x_test_sentence_df['Norm_Score'].values\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 50000\n",
    "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "train_data = pad_sequences(train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len_class = train_data.shape[1]\n",
    "max_words_class = vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36778, 175)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "test_data = pad_sequences(test_seq, maxlen=max_len_class)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('/Users/kurapati/W266/data/glove.42B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common Functions:\n",
    "\n",
    "# Define function to get the multiplication factor. We can get it back from the essay set, but\n",
    "# it is better to do it from the score, because there are scores with value 0, and its MF should be 0\n",
    "\n",
    "def find_mult_factor(x):\n",
    "    \"\"\" Function to find the multiplication factor for denormalizing\"\"\"\n",
    "    if x[1] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.around(x[0]/x[1])\n",
    "    \n",
    "def denormalize(x):\n",
    "    \"\"\" Function to Denormalize the score\"\"\"\n",
    "    return np.around(x[2] * x[3])\n",
    "\n",
    "def find_max_min_mean_score(df):\n",
    "    \"\"\" Function to find the max, min and rounded mean of sentence scores\"\"\"\n",
    "    new_df = pd.DataFrame(columns=['essay_id','essay_set','Orig_Score','Max_Score',\n",
    "                                   'Min_Score','Mean_Score'])\n",
    "    essay_ids = np.unique(df.essay_id)\n",
    "    for e_id in essay_ids:\n",
    "        df_temp = df[df.essay_id == e_id]\n",
    "        max_score = np.max(df_temp.pred_score)\n",
    "        min_score = np.min(df_temp.pred_score)\n",
    "        # we need to round the mean so that kappa score doesnt complain\n",
    "        mean_score = np.around(np.mean(df_temp.pred_score))\n",
    "        new_df = new_df.append({'essay_id':e_id,'essay_set':int(np.unique(df_temp.essay_set)),\n",
    "                                'Orig_Score':int(np.unique(df_temp.orig_score)),\n",
    "                                'Max_Score':max_score,'Min_Score':min_score,\n",
    "                                'Mean_Score':mean_score},ignore_index=True)\n",
    "    return new_df\n",
    "\n",
    "def essay_set_metrics(df):\n",
    "    \"\"\" Calculate per essay set metrics\"\"\"\n",
    "    set_df = pd.DataFrame(columns=['essay_set','RMSE','Kappa','Kappa_Q','Accuracy'])\n",
    "    e_sets = np.unique(df.essay_set)\n",
    "    for e_s in e_sets:\n",
    "        df_s = df[df.essay_set == e_s]\n",
    "        original_score = df_s.Orig_Score.values.astype(int)\n",
    "        predicted_score = df_s.Mean_Score.values.astype(int)\n",
    "        rmse = RMSE(original_score,predicted_score)\n",
    "        kappa = cohen_kappa_score(original_score,predicted_score)\n",
    "        kappa_q = cohen_kappa_score(original_score,predicted_score,weights='quadratic')\n",
    "        accuracy = accuracy_score(original_score,predicted_score)\n",
    "        set_df = set_df.append({'essay_set':e_s,'RMSE':rmse,'Kappa':kappa,'Kappa_Q':kappa_q,\n",
    "                                'Accuracy':accuracy},ignore_index=True)\n",
    "    return set_df\n",
    "\n",
    "\n",
    "def sentence_count(df):\n",
    "    \"\"\" Returns the number of sentences in an essay \"\"\"\n",
    "    essay_count = df.groupby('essay_id').count()\n",
    "    essay_count = essay_count.drop(['sentence','domain1_score','Norm_Score'],axis=1)\n",
    "    essay_count.columns = ['Number_of_Sentences']\n",
    "    return essay_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSE(actual, predict):\n",
    "    diff = actual - predict\n",
    "    diff = sum(diff**2) / len(actual)\n",
    "    return np.sqrt(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model definitions\n",
    "\n",
    "def FF_NN():\n",
    "    \"\"\" Simple feed forward NN\"\"\"\n",
    "    model_ff = Sequential()\n",
    "    model_ff.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_ff.add(tf.keras.layers.Flatten())\n",
    "    model_ff.add(tf.keras.layers.Dense(50,activation='tanh'))\n",
    "    model_ff.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_ff.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
    "    model_ff.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_ff\n",
    "\n",
    "def GRU():\n",
    "    \"\"\" Gated Recurrent Unit\"\"\"\n",
    "    model_gru = Sequential()\n",
    "    model_gru.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_gru.add(tf.keras.layers.GRU(32,activation='tanh'))\n",
    "    model_gru.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_gru.add(tf.keras.layers.Dense(1,name='out_layer'))\n",
    "    model_gru.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_gru\n",
    "\n",
    "def CNN_FF():\n",
    "    \"\"\" CNN with Feed Forward NN \"\"\"\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_conv.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(tf.keras.layers.Flatten())\n",
    "    model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "def CNN_lstm():\n",
    "    \"\"\" CNN with single layer LSTM & Feed Forward NN\"\"\"\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    #model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Conv1D(64, 5, activation='relu'))\n",
    "    model_conv.add(tf.keras.layers.MaxPooling1D(pool_size=4))\n",
    "    model_conv.add(tf.keras.layers.LSTM(100))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "def stack_lstm():\n",
    "    \"\"\" Three layered stacked LSTM.\"\"\"\n",
    "    model_conv = Sequential()\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], trainable=False))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32,return_sequences=True))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32, return_sequences=True))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.2))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    return model_conv\n",
    "\n",
    "def stateful_stacked_lstm():\n",
    "    # In stateful, total samples needs to be divisible by batch size\n",
    "    # we have 147026 samples, so selecting 6683 (6683*22=147026)\n",
    "    # The test sample need to be a multiple of 6683 as well\n",
    "    batch_size=2\n",
    "    model_conv = Sequential()\n",
    "    # In stateful, we have to pass batch_input_shape to the first layer\n",
    "    model_conv.add(tf.keras.layers.Embedding(max_words_class, 300, input_length=max_len_class, weights=[embedding_matrix], \n",
    "                                             trainable=False,batch_input_shape=(batch_size,max_len_class)))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32,stateful=True,return_sequences=True))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.LSTM(32))\n",
    "    model_conv.add(tf.keras.layers.Dropout(0.1))\n",
    "    model_conv.add(tf.keras.layers.Dense(100))\n",
    "    model_conv.add(tf.keras.layers.Dense(1, kernel_initializer='normal'))\n",
    "    #sgd = SGD(lr = 0.1, momentum = 0.9, decay = 0, nesterov = False)\n",
    "    model_conv.compile(loss = 'mse', optimizer = tf.train.AdamOptimizer(), metrics = ['accuracy'])\n",
    "    #model_conv.compile(optimizer=tf.train.AdamOptimizer(),loss='mse',metrics=['accuracy'])\n",
    "    return model_conv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Feed Forward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147123/147123 [==============================] - 37s 248us/step - loss: 0.0468 - acc: 0.0983\n",
      "Epoch 2/20\n",
      "147123/147123 [==============================] - 37s 249us/step - loss: 0.0300 - acc: 0.0990\n",
      "Epoch 3/20\n",
      "147123/147123 [==============================] - 36s 247us/step - loss: 0.0282 - acc: 0.0993\n",
      "Epoch 4/20\n",
      "147123/147123 [==============================] - 36s 244us/step - loss: 0.0270 - acc: 0.1000\n",
      "Epoch 5/20\n",
      "147123/147123 [==============================] - 36s 248us/step - loss: 0.0260 - acc: 0.1007\n",
      "Epoch 6/20\n",
      "147123/147123 [==============================] - 38s 256us/step - loss: 0.0250 - acc: 0.1011\n",
      "Epoch 7/20\n",
      "147123/147123 [==============================] - 36s 248us/step - loss: 0.0241 - acc: 0.1015\n",
      "Epoch 8/20\n",
      "147123/147123 [==============================] - 36s 247us/step - loss: 0.0231 - acc: 0.1020\n",
      "Epoch 9/20\n",
      "147123/147123 [==============================] - 37s 252us/step - loss: 0.0222 - acc: 0.1025\n",
      "Epoch 10/20\n",
      "147123/147123 [==============================] - 36s 247us/step - loss: 0.0213 - acc: 0.1028\n",
      "Epoch 11/20\n",
      "147123/147123 [==============================] - 36s 247us/step - loss: 0.0204 - acc: 0.1030\n",
      "Epoch 12/20\n",
      "147123/147123 [==============================] - 38s 259us/step - loss: 0.0197 - acc: 0.1033\n",
      "Epoch 13/20\n",
      "147123/147123 [==============================] - 35s 235us/step - loss: 0.0190 - acc: 0.1035\n",
      "Epoch 14/20\n",
      "147123/147123 [==============================] - 36s 243us/step - loss: 0.0184 - acc: 0.1036\n",
      "Epoch 15/20\n",
      "147123/147123 [==============================] - 37s 251us/step - loss: 0.0177 - acc: 0.1038\n",
      "Epoch 16/20\n",
      "147123/147123 [==============================] - 36s 243us/step - loss: 0.0171 - acc: 0.1039\n",
      "Epoch 17/20\n",
      "147123/147123 [==============================] - 35s 239us/step - loss: 0.0167 - acc: 0.1041\n",
      "Epoch 18/20\n",
      "147123/147123 [==============================] - 36s 243us/step - loss: 0.0162 - acc: 0.1042\n",
      "Epoch 19/20\n",
      "147123/147123 [==============================] - 35s 239us/step - loss: 0.0157 - acc: 0.1042\n",
      "Epoch 20/20\n",
      "147123/147123 [==============================] - 35s 240us/step - loss: 0.0154 - acc: 0.1044\n",
      "RMSE:  0.17843482393896942\n",
      "**** METRICS BASED ON DENORMALIZED SCORE FOR FEED FORWARD NN****\n",
      "**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\n",
      "RMSE:  3.0158011916323653\n",
      "Kappa Quadratic Weighting:  0.9710287577862428\n",
      "Accuracy:  0.3076295611506879\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator_nn = KerasRegressor(build_fn=FF_NN, epochs=20, batch_size=500)\n",
    "estimator_nn.fit(train_data, y_train)\n",
    "\n",
    "# Predict for test data\n",
    "prediction_nn=estimator_nn.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_nn)\n",
    "# Find the overall RMSE value. This is not very relevant considering different scale for each set.\n",
    "print(\"RMSE: \",rmse_val)\n",
    "\n",
    "# Construct pandas data frame of scores of each sentences\n",
    "nn_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_nn.astype(np.double)]).transpose()\n",
    "nn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "# Find Multiplication factor using the function we defined before\n",
    "nn_df['Mult_Factor'] = nn_df.apply(find_mult_factor,axis=1)\n",
    "# Find the denormalized predicted score\n",
    "nn_df['Denorm_Pred_Score'] = nn_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores of each sentences\n",
    "orig_score = nn_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = nn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "\n",
    "# Provide the metrics at sentence level\n",
    "rmse_nn = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score,weights='quadratic')\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "print(\"**** METRICS BASED ON DENORMALIZED SCORE FOR FEED FORWARD NN****\")\n",
    "print(\"**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\")\n",
    "print(\"RMSE: \",rmse_nn)\n",
    "print(\"Kappa Quadratic Weighting: \",cohen_kappa)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Simple FF NN : Essay level Metrics : After taking MAX score all Sentences ###\n",
      "RMSE:  3.1247187854979463\n",
      "Cohen Kappa:  0.19024697334778928\n",
      "Accuracy:  0.28197226502311246\n",
      "### Simple FF NN : Essay level Metrics : After taking Min Score of all Sentences ###\n",
      "RMSE:  3.4116066731746675\n",
      "Cohen Kappa:  0.2087755187967566\n",
      "Accuracy:  0.31124807395993837\n",
      "### Simple FF NN : Essay level Metrics : After taking Mean Score of all Sentences ###\n",
      "RMSE:  2.0490893915006625\n",
      "Cohen Kappa:  0.28876797313603686\n",
      "Cohen Kappa Quadratic Weight:  0.9727217498960006\n",
      "Accuracy:  0.3817411402157165\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>Kappa_Q</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.372047</td>\n",
       "      <td>0.061841</td>\n",
       "      <td>0.253271</td>\n",
       "      <td>0.284153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.852803</td>\n",
       "      <td>0.143785</td>\n",
       "      <td>0.256081</td>\n",
       "      <td>0.489305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.777683</td>\n",
       "      <td>0.055493</td>\n",
       "      <td>0.231869</td>\n",
       "      <td>0.395210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.647645</td>\n",
       "      <td>0.395261</td>\n",
       "      <td>0.728173</td>\n",
       "      <td>0.580556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.923133</td>\n",
       "      <td>0.117823</td>\n",
       "      <td>0.317429</td>\n",
       "      <td>0.405797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.849837</td>\n",
       "      <td>0.143910</td>\n",
       "      <td>0.466218</td>\n",
       "      <td>0.502646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.238346</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.243856</td>\n",
       "      <td>0.089404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>5.121872</td>\n",
       "      <td>-0.010131</td>\n",
       "      <td>0.354521</td>\n",
       "      <td>0.043796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_set      RMSE     Kappa   Kappa_Q  Accuracy\n",
       "0        1.0  1.372047  0.061841  0.253271  0.284153\n",
       "1        2.0  0.852803  0.143785  0.256081  0.489305\n",
       "2        3.0  0.777683  0.055493  0.231869  0.395210\n",
       "3        4.0  0.647645  0.395261  0.728173  0.580556\n",
       "4        5.0  0.923133  0.117823  0.317429  0.405797\n",
       "5        6.0  0.849837  0.143910  0.466218  0.502646\n",
       "6        7.0  4.238346  0.013377  0.243856  0.089404\n",
       "7        8.0  5.121872 -0.010131  0.354521  0.043796"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,nn_df.Orig_Score.values,\n",
    "                          nn_df.Denorm_Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','pred_score']\n",
    "nn_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "nn_test_df.essay_id = nn_test_df.essay_id.astype(int)\n",
    "nn_test_df.essay_set = nn_test_df.essay_set.astype(int)\n",
    "nn_test_df.Orig_Score = nn_test_df.Orig_Score.astype(int)\n",
    "nn_test_df.Max_Score = nn_test_df.Max_Score.astype(int)\n",
    "nn_test_df.Min_Score = nn_test_df.Min_Score.astype(int)\n",
    "nn_test_df.Mean_Score = nn_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = nn_test_df.Orig_Score.values\n",
    "max_pred_score = nn_test_df.Max_Score.values\n",
    "min_pred_score = nn_test_df.Min_Score.values\n",
    "mean_pred_score = nn_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_nn = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_nn = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_nn = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_nn = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_nn = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_nn = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_nn = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_nn = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_q_nn = cohen_kappa_score(orig_score,mean_pred_score,weights='quadratic')\n",
    "accuracy_mean_nn = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### Simple FF NN : Essay level Metrics : After taking MAX score all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_nn)\n",
    "print(\"Accuracy: \",accuracy_max_nn)\n",
    "\n",
    "print(\"### Simple FF NN : Essay level Metrics : After taking Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_nn)\n",
    "print(\"Accuracy: \",accuracy_min_nn)\n",
    "\n",
    "print(\"### Simple FF NN : Essay level Metrics : After taking Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_nn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_nn)\n",
    "print(\"Cohen Kappa Quadratic Weight: \",cohen_kappa_mean_q_nn)\n",
    "print(\"Accuracy: \",accuracy_mean_nn)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(nn_test_df)\n",
    "essay_set_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147123/147123 [==============================] - 135s 921us/step - loss: 0.0642 - acc: 0.0888\n",
      "Epoch 2/20\n",
      "147123/147123 [==============================] - 137s 932us/step - loss: 0.0347 - acc: 0.0979\n",
      "Epoch 3/20\n",
      "147123/147123 [==============================] - 130s 881us/step - loss: 0.0312 - acc: 0.0993\n",
      "Epoch 4/20\n",
      "147123/147123 [==============================] - 132s 895us/step - loss: 0.0298 - acc: 0.0996\n",
      "Epoch 5/20\n",
      "147123/147123 [==============================] - 132s 900us/step - loss: 0.0289 - acc: 0.0998\n",
      "Epoch 6/20\n",
      "147123/147123 [==============================] - 132s 900us/step - loss: 0.0285 - acc: 0.0999\n",
      "Epoch 7/20\n",
      "147123/147123 [==============================] - 124s 841us/step - loss: 0.0278 - acc: 0.1001\n",
      "Epoch 8/20\n",
      "147123/147123 [==============================] - 120s 816us/step - loss: 0.0276 - acc: 0.1001\n",
      "Epoch 9/20\n",
      "147123/147123 [==============================] - 121s 822us/step - loss: 0.0271 - acc: 0.1002\n",
      "Epoch 10/20\n",
      "147123/147123 [==============================] - 121s 824us/step - loss: 0.0269 - acc: 0.1004\n",
      "Epoch 11/20\n",
      "147123/147123 [==============================] - 127s 864us/step - loss: 0.0266 - acc: 0.1005\n",
      "Epoch 12/20\n",
      "147123/147123 [==============================] - 127s 862us/step - loss: 0.0263 - acc: 0.1005\n",
      "Epoch 13/20\n",
      "147123/147123 [==============================] - 127s 865us/step - loss: 0.0261 - acc: 0.1007\n",
      "Epoch 14/20\n",
      "147123/147123 [==============================] - 121s 819us/step - loss: 0.0259 - acc: 0.1007\n",
      "Epoch 15/20\n",
      "147123/147123 [==============================] - 119s 809us/step - loss: 0.0257 - acc: 0.1009\n",
      "Epoch 16/20\n",
      "147123/147123 [==============================] - 125s 851us/step - loss: 0.0256 - acc: 0.1008\n",
      "Epoch 17/20\n",
      "147123/147123 [==============================] - 132s 897us/step - loss: 0.0254 - acc: 0.1009\n",
      "Epoch 18/20\n",
      "147123/147123 [==============================] - 128s 872us/step - loss: 0.0252 - acc: 0.1011\n",
      "Epoch 19/20\n",
      "147123/147123 [==============================] - 134s 907us/step - loss: 0.0251 - acc: 0.1011\n",
      "Epoch 20/20\n",
      "147123/147123 [==============================] - 121s 822us/step - loss: 0.0249 - acc: 0.1012\n",
      "RMSE:  0.16376953718129628\n",
      "**** METRICS BASED ON DENORMALIZED SCORE FOR SIMPLE RNN****\n",
      "**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\n",
      "RMSE:  2.684139715898777\n",
      "Kappa Quadratic Weighting:  0.9764312881541207\n",
      "Accuracy:  0.32976235793137204\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator = KerasRegressor(build_fn=GRU, epochs=20, batch_size=500)\n",
    "estimator.fit(train_data, y_train)\n",
    "\n",
    "# Predict for test data\n",
    "prediction_rnn=estimator.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_rnn)\n",
    "# Find the overall RMSE value. This is not very relevant considering different scale for each set.\n",
    "print(\"RMSE: \",rmse_val)\n",
    "\n",
    "# Construct pandas data frame of scores of each sentences\n",
    "rnn_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_rnn.astype(np.double)]).transpose()\n",
    "rnn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "# Find Multiplication factor using the function we defined before\n",
    "rnn_df['Mult_Factor'] = rnn_df.apply(find_mult_factor,axis=1)\n",
    "# Find the denormalized predicted score\n",
    "rnn_df['Denorm_Pred_Score'] = rnn_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores of each sentences\n",
    "orig_score = rnn_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = rnn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "\n",
    "# Provide the metrics at sentence level\n",
    "rmse_rnn = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score,weights='quadratic')\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "\n",
    "print(\"**** METRICS BASED ON DENORMALIZED SCORE FOR SIMPLE RNN****\")\n",
    "print(\"**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\")\n",
    "print(\"RMSE: \",rmse_rnn)\n",
    "print(\"Kappa Quadratic Weighting: \",cohen_kappa)\n",
    "print(\"Accuracy: \",accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### RNN GRU Results : MAX Score of all Sentences ###\n",
      "RMSE:  2.6294673630321324\n",
      "Cohen Kappa:  0.23117369442253644\n",
      "Accuracy:  0.3231895223420647\n",
      "### RNN GRU Results : Min Score of all Sentences ###\n",
      "RMSE:  2.5619444416368395\n",
      "Cohen Kappa:  0.30009785322978433\n",
      "Accuracy:  0.39599383667180277\n",
      "### RNN GRU Results : Mean Score of all Sentences ###\n",
      "RMSE:  2.038629256385843\n",
      "Cohen Kappa:  0.2946304356259103\n",
      "Cohen Kappa Quadratic:  0.972956212599746\n",
      "Accuracy:  0.38751926040061635\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>Kappa_Q</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.423841</td>\n",
       "      <td>0.085902</td>\n",
       "      <td>0.211560</td>\n",
       "      <td>0.300546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.797724</td>\n",
       "      <td>0.202530</td>\n",
       "      <td>0.340192</td>\n",
       "      <td>0.526738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.775755</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>0.239757</td>\n",
       "      <td>0.398204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.634648</td>\n",
       "      <td>0.421658</td>\n",
       "      <td>0.743862</td>\n",
       "      <td>0.597222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.915249</td>\n",
       "      <td>0.102742</td>\n",
       "      <td>0.316925</td>\n",
       "      <td>0.397101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.859125</td>\n",
       "      <td>0.088058</td>\n",
       "      <td>0.448045</td>\n",
       "      <td>0.476190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.141536</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>0.261132</td>\n",
       "      <td>0.082781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>5.207378</td>\n",
       "      <td>0.014167</td>\n",
       "      <td>0.320681</td>\n",
       "      <td>0.065693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_set      RMSE     Kappa   Kappa_Q  Accuracy\n",
       "0        1.0  1.423841  0.085902  0.211560  0.300546\n",
       "1        2.0  0.797724  0.202530  0.340192  0.526738\n",
       "2        3.0  0.775755  0.060800  0.239757  0.398204\n",
       "3        4.0  0.634648  0.421658  0.743862  0.597222\n",
       "4        5.0  0.915249  0.102742  0.316925  0.397101\n",
       "5        6.0  0.859125  0.088058  0.448045  0.476190\n",
       "6        7.0  4.141536  0.003170  0.261132  0.082781\n",
       "7        8.0  5.207378  0.014167  0.320681  0.065693"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,rnn_df.Orig_Score.values,\n",
    "                          rnn_df.Denorm_Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','pred_score']\n",
    "rnn_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "rnn_test_df.essay_id = rnn_test_df.essay_id.astype(int)\n",
    "rnn_test_df.essay_set = rnn_test_df.essay_set.astype(int)\n",
    "rnn_test_df.Orig_Score = rnn_test_df.Orig_Score.astype(int)\n",
    "rnn_test_df.Max_Score = rnn_test_df.Max_Score.astype(int)\n",
    "rnn_test_df.Min_Score = rnn_test_df.Min_Score.astype(int)\n",
    "rnn_test_df.Mean_Score = rnn_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = rnn_test_df.Orig_Score.values\n",
    "max_pred_score = rnn_test_df.Max_Score.values\n",
    "min_pred_score = rnn_test_df.Min_Score.values\n",
    "mean_pred_score = rnn_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_rnn = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_rnn = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_rnn = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_rnn = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_rnn = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_rnn = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_rnn = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_rnn = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_q_rnn = cohen_kappa_score(orig_score,mean_pred_score,weights='quadratic')\n",
    "accuracy_mean_rnn = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### RNN GRU Results : MAX Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_rnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_rnn)\n",
    "print(\"Accuracy: \",accuracy_max_rnn)\n",
    "\n",
    "print(\"### RNN GRU Results : Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_rnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_rnn)\n",
    "print(\"Accuracy: \",accuracy_min_rnn)\n",
    "\n",
    "print(\"### RNN GRU Results : Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_rnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_rnn)\n",
    "print(\"Cohen Kappa Quadratic: \",cohen_kappa_mean_q_rnn)\n",
    "print(\"Accuracy: \",accuracy_mean_rnn)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(rnn_test_df)\n",
    "essay_set_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147123/147123 [==============================] - 143s 970us/step - loss: 0.4338 - acc: 0.0795\n",
      "Epoch 2/20\n",
      "147123/147123 [==============================] - 146s 992us/step - loss: 0.0432 - acc: 0.0955\n",
      "Epoch 3/20\n",
      "147123/147123 [==============================] - 132s 897us/step - loss: 0.0386 - acc: 0.0975\n",
      "Epoch 4/20\n",
      "147123/147123 [==============================] - 132s 896us/step - loss: 0.0345 - acc: 0.0982\n",
      "Epoch 5/20\n",
      "147123/147123 [==============================] - 131s 893us/step - loss: 0.0334 - acc: 0.0984\n",
      "Epoch 6/20\n",
      "147123/147123 [==============================] - 131s 889us/step - loss: 0.0329 - acc: 0.0985\n",
      "Epoch 7/20\n",
      "147123/147123 [==============================] - 138s 939us/step - loss: 0.0321 - acc: 0.0987\n",
      "Epoch 8/20\n",
      "147123/147123 [==============================] - 133s 907us/step - loss: 0.0317 - acc: 0.0987\n",
      "Epoch 9/20\n",
      "147123/147123 [==============================] - 132s 894us/step - loss: 0.0315 - acc: 0.0986\n",
      "Epoch 10/20\n",
      "147123/147123 [==============================] - 132s 897us/step - loss: 0.0311 - acc: 0.0987\n",
      "Epoch 11/20\n",
      "147123/147123 [==============================] - 140s 948us/step - loss: 0.0308 - acc: 0.0989\n",
      "Epoch 12/20\n",
      "147123/147123 [==============================] - 140s 949us/step - loss: 0.0305 - acc: 0.0989\n",
      "Epoch 13/20\n",
      "147123/147123 [==============================] - 138s 938us/step - loss: 0.0306 - acc: 0.0989\n",
      "Epoch 14/20\n",
      "147123/147123 [==============================] - 134s 914us/step - loss: 0.0301 - acc: 0.0989\n",
      "Epoch 15/20\n",
      "147123/147123 [==============================] - 142s 968us/step - loss: 0.0296 - acc: 0.0992\n",
      "Epoch 16/20\n",
      "147123/147123 [==============================] - 134s 913us/step - loss: 0.0297 - acc: 0.0991\n",
      "Epoch 17/20\n",
      "147123/147123 [==============================] - 129s 875us/step - loss: 0.0295 - acc: 0.0991\n",
      "Epoch 18/20\n",
      "147123/147123 [==============================] - 131s 888us/step - loss: 0.0290 - acc: 0.0994\n",
      "Epoch 19/20\n",
      "147123/147123 [==============================] - 130s 884us/step - loss: 0.0289 - acc: 0.0993\n",
      "Epoch 20/20\n",
      "147123/147123 [==============================] - 130s 884us/step - loss: 0.0287 - acc: 0.0994\n",
      "RMSE BEFORE DENORMALIZING:  0.17770103259475886\n",
      "**** METRICS BASED ON DENORMALIZED SCORE FOR SIMPLE CNN****\n",
      "**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\n",
      "RMSE:  2.9199863154915833\n",
      "Kappa Quadratic Weighting:  0.9718479854168396\n",
      "Accuracy:  0.2994725107401164\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator = KerasRegressor(build_fn=CNN_FF, epochs=20, batch_size=500)\n",
    "estimator.fit(train_data, y_train)\n",
    "\n",
    "# Precit for test data\n",
    "prediction_cnn_glove=estimator.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_cnn_glove)\n",
    "print(\"RMSE BEFORE DENORMALIZING: \",rmse_val)\n",
    "\n",
    "# Construct data frame for denormalizing the score\n",
    "cnn_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_cnn_glove.astype(np.double)]).transpose()\n",
    "cnn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "cnn_df['Mult_Factor'] = cnn_df.apply(find_mult_factor,axis=1)\n",
    "cnn_df['Denorm_Pred_Score'] = cnn_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = cnn_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = cnn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "rmse_cnn = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score,weights='quadratic')\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "\n",
    "print(\"**** METRICS BASED ON DENORMALIZED SCORE FOR SIMPLE CNN****\")\n",
    "print(\"**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\")\n",
    "print(\"RMSE: \",rmse_cnn)\n",
    "print(\"Kappa Quadratic Weighting: \",cohen_kappa)\n",
    "print(\"Accuracy: \",accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### CNN FF Results : MAX Score of all Sentences ###\n",
      "RMSE:  3.174860321108886\n",
      "Cohen Kappa:  0.21270831634041953\n",
      "Accuracy:  0.3039291217257319\n",
      "### CNN FF Results : Min Score of all Sentences ###\n",
      "RMSE:  2.8558053219678627\n",
      "Cohen Kappa:  0.255875235499437\n",
      "Accuracy:  0.35554699537750384\n",
      "### CNN FF Results : Mean Score of all Sentences ###\n",
      "RMSE:  2.304447364973929\n",
      "Cohen Kappa:  0.250191281372947\n",
      "Cohen Kappa Quadratic:  0.9654299265751505\n",
      "Accuracy:  0.3447611710323575\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>Kappa_Q</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.542654</td>\n",
       "      <td>0.009126</td>\n",
       "      <td>0.035933</td>\n",
       "      <td>0.196721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.013281</td>\n",
       "      <td>-0.000246</td>\n",
       "      <td>0.007121</td>\n",
       "      <td>0.390374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.789148</td>\n",
       "      <td>0.056893</td>\n",
       "      <td>0.213993</td>\n",
       "      <td>0.395210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.703167</td>\n",
       "      <td>0.380702</td>\n",
       "      <td>0.706576</td>\n",
       "      <td>0.569444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.963087</td>\n",
       "      <td>0.076782</td>\n",
       "      <td>0.252134</td>\n",
       "      <td>0.376812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.856040</td>\n",
       "      <td>0.095241</td>\n",
       "      <td>0.448504</td>\n",
       "      <td>0.481481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.792724</td>\n",
       "      <td>0.003852</td>\n",
       "      <td>0.056530</td>\n",
       "      <td>0.072848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>5.801460</td>\n",
       "      <td>-0.009166</td>\n",
       "      <td>0.103354</td>\n",
       "      <td>0.043796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_set      RMSE     Kappa   Kappa_Q  Accuracy\n",
       "0        1.0  1.542654  0.009126  0.035933  0.196721\n",
       "1        2.0  1.013281 -0.000246  0.007121  0.390374\n",
       "2        3.0  0.789148  0.056893  0.213993  0.395210\n",
       "3        4.0  0.703167  0.380702  0.706576  0.569444\n",
       "4        5.0  0.963087  0.076782  0.252134  0.376812\n",
       "5        6.0  0.856040  0.095241  0.448504  0.481481\n",
       "6        7.0  4.792724  0.003852  0.056530  0.072848\n",
       "7        8.0  5.801460 -0.009166  0.103354  0.043796"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,cnn_df.Orig_Score.values,\n",
    "                          cnn_df.Denorm_Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','pred_score']\n",
    "cnn_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "cnn_test_df.essay_id = cnn_test_df.essay_id.astype(int)\n",
    "cnn_test_df.essay_set = cnn_test_df.essay_set.astype(int)\n",
    "cnn_test_df.Orig_Score = cnn_test_df.Orig_Score.astype(int)\n",
    "cnn_test_df.Max_Score = cnn_test_df.Max_Score.astype(int)\n",
    "cnn_test_df.Min_Score = cnn_test_df.Min_Score.astype(int)\n",
    "cnn_test_df.Mean_Score = cnn_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = cnn_test_df.Orig_Score.values\n",
    "max_pred_score = cnn_test_df.Max_Score.values\n",
    "min_pred_score = cnn_test_df.Min_Score.values\n",
    "mean_pred_score = cnn_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_cnn = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_cnn = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_cnn = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_cnn = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_cnn = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_cnn = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_cnn = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_cnn = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_q_cnn = cohen_kappa_score(orig_score,mean_pred_score,weights='quadratic')\n",
    "accuracy_mean_cnn = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### CNN FF Results : MAX Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_cnn)\n",
    "print(\"Accuracy: \",accuracy_max_cnn)\n",
    "\n",
    "print(\"### CNN FF Results : Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_cnn)\n",
    "print(\"Accuracy: \",accuracy_min_cnn)\n",
    "\n",
    "print(\"### CNN FF Results : Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_cnn)\n",
    "print(\"Cohen Kappa Quadratic: \",cohen_kappa_mean_q_cnn)\n",
    "print(\"Accuracy: \",accuracy_mean_cnn)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(cnn_test_df)\n",
    "essay_set_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147123/147123 [==============================] - 208s 1ms/step - loss: 0.1178 - acc: 0.0909\n",
      "Epoch 2/20\n",
      "147123/147123 [==============================] - 207s 1ms/step - loss: 0.0320 - acc: 0.0986\n",
      "Epoch 3/20\n",
      "147123/147123 [==============================] - 211s 1ms/step - loss: 0.0303 - acc: 0.0987\n",
      "Epoch 4/20\n",
      "147123/147123 [==============================] - 214s 1ms/step - loss: 0.0293 - acc: 0.0989\n",
      "Epoch 5/20\n",
      "147123/147123 [==============================] - 207s 1ms/step - loss: 0.0286 - acc: 0.0991\n",
      "Epoch 6/20\n",
      "147123/147123 [==============================] - 208s 1ms/step - loss: 0.0279 - acc: 0.0992\n",
      "Epoch 7/20\n",
      "147123/147123 [==============================] - 209s 1ms/step - loss: 0.0274 - acc: 0.0995\n",
      "Epoch 8/20\n",
      "147123/147123 [==============================] - 206s 1ms/step - loss: 0.0268 - acc: 0.0998\n",
      "Epoch 9/20\n",
      "147123/147123 [==============================] - 194s 1ms/step - loss: 0.0262 - acc: 0.1000\n",
      "Epoch 10/20\n",
      "147123/147123 [==============================] - 194s 1ms/step - loss: 0.0256 - acc: 0.1004\n",
      "Epoch 11/20\n",
      "147123/147123 [==============================] - 192s 1ms/step - loss: 0.0251 - acc: 0.1005\n",
      "Epoch 12/20\n",
      "147123/147123 [==============================] - 204s 1ms/step - loss: 0.0246 - acc: 0.1008\n",
      "Epoch 13/20\n",
      "147123/147123 [==============================] - 198s 1ms/step - loss: 0.0241 - acc: 0.1011\n",
      "Epoch 14/20\n",
      "147123/147123 [==============================] - 193s 1ms/step - loss: 0.0235 - acc: 0.1013\n",
      "Epoch 15/20\n",
      "147123/147123 [==============================] - 195s 1ms/step - loss: 0.0231 - acc: 0.1015\n",
      "Epoch 16/20\n",
      "147123/147123 [==============================] - 191s 1ms/step - loss: 0.0225 - acc: 0.1016\n",
      "Epoch 17/20\n",
      "147123/147123 [==============================] - 204s 1ms/step - loss: 0.0222 - acc: 0.1017\n",
      "Epoch 18/20\n",
      "147123/147123 [==============================] - 212s 1ms/step - loss: 0.0216 - acc: 0.1022\n",
      "Epoch 19/20\n",
      "147123/147123 [==============================] - 200s 1ms/step - loss: 0.0213 - acc: 0.1023\n",
      "Epoch 20/20\n",
      "147123/147123 [==============================] - 208s 1ms/step - loss: 0.0209 - acc: 0.1024\n",
      "RMSE BEFORE DENORMALIZING:  0.1802107681054047\n",
      "**** METRICS BASED ON DENORMALIZED SCORE FOR CNN+LSTM****\n",
      "**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\n",
      "RMSE:  3.031107414698982\n",
      "Kappa Quadratic Weighting:  0.9707114569611448\n",
      "Accuracy:  0.3067594757735603\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator = KerasRegressor(build_fn=CNN_lstm, epochs=20, batch_size=500)\n",
    "estimator.fit(train_data, y_train)\n",
    "\n",
    "# Precit for test data\n",
    "prediction_cnn_lstm=estimator.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_cnn_lstm)\n",
    "print(\"RMSE BEFORE DENORMALIZING: \",rmse_val)\n",
    "\n",
    "# Construct data frame for denormalizing the score\n",
    "cnn_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_cnn_lstm.astype(np.double)]).transpose()\n",
    "cnn_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "cnn_df['Mult_Factor'] = cnn_df.apply(find_mult_factor,axis=1)\n",
    "cnn_df['Denorm_Pred_Score'] = cnn_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = cnn_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = cnn_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "rmse_cnn = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score,weights='quadratic')\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "\n",
    "print(\"**** METRICS BASED ON DENORMALIZED SCORE FOR CNN+LSTM****\")\n",
    "print(\"**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\")\n",
    "print(\"RMSE: \",rmse_cnn)\n",
    "print(\"Kappa Quadratic Weighting: \",cohen_kappa)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### CNN LSTM Results : MAX Score of all Sentences ###\n",
      "RMSE:  3.4449221244840396\n",
      "Cohen Kappa:  0.16762993097773926\n",
      "Accuracy:  0.2569337442218798\n",
      "### CNN LSTM Results : Min Score of all Sentences ###\n",
      "RMSE:  3.275369161376681\n",
      "Cohen Kappa:  0.22088848804757633\n",
      "Accuracy:  0.32126348228043144\n",
      "### CNN LSTM Results : Mean Score of all Sentences ###\n",
      "RMSE:  2.1855386871090934\n",
      "Cohen Kappa:  0.27847628160086124\n",
      "Cohen Kappa Quadratic:  0.9696044936380251\n",
      "Accuracy:  0.3697996918335901\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>Kappa_Q</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.436260</td>\n",
       "      <td>0.056193</td>\n",
       "      <td>0.211959</td>\n",
       "      <td>0.254098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.922099</td>\n",
       "      <td>0.092158</td>\n",
       "      <td>0.157407</td>\n",
       "      <td>0.451872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.787249</td>\n",
       "      <td>0.062492</td>\n",
       "      <td>0.234437</td>\n",
       "      <td>0.398204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.643342</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.740524</td>\n",
       "      <td>0.594444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.921561</td>\n",
       "      <td>0.126952</td>\n",
       "      <td>0.328317</td>\n",
       "      <td>0.411594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.875897</td>\n",
       "      <td>0.122077</td>\n",
       "      <td>0.450229</td>\n",
       "      <td>0.486772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.521106</td>\n",
       "      <td>-0.014737</td>\n",
       "      <td>0.187675</td>\n",
       "      <td>0.056291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>5.546749</td>\n",
       "      <td>0.005235</td>\n",
       "      <td>0.236956</td>\n",
       "      <td>0.058394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_set      RMSE     Kappa   Kappa_Q  Accuracy\n",
       "0        1.0  1.436260  0.056193  0.211959  0.254098\n",
       "1        2.0  0.922099  0.092158  0.157407  0.451872\n",
       "2        3.0  0.787249  0.062492  0.234437  0.398204\n",
       "3        4.0  0.643342  0.419355  0.740524  0.594444\n",
       "4        5.0  0.921561  0.126952  0.328317  0.411594\n",
       "5        6.0  0.875897  0.122077  0.450229  0.486772\n",
       "6        7.0  4.521106 -0.014737  0.187675  0.056291\n",
       "7        8.0  5.546749  0.005235  0.236956  0.058394"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,cnn_df.Orig_Score.values,\n",
    "                          cnn_df.Denorm_Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','pred_score']\n",
    "cnn_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "cnn_test_df.essay_id = cnn_test_df.essay_id.astype(int)\n",
    "cnn_test_df.essay_set = cnn_test_df.essay_set.astype(int)\n",
    "cnn_test_df.Orig_Score = cnn_test_df.Orig_Score.astype(int)\n",
    "cnn_test_df.Max_Score = cnn_test_df.Max_Score.astype(int)\n",
    "cnn_test_df.Min_Score = cnn_test_df.Min_Score.astype(int)\n",
    "cnn_test_df.Mean_Score = cnn_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = cnn_test_df.Orig_Score.values\n",
    "max_pred_score = cnn_test_df.Max_Score.values\n",
    "min_pred_score = cnn_test_df.Min_Score.values\n",
    "mean_pred_score = cnn_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_cnn = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_cnn = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_cnn = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_cnn = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_cnn = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_cnn = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_cnn = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_cnn = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_q_cnn = cohen_kappa_score(orig_score,mean_pred_score,weights='quadratic')\n",
    "accuracy_mean_cnn = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### CNN LSTM Results : MAX Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_cnn)\n",
    "print(\"Accuracy: \",accuracy_max_cnn)\n",
    "\n",
    "print(\"### CNN LSTM Results : Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_cnn)\n",
    "print(\"Accuracy: \",accuracy_min_cnn)\n",
    "\n",
    "print(\"### CNN LSTM Results : Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_cnn)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_cnn)\n",
    "print(\"Cohen Kappa Quadratic: \",cohen_kappa_mean_q_cnn)\n",
    "print(\"Accuracy: \",accuracy_mean_cnn)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(cnn_test_df)\n",
    "essay_set_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "147123/147123 [==============================] - 252s 2ms/step - loss: 0.0365 - acc: 0.0973\n",
      "Epoch 2/20\n",
      "147123/147123 [==============================] - 248s 2ms/step - loss: 0.0289 - acc: 0.0988\n",
      "Epoch 3/20\n",
      "147123/147123 [==============================] - 248s 2ms/step - loss: 0.0283 - acc: 0.0992\n",
      "Epoch 4/20\n",
      "147123/147123 [==============================] - 246s 2ms/step - loss: 0.0275 - acc: 0.0997\n",
      "Epoch 5/20\n",
      "147123/147123 [==============================] - 246s 2ms/step - loss: 0.0272 - acc: 0.1000\n",
      "Epoch 6/20\n",
      "147123/147123 [==============================] - 436s 3ms/step - loss: 0.0268 - acc: 0.1003\n",
      "Epoch 7/20\n",
      "147123/147123 [==============================] - 247s 2ms/step - loss: 0.0265 - acc: 0.1005\n",
      "Epoch 8/20\n",
      "147123/147123 [==============================] - 247s 2ms/step - loss: 0.0263 - acc: 0.1005\n",
      "Epoch 9/20\n",
      "147123/147123 [==============================] - 246s 2ms/step - loss: 0.0261 - acc: 0.1006\n",
      "Epoch 10/20\n",
      "147123/147123 [==============================] - 251s 2ms/step - loss: 0.0257 - acc: 0.1008\n",
      "Epoch 11/20\n",
      "147123/147123 [==============================] - 785s 5ms/step - loss: 0.0255 - acc: 0.1010\n",
      "Epoch 12/20\n",
      "147123/147123 [==============================] - 257s 2ms/step - loss: 0.0254 - acc: 0.1011\n",
      "Epoch 13/20\n",
      "147123/147123 [==============================] - 247s 2ms/step - loss: 0.0251 - acc: 0.1012\n",
      "Epoch 14/20\n",
      "147123/147123 [==============================] - 248s 2ms/step - loss: 0.0249 - acc: 0.1014\n",
      "Epoch 15/20\n",
      "147123/147123 [==============================] - 259s 2ms/step - loss: 0.0246 - acc: 0.1015\n",
      "Epoch 16/20\n",
      "147123/147123 [==============================] - 260s 2ms/step - loss: 0.0244 - acc: 0.1018\n",
      "Epoch 17/20\n",
      "147123/147123 [==============================] - 252s 2ms/step - loss: 0.0241 - acc: 0.1017\n",
      "Epoch 18/20\n",
      "147123/147123 [==============================] - 248s 2ms/step - loss: 0.0239 - acc: 0.1020\n",
      "Epoch 19/20\n",
      "147123/147123 [==============================] - 250s 2ms/step - loss: 0.0237 - acc: 0.1020\n",
      "Epoch 20/20\n",
      "147123/147123 [==============================] - 250s 2ms/step - loss: 0.0236 - acc: 0.1021\n",
      "RMSE:  0.16435884045638574\n",
      "RMSE Sentence Level:  2.682513368191454\n",
      "Kappa Sentence Level:  0.26781235443159734\n",
      "Accuracy Sentence Level:  0.33120343683723963\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "estimator_lstm = KerasRegressor(build_fn=stack_lstm, epochs=20, batch_size=500)\n",
    "estimator_lstm.fit(train_data, y_train)\n",
    "\n",
    "# Predict for test data\n",
    "prediction_lstm=estimator_lstm.predict(test_data)\n",
    "rmse_val = RMSE(y_test,prediction_lstm)\n",
    "# Find the overall RMSE value. This is not very relevant considering different scale for each set.\n",
    "print(\"RMSE: \",rmse_val)\n",
    "\n",
    "# Construct pandas data frame of scores of each sentences\n",
    "lstm_df = pd.DataFrame([x_test_sentence_df['domain1_score'].astype(np.double), y_test.astype(np.double),\n",
    "                      prediction_lstm.astype(np.double)]).transpose()\n",
    "lstm_df.columns = ['Orig_Score','Norm_Score','Pred_Score']\n",
    "# Find Multiplication factor using the function we defined before\n",
    "lstm_df['Mult_Factor'] = lstm_df.apply(find_mult_factor,axis=1)\n",
    "# Find the denormalized predicted score\n",
    "lstm_df['Denorm_Pred_Score'] = lstm_df.apply(denormalize,axis=1)\n",
    "\n",
    "# Extract the scores of each sentences\n",
    "orig_score = lstm_df.Orig_Score.as_matrix(columns=None)\n",
    "orig_score = orig_score.astype(np.int)\n",
    "pred_score = lstm_df.Denorm_Pred_Score.as_matrix(columns=None)\n",
    "pred_score = pred_score.astype(np.int)\n",
    "\n",
    "# Provide the metrics at sentence level\n",
    "rmse_lstm = RMSE(orig_score,pred_score)\n",
    "cohen_kappa = cohen_kappa_score(orig_score,pred_score,weights='quadratic')\n",
    "accuracy = accuracy_score(orig_score,pred_score)\n",
    "\n",
    "print(\"**** METRICS BASED ON DENORMALIZED SCORE FOR SIMPLE CNN****\")\n",
    "print(\"**** NOTE: THIS IS STILL AT SENTENCE LEVEL. NEXT SECTION WILL MERGE SENTENCE SCORES****\")\n",
    "print(\"RMSE: \",rmse_lstm)\n",
    "print(\"Kappa Quadratic Weighting: \",cohen_kappa)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Stacked LSTM Results : MAX Score of all Sentences ###\n",
      "RMSE:  2.6410881634680607\n",
      "Cohen Kappa:  0.23329561791239495\n",
      "Accuracy:  0.3274268104776579\n",
      "### Stacked LSTM Results : Min Score of all Sentences ###\n",
      "RMSE:  2.71892018355833\n",
      "Cohen Kappa:  0.29402029329686785\n",
      "Accuracy:  0.38906009244992296\n",
      "### Stacked LSTM Results : Mean Score of all Sentences ###\n",
      "RMSE:  2.0400459223206706\n",
      "Cohen Kappa:  0.2959414229512113\n",
      "Accuracy:  0.387904468412943\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_set</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.389854</td>\n",
       "      <td>0.076284</td>\n",
       "      <td>0.297814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.822478</td>\n",
       "      <td>0.172556</td>\n",
       "      <td>0.505348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.775755</td>\n",
       "      <td>0.064008</td>\n",
       "      <td>0.398204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.628048</td>\n",
       "      <td>0.433737</td>\n",
       "      <td>0.605556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.892805</td>\n",
       "      <td>0.109588</td>\n",
       "      <td>0.402899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.819730</td>\n",
       "      <td>0.125403</td>\n",
       "      <td>0.494709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.165055</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>0.079470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>5.219978</td>\n",
       "      <td>0.010692</td>\n",
       "      <td>0.058394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_set      RMSE     Kappa  Accuracy\n",
       "0        1.0  1.389854  0.076284  0.297814\n",
       "1        2.0  0.822478  0.172556  0.505348\n",
       "2        3.0  0.775755  0.064008  0.398204\n",
       "3        4.0  0.628048  0.433737  0.605556\n",
       "4        5.0  0.892805  0.109588  0.402899\n",
       "5        6.0  0.819730  0.125403  0.494709\n",
       "6        7.0  4.165055  0.003667  0.079470\n",
       "7        8.0  5.219978  0.010692  0.058394"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will combine the sentences back to essay in this section\n",
    "\n",
    "result_df = pd.DataFrame([x_test_sentence_df.essay_id.values,x_test_sentence_df.essay_set.values,\n",
    "                         x_test_sentence_df.sentence.values,lstm_df.Orig_Score.values,\n",
    "                          lstm_df.Denorm_Pred_Score.values]).transpose()\n",
    "result_df.columns = ['essay_id','essay_set','sentence','orig_score','pred_score']\n",
    "lstm_test_df = find_max_min_mean_score(result_df)\n",
    "\n",
    "#Convert the dataframe parameters into integers as they are returned as floats\n",
    "lstm_test_df.essay_id = lstm_test_df.essay_id.astype(int)\n",
    "lstm_test_df.essay_set = lstm_test_df.essay_set.astype(int)\n",
    "lstm_test_df.Orig_Score = lstm_test_df.Orig_Score.astype(int)\n",
    "lstm_test_df.Max_Score = lstm_test_df.Max_Score.astype(int)\n",
    "lstm_test_df.Min_Score = lstm_test_df.Min_Score.astype(int)\n",
    "lstm_test_df.Mean_Score = lstm_test_df.Mean_Score.astype(int)\n",
    "\n",
    "# Extract the scores\n",
    "orig_score = lstm_test_df.Orig_Score.values\n",
    "max_pred_score = lstm_test_df.Max_Score.values\n",
    "min_pred_score = lstm_test_df.Min_Score.values\n",
    "mean_pred_score = lstm_test_df.Mean_Score.values\n",
    "\n",
    "# Compare the metrics if Max score of all sentences are taken\n",
    "rmse_max_lstm = RMSE(orig_score,max_pred_score)\n",
    "cohen_kappa_max_lstm = cohen_kappa_score(orig_score,max_pred_score)\n",
    "accuracy_max_lstm = accuracy_score(orig_score,max_pred_score)\n",
    "\n",
    "# Compare the metrics if Min score of all sentences are taken\n",
    "rmse_min_lstm = RMSE(orig_score,min_pred_score)\n",
    "cohen_kappa_min_lstm = cohen_kappa_score(orig_score,min_pred_score)\n",
    "accuracy_min_lstm = accuracy_score(orig_score,min_pred_score)\n",
    "\n",
    "# Compare the metrics if Mean score of all sentences are taken\n",
    "rmse_mean_lstm = RMSE(orig_score,mean_pred_score)\n",
    "cohen_kappa_mean_lstm = cohen_kappa_score(orig_score,mean_pred_score)\n",
    "accuracy_mean_lstm = accuracy_score(orig_score,mean_pred_score)\n",
    "\n",
    "print(\"### Stacked LSTM Results : MAX Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_max_lstm)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_max_lstm)\n",
    "print(\"Accuracy: \",accuracy_max_lstm)\n",
    "\n",
    "print(\"### Stacked LSTM Results : Min Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_min_lstm)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_min_lstm)\n",
    "print(\"Accuracy: \",accuracy_min_lstm)\n",
    "\n",
    "print(\"### Stacked LSTM Results : Mean Score of all Sentences ###\")\n",
    "print(\"RMSE: \",rmse_mean_lstm)\n",
    "print(\"Cohen Kappa: \",cohen_kappa_mean_lstm)\n",
    "print(\"Accuracy: \",accuracy_mean_lstm)\n",
    "\n",
    "# Provide per-essay-set metrics based on mean score\n",
    "essay_set_results = essay_set_metrics(lstm_test_df)\n",
    "essay_set_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
